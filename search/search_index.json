{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pyjanitor pyjanitor is a Python implementation of the R package janitor , and provides a clean API for cleaning data. Quick start Installation: conda install -c conda-forge pyjanitor Check out the collection of general functions Why janitor? Originally a port of the R package, pyjanitor has evolved from a set of convenient data cleaning routines into an experiment with the method chaining paradigm. Data preprocessing usually consists of a series of steps that involve transforming raw data into an understandable/usable format. These series of steps need to be run in a certain sequence to achieve success. We take a base data file as the starting point, and perform actions on it, such as removing null/empty rows, replacing them with other values, adding/renaming/removing columns of data, filtering rows and others. More formally, these steps along with their relationships and dependencies are commonly referred to as a Directed Acyclic Graph (DAG). The pandas API has been invaluable for the Python data science ecosystem, and implements method chaining of a subset of methods as part of the API. For example, resetting indexes ( .reset_index() ), dropping null values ( .dropna() ), and more, are accomplished via the appropriate pd.DataFrame method calls. Inspired by the ease-of-use and expressiveness of the dplyr package of the R statistical language ecosystem, we have evolved pyjanitor into a language for expressing the data processing DAG for pandas users. To accomplish this, actions for which we would need to invoke imperative-style statements, can be replaced with method chains that allow one to read off the logical order of actions taken. Let us see the annotated example below. First off, here is the textual description of a data cleaning pathway: Create a DataFrame . Delete one column. Drop rows with empty values in two particular columns. Rename another two columns. Add a new column. Let's import some libraries and begin with some sample data for this example: # Libraries import numpy as np import pandas as pd import janitor # Sample Data curated for this example company_sales = { 'SalesMonth': ['Jan', 'Feb', 'Mar', 'April'], 'Company1': [150.0, 200.0, 300.0, 400.0], 'Company2': [180.0, 250.0, np.nan, 500.0], 'Company3': [400.0, 500.0, 600.0, 675.0] } In pandas code, most users might type something like this: # The Pandas Way # 1. Create a pandas DataFrame from the company_sales dictionary df = pd.DataFrame.from_dict(company_sales) # 2. Delete a column from the DataFrame. Say 'Company1' del df['Company1'] # 3. Drop rows that have empty values in columns 'Company2' and 'Company3' df = df.dropna(subset=['Company2', 'Company3']) # 4. Rename 'Company2' to 'Amazon' and 'Company3' to 'Facebook' df = df.rename( { 'Company2': 'Amazon', 'Company3': 'Facebook', }, axis=1, ) # 5. Let's add some data for another company. Say 'Google' df['Google'] = [450.0, 550.0, 800.0] # Output looks like this: # Out[15]: # SalesMonth Amazon Facebook Google # 0 Jan 180.0 400.0 450.0 # 1 Feb 250.0 500.0 550.0 # 3 April 500.0 675.0 800.0 Slightly more advanced users might take advantage of the functional API: df = ( pd.DataFrame(company_sales) .drop(columns=\"Company1\") .dropna(subset=[\"Company2\", \"Company3\"]) .rename(columns={\"Company2\": \"Amazon\", \"Company3\": \"Facebook\"}) .assign(Google=[450.0, 550.0, 800.0]) ) # The output is the same as before, and looks like this: # Out[15]: # SalesMonth Amazon Facebook Google # 0 Jan 180.0 400.0 450.0 # 1 Feb 250.0 500.0 550.0 # 3 April 500.0 675.0 800.0 With pyjanitor , we enable method chaining with method names that are explicitly named verbs , which describe the action taken. df = ( pd.DataFrame.from_dict(company_sales) .remove_columns([\"Company1\"]) .dropna(subset=[\"Company2\", \"Company3\"]) .rename_column(\"Company2\", \"Amazon\") .rename_column(\"Company3\", \"Facebook\") .add_column(\"Google\", [450.0, 550.0, 800.0]) ) # Output looks like this: # Out[15]: # SalesMonth Amazon Facebook Google # 0 Jan 180.0 400.0 450.0 # 1 Feb 250.0 500.0 550.0 # 3 April 500.0 675.0 800.0 As such, pyjanitor 's etymology has a two-fold relationship to \"cleanliness\". Firstly, it's about extending Pandas with convenient data cleaning routines. Secondly, it's about providing a cleaner, method-chaining, verb-based API for common pandas routines. Installation pyjanitor is currently installable from PyPI: pip install pyjanitor pyjanitor also can be installed by the conda package manager: conda install pyjanitor -c conda-forge pyjanitor can be installed by the pipenv environment manager too. This requires enabling prerelease dependencies: pipenv install --pre pyjanitor pyjanitor requires Python 3.6+. Functionality Current functionality includes: Cleaning columns name (multi-indexes are possible!) Removing empty rows and columns Identifying duplicate entries Encoding columns as categorical Splitting your data into features and targets (for machine learning) Adding, removing, and renaming columns Coalesce multiple columns into a single column Date conversions (from matlab, excel, unix) to Python datetime format Expand a single column that has delimited, categorical values into dummy-encoded variables Concatenating and deconcatenating columns, based on a delimiter Syntactic sugar for filtering the dataframe based on queries on a column Experimental submodules for finance, biology, chemistry, engineering, and pyspark API The idea behind the API is two-fold: Copy the R package function names, but enable Pythonic use with method chaining or pandas piping. Add other utility functions that make it easy to do data cleaning/preprocessing in pandas . Continuing with the company_sales dataframe previously used: import pandas as pd import numpy as np company_sales = { 'SalesMonth': ['Jan', 'Feb', 'Mar', 'April'], 'Company1': [150.0, 200.0, 300.0, 400.0], 'Company2': [180.0, 250.0, np.nan, 500.0], 'Company3': [400.0, 500.0, 600.0, 675.0] } As such, there are three ways to use the API. The first, and most strongly recommended one, is to use pyjanitor 's functions as if they were native to pandas. import janitor # upon import, functions are registered as part of pandas. # This cleans the column names as well as removes any duplicate rows df = pd.DataFrame.from_dict(company_sales).clean_names().remove_empty() The second is the functional API. from janitor import clean_names, remove_empty df = pd.DataFrame.from_dict(company_sales) df = clean_names(df) df = remove_empty(df) The final way is to use the pipe() method: from janitor import clean_names, remove_empty df = ( pd.DataFrame.from_dict(company_sales) .pipe(clean_names) .pipe(remove_empty) ) Contributing Follow the development guide for a full description of the process of contributing to pyjanitor . Adding new functionality Keeping in mind the etymology of pyjanitor, contributing a new function to pyjanitor is a task that is not difficult at all. Define a function First off, you will need to define the function that expresses the data processing/cleaning routine, such that it accepts a dataframe as the first argument, and returns a modified dataframe: import pandas_flavor as pf @pf.register_dataframe_method def my_data_cleaning_function(df, arg1, arg2, ...): # Put data processing function here. return df We use pandas_flavor to register the function natively on a pandas.DataFrame . Add a test case Secondly, we ask that you contribute a test case, to ensure that the function works as intended. Follow the contribution docs for further details. Feature requests If you have a feature request, please post it as an issue on the GitHub repository issue tracker. Even better, put in a PR for it! We are more than happy to guide you through the codebase so that you can put in a contribution to the codebase. Because pyjanitor is currently maintained by volunteers and has no fiscal support, any feature requests will be prioritized according to what maintainers encounter as a need in our day-to-day jobs. Please temper expectations accordingly. API Policy pyjanitor only extends or aliases the pandas API (and other dataframe APIs), but will never fix or replace them. Undesirable pandas behaviour should be reported upstream in the pandas issue tracker . We explicitly do not fix the pandas API. If at some point the pandas devs decide to take something from pyjanitor and internalize it as part of the official pandas API, then we will deprecate it from pyjanitor , while acknowledging the original contributors' contribution as part of the official deprecation record. Credits Test data for chemistry submodule can be found at Predictive Toxicology .","title":"Home"},{"location":"#pyjanitor","text":"pyjanitor is a Python implementation of the R package janitor , and provides a clean API for cleaning data.","title":"pyjanitor"},{"location":"#quick-start","text":"Installation: conda install -c conda-forge pyjanitor Check out the collection of general functions","title":"Quick start"},{"location":"#why-janitor","text":"Originally a port of the R package, pyjanitor has evolved from a set of convenient data cleaning routines into an experiment with the method chaining paradigm. Data preprocessing usually consists of a series of steps that involve transforming raw data into an understandable/usable format. These series of steps need to be run in a certain sequence to achieve success. We take a base data file as the starting point, and perform actions on it, such as removing null/empty rows, replacing them with other values, adding/renaming/removing columns of data, filtering rows and others. More formally, these steps along with their relationships and dependencies are commonly referred to as a Directed Acyclic Graph (DAG). The pandas API has been invaluable for the Python data science ecosystem, and implements method chaining of a subset of methods as part of the API. For example, resetting indexes ( .reset_index() ), dropping null values ( .dropna() ), and more, are accomplished via the appropriate pd.DataFrame method calls. Inspired by the ease-of-use and expressiveness of the dplyr package of the R statistical language ecosystem, we have evolved pyjanitor into a language for expressing the data processing DAG for pandas users. To accomplish this, actions for which we would need to invoke imperative-style statements, can be replaced with method chains that allow one to read off the logical order of actions taken. Let us see the annotated example below. First off, here is the textual description of a data cleaning pathway: Create a DataFrame . Delete one column. Drop rows with empty values in two particular columns. Rename another two columns. Add a new column. Let's import some libraries and begin with some sample data for this example: # Libraries import numpy as np import pandas as pd import janitor # Sample Data curated for this example company_sales = { 'SalesMonth': ['Jan', 'Feb', 'Mar', 'April'], 'Company1': [150.0, 200.0, 300.0, 400.0], 'Company2': [180.0, 250.0, np.nan, 500.0], 'Company3': [400.0, 500.0, 600.0, 675.0] } In pandas code, most users might type something like this: # The Pandas Way # 1. Create a pandas DataFrame from the company_sales dictionary df = pd.DataFrame.from_dict(company_sales) # 2. Delete a column from the DataFrame. Say 'Company1' del df['Company1'] # 3. Drop rows that have empty values in columns 'Company2' and 'Company3' df = df.dropna(subset=['Company2', 'Company3']) # 4. Rename 'Company2' to 'Amazon' and 'Company3' to 'Facebook' df = df.rename( { 'Company2': 'Amazon', 'Company3': 'Facebook', }, axis=1, ) # 5. Let's add some data for another company. Say 'Google' df['Google'] = [450.0, 550.0, 800.0] # Output looks like this: # Out[15]: # SalesMonth Amazon Facebook Google # 0 Jan 180.0 400.0 450.0 # 1 Feb 250.0 500.0 550.0 # 3 April 500.0 675.0 800.0 Slightly more advanced users might take advantage of the functional API: df = ( pd.DataFrame(company_sales) .drop(columns=\"Company1\") .dropna(subset=[\"Company2\", \"Company3\"]) .rename(columns={\"Company2\": \"Amazon\", \"Company3\": \"Facebook\"}) .assign(Google=[450.0, 550.0, 800.0]) ) # The output is the same as before, and looks like this: # Out[15]: # SalesMonth Amazon Facebook Google # 0 Jan 180.0 400.0 450.0 # 1 Feb 250.0 500.0 550.0 # 3 April 500.0 675.0 800.0 With pyjanitor , we enable method chaining with method names that are explicitly named verbs , which describe the action taken. df = ( pd.DataFrame.from_dict(company_sales) .remove_columns([\"Company1\"]) .dropna(subset=[\"Company2\", \"Company3\"]) .rename_column(\"Company2\", \"Amazon\") .rename_column(\"Company3\", \"Facebook\") .add_column(\"Google\", [450.0, 550.0, 800.0]) ) # Output looks like this: # Out[15]: # SalesMonth Amazon Facebook Google # 0 Jan 180.0 400.0 450.0 # 1 Feb 250.0 500.0 550.0 # 3 April 500.0 675.0 800.0 As such, pyjanitor 's etymology has a two-fold relationship to \"cleanliness\". Firstly, it's about extending Pandas with convenient data cleaning routines. Secondly, it's about providing a cleaner, method-chaining, verb-based API for common pandas routines.","title":"Why janitor?"},{"location":"#installation","text":"pyjanitor is currently installable from PyPI: pip install pyjanitor pyjanitor also can be installed by the conda package manager: conda install pyjanitor -c conda-forge pyjanitor can be installed by the pipenv environment manager too. This requires enabling prerelease dependencies: pipenv install --pre pyjanitor pyjanitor requires Python 3.6+.","title":"Installation"},{"location":"#functionality","text":"Current functionality includes: Cleaning columns name (multi-indexes are possible!) Removing empty rows and columns Identifying duplicate entries Encoding columns as categorical Splitting your data into features and targets (for machine learning) Adding, removing, and renaming columns Coalesce multiple columns into a single column Date conversions (from matlab, excel, unix) to Python datetime format Expand a single column that has delimited, categorical values into dummy-encoded variables Concatenating and deconcatenating columns, based on a delimiter Syntactic sugar for filtering the dataframe based on queries on a column Experimental submodules for finance, biology, chemistry, engineering, and pyspark","title":"Functionality"},{"location":"#api","text":"The idea behind the API is two-fold: Copy the R package function names, but enable Pythonic use with method chaining or pandas piping. Add other utility functions that make it easy to do data cleaning/preprocessing in pandas . Continuing with the company_sales dataframe previously used: import pandas as pd import numpy as np company_sales = { 'SalesMonth': ['Jan', 'Feb', 'Mar', 'April'], 'Company1': [150.0, 200.0, 300.0, 400.0], 'Company2': [180.0, 250.0, np.nan, 500.0], 'Company3': [400.0, 500.0, 600.0, 675.0] } As such, there are three ways to use the API. The first, and most strongly recommended one, is to use pyjanitor 's functions as if they were native to pandas. import janitor # upon import, functions are registered as part of pandas. # This cleans the column names as well as removes any duplicate rows df = pd.DataFrame.from_dict(company_sales).clean_names().remove_empty() The second is the functional API. from janitor import clean_names, remove_empty df = pd.DataFrame.from_dict(company_sales) df = clean_names(df) df = remove_empty(df) The final way is to use the pipe() method: from janitor import clean_names, remove_empty df = ( pd.DataFrame.from_dict(company_sales) .pipe(clean_names) .pipe(remove_empty) )","title":"API"},{"location":"#contributing","text":"Follow the development guide for a full description of the process of contributing to pyjanitor .","title":"Contributing"},{"location":"#adding-new-functionality","text":"Keeping in mind the etymology of pyjanitor, contributing a new function to pyjanitor is a task that is not difficult at all.","title":"Adding new functionality"},{"location":"#define-a-function","text":"First off, you will need to define the function that expresses the data processing/cleaning routine, such that it accepts a dataframe as the first argument, and returns a modified dataframe: import pandas_flavor as pf @pf.register_dataframe_method def my_data_cleaning_function(df, arg1, arg2, ...): # Put data processing function here. return df We use pandas_flavor to register the function natively on a pandas.DataFrame .","title":"Define a function"},{"location":"#add-a-test-case","text":"Secondly, we ask that you contribute a test case, to ensure that the function works as intended. Follow the contribution docs for further details.","title":"Add a test case"},{"location":"#feature-requests","text":"If you have a feature request, please post it as an issue on the GitHub repository issue tracker. Even better, put in a PR for it! We are more than happy to guide you through the codebase so that you can put in a contribution to the codebase. Because pyjanitor is currently maintained by volunteers and has no fiscal support, any feature requests will be prioritized according to what maintainers encounter as a need in our day-to-day jobs. Please temper expectations accordingly.","title":"Feature requests"},{"location":"#api-policy","text":"pyjanitor only extends or aliases the pandas API (and other dataframe APIs), but will never fix or replace them. Undesirable pandas behaviour should be reported upstream in the pandas issue tracker . We explicitly do not fix the pandas API. If at some point the pandas devs decide to take something from pyjanitor and internalize it as part of the official pandas API, then we will deprecate it from pyjanitor , while acknowledging the original contributors' contribution as part of the official deprecation record.","title":"API Policy"},{"location":"#credits","text":"Test data for chemistry submodule can be found at Predictive Toxicology .","title":"Credits"},{"location":"AUTHORS/","text":"Contributors Once you have added your contribution to pyjanitor , please add your name using this markdown template: [@githubname](https://github.com/githubname) | [contributions](https://github.com/pyjanitor-devs/pyjanitor/issues?q=is%3Aclosed+mentions%3Agithubname) You can copy/paste the template and replace githubname with your username. Contributions that did not leave a commit trace are indicated in bullet points below each user's username. Leads @ericmjl | contributions @szuckerman | contributions @zbarry | contributions Co-led sprint at SciPy 2019. @hectormz | contributions @jk3587 | contributions Tagged issues at SciPy 2019. @sallyhong | contributions Tagged issues at SciPy 2019. @zjpoh | contributions Started pyspark sub-module. @anzelpwj | contributions @samukweku | contributions @loganthomas | contributions Helped others with git issues at SciPy 2019. @nvamsikrishna05 | contributions Contributors @JoshuaC3 | contributions @cduvallet | contributions @shantanuo | contributions @jcvall | contributions @CWen001 | contributions @bhallaY | contributions @jekwatt | contributions Helped other sprinters with git issues at SciPy 2019. @kurtispinkney | contributions @lphk92 | contributions @jonnybazookatone | contributions @SorenFrohlich | contributions @dave-frazzetto | contributions @dsouzadaniel | contributions @Eidhagen | contributions @mdini | contributions @kimt33 | contributions @jack-kessler-88 | user no longer found @NapsterInBlue | contributions @ricky-lim | contributions @catherinedevlin | contributions @StephenSchroed | contributions @Rajat-181 | contributions @dendrondal | contributions @rahosbach | contributions @asearfos | contributions @emnemnemnem | contributions @rebeccawperry | contributions @TomMonks | contributions @benjaminjack | contributions @kulini | contributions @dwgoltra | contributions @shandou | contributions @samwalkow | contributions @portc13 | contributions @DSNortsev | contributions @qtson | contributions @keoghdata | contributions @cjmayers | contributions @gjlynx | contributions @aopisco | contributions @gaworecki5 | contributions @puruckertom | contributions @thomasjpfan | contributions @jiafengkevinchen | contributions @mralbu | contributions @Ram-N | contributions @eyaltrabelsi | contributions @gddcunh | contributions @DollofCuty | contributions @bdice | contributions @evan-anderson | contributions @smu095 | contributions @VPerrollaz | contributions @UGuntupalli | contributions @mphirke | contributions @sauln | contributions @richardqiu | contributions @MinchinWeb | contributions @BaritoneBeard | contributions @Sousa8697 | contributions @MollyCroke | contributions @ericclessantostv | contributions @fireddd | contributions @Zeroto521 | contributions @thatlittleboy | contributions @robertmitchellv | contributions @Econundrums | contributions @ashenafiyb | contributions @gahjelle | contributions @ethompsy | contributions @apatao | contributions @OdinTech3 | contributions @asmirnov69 | contributions","title":"Authors"},{"location":"AUTHORS/#contributors","text":"Once you have added your contribution to pyjanitor , please add your name using this markdown template: [@githubname](https://github.com/githubname) | [contributions](https://github.com/pyjanitor-devs/pyjanitor/issues?q=is%3Aclosed+mentions%3Agithubname) You can copy/paste the template and replace githubname with your username. Contributions that did not leave a commit trace are indicated in bullet points below each user's username.","title":"Contributors"},{"location":"AUTHORS/#leads","text":"@ericmjl | contributions @szuckerman | contributions @zbarry | contributions Co-led sprint at SciPy 2019. @hectormz | contributions @jk3587 | contributions Tagged issues at SciPy 2019. @sallyhong | contributions Tagged issues at SciPy 2019. @zjpoh | contributions Started pyspark sub-module. @anzelpwj | contributions @samukweku | contributions @loganthomas | contributions Helped others with git issues at SciPy 2019. @nvamsikrishna05 | contributions","title":"Leads"},{"location":"AUTHORS/#contributors_1","text":"@JoshuaC3 | contributions @cduvallet | contributions @shantanuo | contributions @jcvall | contributions @CWen001 | contributions @bhallaY | contributions @jekwatt | contributions Helped other sprinters with git issues at SciPy 2019. @kurtispinkney | contributions @lphk92 | contributions @jonnybazookatone | contributions @SorenFrohlich | contributions @dave-frazzetto | contributions @dsouzadaniel | contributions @Eidhagen | contributions @mdini | contributions @kimt33 | contributions @jack-kessler-88 | user no longer found @NapsterInBlue | contributions @ricky-lim | contributions @catherinedevlin | contributions @StephenSchroed | contributions @Rajat-181 | contributions @dendrondal | contributions @rahosbach | contributions @asearfos | contributions @emnemnemnem | contributions @rebeccawperry | contributions @TomMonks | contributions @benjaminjack | contributions @kulini | contributions @dwgoltra | contributions @shandou | contributions @samwalkow | contributions @portc13 | contributions @DSNortsev | contributions @qtson | contributions @keoghdata | contributions @cjmayers | contributions @gjlynx | contributions @aopisco | contributions @gaworecki5 | contributions @puruckertom | contributions @thomasjpfan | contributions @jiafengkevinchen | contributions @mralbu | contributions @Ram-N | contributions @eyaltrabelsi | contributions @gddcunh | contributions @DollofCuty | contributions @bdice | contributions @evan-anderson | contributions @smu095 | contributions @VPerrollaz | contributions @UGuntupalli | contributions @mphirke | contributions @sauln | contributions @richardqiu | contributions @MinchinWeb | contributions @BaritoneBeard | contributions @Sousa8697 | contributions @MollyCroke | contributions @ericclessantostv | contributions @fireddd | contributions @Zeroto521 | contributions @thatlittleboy | contributions @robertmitchellv | contributions @Econundrums | contributions @ashenafiyb | contributions @gahjelle | contributions @ethompsy | contributions @apatao | contributions @OdinTech3 | contributions @asmirnov69 | contributions","title":"Contributors"},{"location":"CHANGELOG/","text":"Changelog Unreleased [ENH] Add lazy imports to speed up the time taken to load pyjanitor (part 2) [DOC] Updated developer guide docs. [ENH] Allow column selection/renaming within conditional_join. Issue #1102. Also allow first or last match. Issue #1020 @samukweku. [ENH] New decorator deprecated_kwargs for breaking API. #1103 @Zeroto521 [ENH] Extend select_columns to support non-string columns. Issue #1105 @samukweku [ENH] Performance improvement for groupby_topk. Issue #1093 @samukweku [ENH] min_max_scale drop old_min and old_max to fit sklearn's method API. Issue #1068 @Zeroto521 [ENH] Add jointly option for min_max_scale support to transform each column values or entire values. Default transform each column, similar behavior to sklearn.preprocessing.MinMaxScaler . (Issue #1067, PR #1112, PR #1123) @Zeroto521 [INF] Require pyspark minimal version is v3.2.0 to cut duplicates codes. Issue #1110 @Zeroto521 [ENH] Add support for extension arrays in expand_grid . Issue #1121 @samukweku [ENH] Add names_expand and index_expand parameters to pivot_wider for exposing missing categoricals. Issue #1108 @samukweku [ENH] Add fix for slicing error when selecting columns in pivot_wider . Issue #1134 @samukweku [ENH] dropna parameter added to pivot_longer . Issue #1132 @samukweku [INF] Update mkdocstrings version and to fit its new coming features. PR #1138 @Zeroto521 [BUG] Force math.softmax returning Series . PR #1139 @Zeroto521 [INF] Set independent environment for building documentation. PR #1141 @Zeroto521 [DOC] Add local documentation preview via github action artifact. PR #1149 @Zeroto521 [ENH] Enable encode_categorical handle 2 (or more ) dimensions array. PR #1153 @Zeroto521 [TST] Fix testcases failing on Window. Issue #1160 @Zeroto521, and @samukweku [INF] Cancel old workflow runs via Github Action concurrency . PR #1161 @Zeroto521 [ENH] Faster computation for non-equi join, with a numba engine. Speed improvement for left/right joins when sort_by_appearance is False. Issue #1102 @samukweku [BUG] Avoid change_type mutating original DataFrame . PR #1162 @Zeroto521 [ENH] The parameter column_name of change_type totally supports inputing multi-column now. #1163 @Zeroto521 [ENH] Fix error when sort_by_appearance=True is combined with dropna=True . Issue #1168 @samukweku [ENH] Add explicit default parameter to case_when function. Issue #1159 @samukweku [BUG] pandas 1.5.x _MergeOperation doesn't have copy keyword anymore. Issue #1174 @Zeroto521 [ENH] select_rows function added for flexible row selection. Generic select function added as well. Add support for MultiIndex selection via dictionary. Issue #1124 @samukweku [TST] Compat with macos and window, to fix FailedHealthCheck Issue #1181 @Zeroto521 [INF] Merge two docs CIs ( docs-preview.yml and docs.yml ) to one. And add documentation pytest mark. PR #1183 @Zeroto521 [INF] Merge codecov.yml (only works for the dev branch pushing event) into tests.yml (only works for PR event). PR #1185 @Zeroto521 [TST] Fix failure for test/timeseries/test_fill_missing_timestamp. Issue #1184 @samukweku [BUG] Import DataDescription to fix: AttributeError: 'DataFrame' object has no attribute 'data_description' . PR #1191 @Zeroto521 v0.23.1 - 2022-05-03 [DOC] Updated fill.py and update_where.py documentation with working examples. [ENH] Deprecate num_bins from bin_numeric in favour of bins , and allow generic **kwargs to be passed into pd.cut . Issue #969. @thatlittleboy [ENH] Fix concatenate_columns not working on category inputs @zbarry [INF] Simplify CI system @ericmjl [ENH] Added \"read_commandline\" function to janitor.io @BaritoneBeard [BUG] Fix bug with the complement parameter of filter_on . Issue #988. @thatlittleboy [ENH] Add xlsx_table , for reading tables from an Excel sheet. @samukweku [ENH] minor improvements for conditional_join; equality only joins are no longer supported; there has to be at least one non-equi join present. @samukweku [BUG] sort_column_value_order no longer mutates original dataframe. [BUG] Extend fill_empty 's column_names type range. Issue #998. @Zeroto521 [BUG] Removed/updated error-inducing default arguments in row_to_names (#1004) and round_to_fraction (#1005). @thatlittleboy [ENH] patterns deprecated in favour of importing re.compile . #1007 @samukweku [ENH] Changes to kwargs in encode_categorical , where the values can either be a string or a 1D array. #1021 @samukweku [ENH] Add fill_value and explicit parameters to the complete function. #1019 @samukweku [ENH] Performance improvement for expand_grid . @samukweku [BUG] Make factorize_columns (PR #1028) and truncate_datetime_dataframe (PR #1040) functions non-mutating. @thatlittleboy [BUG] Fix SettingWithCopyWarning and other minor bugs when using truncate_datetime_dataframe , along with further performance improvements (PR #1040). @thatlittleboy [ENH] Performance improvement for conditional_join . @samukweku [ENH] Multiple .value is now supported in pivot_longer . Multiple values_to is also supported, when names_pattern is a list or tuple. names_transform parameter added, for efficient dtype transformation of unpivoted columns. #1034, #1048, #1051 @samukweku [ENH] Add xlsx_cells for reading a spreadsheet as a table of individual cells. #929 @samukweku. [ENH] Let filter_string suit parameters of Series.str.contains Issue #1003 and #1047. @Zeroto521 [ENH] names_glue in pivot_wider now takes a string form, using str.format_map under the hood. levels_order is also deprecated. @samukweku [BUG] Fixed bug in transform_columns which ignored the column_names specification when new_column_names dictionary was provided as an argument, issue #1063. @thatlittleboy [BUG] count_cumulative_unique no longer modifies the column being counted in the output when case_sensitive argument is set to False, issue #1065. @thatlittleboy [BUG] Fix for gcc missing error in dev container [DOC] Added a step in the dev guide to install Remote Container in VS Code. @ashenafiyb [DOC] Convert expand_column and find_replace code examples to doctests, issue #972. @gahjelle [DOC] Convert expand_column code examples to doctests, issue #972. @gahjelle [DOC] Convert get_dupes code examples to doctests, issue #972. @ethompsy [DOC] Convert engineering code examples to doctests, issue #972 @ashenafiyb [DOC] Convert groupby_topk code examples to doctests, issue #972. @ethompsy [DOC] Add doctests to math , issue #972. @gahjelle [DOC] Add doctests to math and ml , issue #972. @gahjelle [DOC] Add doctests to math , ml , and xarray , issue #972. @gahjelle v0.22.0 - 2021-11-21 [BUG] Fix conditional join issue for multiple conditions, where pd.eval fails to evaluate if numexpr is installed. #898 @samukweku [ENH] Added case_when to handle multiple conditionals and replacement values. Issue #736. @robertmitchellv [ENH] Deprecate new_column_names and merge_frame from process_text . Only existing columns are supported. @samukweku [ENH] complete uses pd.merge internally, providing a simpler logic, with some speed improvements in certain cases over pd.reindex . @samukweku [ENH] expand_grid returns a MultiIndex DataFrame, allowing the user to decide how to manipulate the columns. @samukweku [INF] Simplify a bit linting, use pre-commit as the CI linting checker. @Zeroto521 [ENH] Fix bug in pivot_longer for wrong output when names_pattern is a sequence with a single value. Issue #885 @samukweku [ENH] Deprecate aggfunc from pivot_wider ; aggregation can be chained with pandas' groupby . [ENH] As_Categorical deprecated from encode_categorical ; a tuple of (categories, order) suffices for **kwargs. @samukweku [ENH] Deprecate names_sort from pivot_wider .@samukweku [ENH] Add softmax to math module. Issue #902. @loganthomas v0.21.2 - 2021-09-01 [ENH] Fix warning message in coalesce , from bfill/fill; coalesce now uses variable arguments. Issue #882 @samukweku [INF] Add SciPy as explicit dependency in base.in . Issue #895 @ericmjl v0.21.1 - 2021-08-29 [DOC] Fix references and broken links in AUTHORS.rst. @loganthomas [DOC] Updated Broken links in the README and contributing docs. @nvamsikrishna05 [INF] Update pre-commit hooks and remove mutable references. Issue #844. @loganthomas [INF] Add GitHub Release pointer to auto-release script. Issue #818. @loganthomas [INF] Updated black version in github actions code-checks to match pre-commit hooks. @nvamsikrishna05 [ENH] Add reset_index flag to row_to_names function. @fireddd [ENH] Updated label_encode to use pandas factorize instead of scikit-learn LabelEncoder. @nvamsikrishna05 [INF] Removed the scikit-learn package from the dependencies from environment-dev.yml and base.in files. @nvamsikrishna05 [ENH] Add function to remove constant columns. @fireddd [ENH] Added factorize_columns method which will deprecate the label_encode method in future release. @nvamsikrishna05 [DOC] Delete Read the Docs project and remove all readthedocs.io references from the repo. Issue #863. @loganthomas [DOC] Updated various documentation sources to reflect pyjanitor-dev ownership. @loganthomas [INF] Fix isort automatic checks. Issue #845. @loganthomas [ENH] complete function now uses variable args (*args) - @samukweku [ENH] Set expand_column 's sep default is \"|\" , same to pandas.Series.str.get_dummies . Issue #876. @Zeroto521 [ENH] Deprecate limit from fill_direction. fill_direction now uses kwargs. @samukweku [ENH] Added conditional_join function that supports joins on non-equi operators. @samukweku [INF] Speed up pytest via -n (pytest-xdist) option. Issue #881. @Zeroto521 [DOC] Add list mark to keep select_columns 's example same style. @Zeroto521 [ENH] Updated rename_columns to take optional function argument for mapping. @nvamsikrishna05 v0.21.0 - 2021-07-16 [ENH] Drop fill_value parameter from complete . Users can use fillna instead. @samukweku [BUG] Fix bug in pivot_longer with single level columns. @samukweku [BUG] Disable exchange rates API until we can find another one to hit. @ericmjl [ENH] Change coalesce to return columns; also use bfill , ffill , which is faster than combine_first @samukweku [ENH] Use eval for string conditions in update_where . @samukweku [ENH] Add clearer error messages for pivot_longer . h/t to @tdhock for the observation. Issue #836 @samukweku [ENH] select_columns now uses variable arguments (*args), to provide a simpler selection without the need for lists. - @samukweku [ENH] encode_categoricals refactored to use generic functions via functools.dispatch . - @samukweku [ENH] Updated convert_excel_date to throw meaningful error when values contain non-numeric. @nvamsikrishna05 v0.20.14 - 2021-03-25 [ENH] Add dropna parameter to groupby_agg. @samukweku [ENH] complete adds a by parameter to expose explicit missing values per group, via groupby. @samukweku [ENH] Fix check_column to support single inputs - fixes label_encode . @zbarry v0.20.13 - 2021-02-25 [ENH] Performance improvements to expand_grid . @samukweku [HOTFIX] Add multipledispatch to pip requirements. @ericmjl v0.20.12 - 2021-02-25 [INF] Auto-release GitHub action maintenance. @loganthomas v0.20.11 - 2021-02-24 [INF] Setup auto-release GitHub action. @loganthomas [INF] Deploy darglint package for docstring linting. Issue #745. @loganthomas [ENH] Added optional truncation to clean_names function. Issue #753. @richardqiu [ENH] Added timeseries.flag_jumps() function. Issue #711. @loganthomas [ENH] pivot_longer can handle multiple values in paired columns, and can reshape using a list/tuple of regular expressions in names_pattern . @samukweku [ENH] Replaced default numeric conversion of dataframe with a dtypes parameter, allowing the user to control the data types. - @samukweku [INF] Loosen dependency specifications. Switch to pip-tools for managing dependencies. Issue #760. @MinchinWeb [DOC] added pipenv installation instructions @evan-anderson [ENH] Add pivot_wider function, which is the inverse of the pivot_longer function. @samukweku [INF] Add openpyxl to environment-dev.yml . @samukweku [ENH] Reduce code by reusing existing functions for fill_direction. @samukweku [ENH] Improvements to pivot_longer function, with improved speed and cleaner code. dtypes parameter dropped; user can change dtypes with pandas' astype method, or pyjanitor's change_type method. @samukweku [ENH] Add kwargs to encode_categorical function, to create ordered categorical columns, or categorical columns with explicit categories. @samukweku [ENH] Improvements to complete method. Use pd.merge to handle duplicates and null values. @samukweku [ENH] Add new_column_names parameter to process_text , allowing a user to create a new column name after processing a text column. Also added a merge_frame parameter, allowing dataframe merging, if the result of the text processing is a dataframe.@samukweku [ENH] Add aggfunc parameter to pivot_wider. @samukweku [ENH] Modified the check function in utils to verify if a value is a callable. @samukweku [ENH] Add a base _select_column function, using functools.singledispatch , to allow for flexible columns selection. @samukweku [ENH] pivot_longer and pivot_wider now support janitor.select_columns syntax, allowing for more flexible and dynamic column selection. @samukweku v0.20.10 [ENH] Added function sort_timestamps_monotonically to timeseries functions @UGuntupalli [ENH] Added the complete function for converting implicit missing values to explicit ones. @samukweku [ENH] Further simplification of expand_grid. @samukweku [BUGFIX] Added copy() method to original dataframe, to avoid mutation. Issue #729. @samukweku [ENH] Added also method for running functions in chain with no return values. [DOC] Added a timeseries module section to website docs. Issue #742. @loganthomas [ENH] Added a pivot_longer function, a wrapper around pd.melt and similar to tidyr's pivot_longer function. Also added an example notebook. @samukweku [ENH] Fixed code to returns error if fill_value is not a dictionary. @samukweku [INF] Welcome bot (.github/config.yml) for new users added. Issue #739. @samukweku v0.20.9 [ENH] Updated groupby_agg function to account for null entries in the by argument. @samukweku [ENH] Added function groupby_topk to janitor functions @mphirke v0.20.8 [ENH] Upgraded update_where function to use either the pandas query style, or boolean indexing via the loc method. Also updated find_replace function to use the loc method directly, instead of routing it through the update_where function. @samukweku [INF] Update pandas minimum version to 1.0.0. @hectormz [DOC] Updated the general functions API page to show all available functions. @samukweku [DOC] Fix the few lacking type annotations of functions. @VPerrollaz [DOC] Changed the signature from str to Optional[str] when initialized by None. @VPerrollaz [DOC] Add the Optional type for all signatures of the API. @VPerrollaz [TST] Updated test_expand_grid to account for int dtype difference in Windows OS @samukweku [TST] Make importing pandas testing functions follow uniform pattern. @hectormz [ENH] Added process_text wrapper function for all Pandas string methods. @samukweku [TST] Only skip tests for non-installed libraries on local machine. @hectormz [DOC] Fix minor issues in documentation. @hectormz [ENH] Added fill_direction function for forward/backward fills on missing values for selected columns in a dataframe. @samukweku [ENH] Simpler logic and less lines of code for expand_grid function @samukweku v0.20.7 [TST] Add a test for transform_column to check for nonmutation. @VPerrollaz [ENH] Contributed expand_grid function by @samukweku v0.20.6 [DOC] Pep8 all examples. @VPerrollaz [TST] Add docstrings to tests @hectormz [INF] Add debug-statements , requirements-txt-fixer , and interrogate to pre-commit . @hectormz [ENH] Upgraded transform_column to use df.assign underneath the hood, and also added option to transform column elementwise (via apply) or columnwise (thus operating on a series). @ericmjl v0.20.5 [INF] Replace pycodestyle with flake8 in order to add pandas-vet linter @hectormz [ENH] select_columns() now raises NameError if column label in search_columns_labels is missing from DataFrame columns. @smu095 v0.20.1 [DOC] Added an example for groupby_agg in general functions @samukweku [ENH] Contributed sort_naturally() function. @ericmjl v0.20.0 [DOC] Edited transform_column dest_column_name kwarg description to be clearer on defaults by @evan-anderson. [ENH] Replace apply() in favor of pandas functions in several functions. @hectormz [ENH] Add ecdf() Series function by @ericmjl. [DOC] Update API policy for clarity. @ericmjl [ENH] Enforce string conversion when cleaning names. @ericmjl [ENH] Change find_replace implementation to use keyword arguments to specify columns to perform find and replace on. @ericmjl [ENH] Add jitter() dataframe function by @rahosbach v0.19.0 [ENH] Add xarray support and clone_using / convert_datetime_to_number funcs by @zbarry. v0.18.3 [ENH] Series toset() functionality #570 @eyaltrabelsi [ENH] Added option to coalesce function to not delete coalesced columns. @gddcunh [ENH] Added functionality to deconcatenate tuple/list/collections in a column to deconcatenate_column @zbarry [ENH] Fix error message when length of new_column_names is wrong @DollofCutty [DOC] Fixed several examples of functional syntax in functions.py . @bdice [DOC] Fix #noqa comments showing up in docs by @hectormz [ENH] Add unionizing a group of dataframes' categoricals. @zbarry [DOC] Fix contributions hyperlinks in AUTHORS.rst and contributions by @hectormz [INF] Add pre-commit hooks to repository by @ericmjl [DOC] Fix formatting code in CONTRIBUTING.rst by @hectormz [DOC] Changed the typing for most \"column_name(s)\" to Hashable rather than enforcing strings, to more closely match Pandas API by @dendrondal [INF] Edited pycodestyle and Black parameters to avoid venvs by @dendrondal v0.18.2 [INF] Make requirements.txt smaller @eyaltrabelsi [ENH] Add a reset_index parameter to shuffle @eyaltrabelsi [DOC] Added contribution page link to readme @eyaltrabelsi [DOC] fix example for update_where , provide a bit more detail, and expand the bad_values example notebook to demonstrate its use by @anzelpwj. [INF] Fix pytest marks by @ericmjl (issue #520) [ENH] add example notebook with use of finance submodule methods by @rahosbach [DOC] added a couple of admonitions for Windows users. h/t @anzelpwj for debugging help when a few tests failed for win32 @Ram-N [ENH] Pyjanitor for PySpark @zjpoh [ENH] Add pyspark clean_names @zjpoh [ENH] Convert asserts to raise exceptions by @hectormz [ENH] Add decorator functions for missing and error handling @jiafengkevinchen [DOC] Update README with functional pandas API example. @ericmjl [INF] Move get_features_targets() to new ml.py module by @hectormz [ENH] Add chirality to morgan fingerprints in janitor.chemistry submodule by @Clayton-Springer [INF] import_message suggests python dist. appropriate installs by @hectormz [ENH] Add count_cumulative_unique() method to janitor.functions submodule by @rahosbach [ENH] Add update_where() method to janitor.spark.functions submodule by @zjpoh v0.18.1 [ENH] extend find_replace functionality to allow both exact match and regular-expression-based fuzzy match by @shandou [ENH] add preserve_position kwarg to deconcatenate_column with tests by @shandou and @ericmjl [DOC] add contributions that did not leave git traces by @ericmjl [ENH] add inflation adjustment in finance submodule by @rahosbach [DOC] clarified how new functions should be implemented by @shandou [ENH] add optional removal of accents on functions.clean_names, enabled by default by @mralbu [ENH] add camelCase conversion to snake_case on clean_names by @ericmjl, h/t @jtaylor for sharing original [ENH] Added null_flag function which can mark null values in rows. Implemented by @anzelpwj [ENH] add engineering submodule with unit conversion method by @rahosbach [DOC] add PyPI project description [ENH] add example notebook with use of finance submodule methods by @rahosbach For changes that happened prior to v0.18.1, please consult the closed PRs, which can be found here . We thank all contributors who have helped make pyjanitor the package that it is today.","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CHANGELOG/#unreleased","text":"[ENH] Add lazy imports to speed up the time taken to load pyjanitor (part 2) [DOC] Updated developer guide docs. [ENH] Allow column selection/renaming within conditional_join. Issue #1102. Also allow first or last match. Issue #1020 @samukweku. [ENH] New decorator deprecated_kwargs for breaking API. #1103 @Zeroto521 [ENH] Extend select_columns to support non-string columns. Issue #1105 @samukweku [ENH] Performance improvement for groupby_topk. Issue #1093 @samukweku [ENH] min_max_scale drop old_min and old_max to fit sklearn's method API. Issue #1068 @Zeroto521 [ENH] Add jointly option for min_max_scale support to transform each column values or entire values. Default transform each column, similar behavior to sklearn.preprocessing.MinMaxScaler . (Issue #1067, PR #1112, PR #1123) @Zeroto521 [INF] Require pyspark minimal version is v3.2.0 to cut duplicates codes. Issue #1110 @Zeroto521 [ENH] Add support for extension arrays in expand_grid . Issue #1121 @samukweku [ENH] Add names_expand and index_expand parameters to pivot_wider for exposing missing categoricals. Issue #1108 @samukweku [ENH] Add fix for slicing error when selecting columns in pivot_wider . Issue #1134 @samukweku [ENH] dropna parameter added to pivot_longer . Issue #1132 @samukweku [INF] Update mkdocstrings version and to fit its new coming features. PR #1138 @Zeroto521 [BUG] Force math.softmax returning Series . PR #1139 @Zeroto521 [INF] Set independent environment for building documentation. PR #1141 @Zeroto521 [DOC] Add local documentation preview via github action artifact. PR #1149 @Zeroto521 [ENH] Enable encode_categorical handle 2 (or more ) dimensions array. PR #1153 @Zeroto521 [TST] Fix testcases failing on Window. Issue #1160 @Zeroto521, and @samukweku [INF] Cancel old workflow runs via Github Action concurrency . PR #1161 @Zeroto521 [ENH] Faster computation for non-equi join, with a numba engine. Speed improvement for left/right joins when sort_by_appearance is False. Issue #1102 @samukweku [BUG] Avoid change_type mutating original DataFrame . PR #1162 @Zeroto521 [ENH] The parameter column_name of change_type totally supports inputing multi-column now. #1163 @Zeroto521 [ENH] Fix error when sort_by_appearance=True is combined with dropna=True . Issue #1168 @samukweku [ENH] Add explicit default parameter to case_when function. Issue #1159 @samukweku [BUG] pandas 1.5.x _MergeOperation doesn't have copy keyword anymore. Issue #1174 @Zeroto521 [ENH] select_rows function added for flexible row selection. Generic select function added as well. Add support for MultiIndex selection via dictionary. Issue #1124 @samukweku [TST] Compat with macos and window, to fix FailedHealthCheck Issue #1181 @Zeroto521 [INF] Merge two docs CIs ( docs-preview.yml and docs.yml ) to one. And add documentation pytest mark. PR #1183 @Zeroto521 [INF] Merge codecov.yml (only works for the dev branch pushing event) into tests.yml (only works for PR event). PR #1185 @Zeroto521 [TST] Fix failure for test/timeseries/test_fill_missing_timestamp. Issue #1184 @samukweku [BUG] Import DataDescription to fix: AttributeError: 'DataFrame' object has no attribute 'data_description' . PR #1191 @Zeroto521","title":"Unreleased"},{"location":"CHANGELOG/#v0231-2022-05-03","text":"[DOC] Updated fill.py and update_where.py documentation with working examples. [ENH] Deprecate num_bins from bin_numeric in favour of bins , and allow generic **kwargs to be passed into pd.cut . Issue #969. @thatlittleboy [ENH] Fix concatenate_columns not working on category inputs @zbarry [INF] Simplify CI system @ericmjl [ENH] Added \"read_commandline\" function to janitor.io @BaritoneBeard [BUG] Fix bug with the complement parameter of filter_on . Issue #988. @thatlittleboy [ENH] Add xlsx_table , for reading tables from an Excel sheet. @samukweku [ENH] minor improvements for conditional_join; equality only joins are no longer supported; there has to be at least one non-equi join present. @samukweku [BUG] sort_column_value_order no longer mutates original dataframe. [BUG] Extend fill_empty 's column_names type range. Issue #998. @Zeroto521 [BUG] Removed/updated error-inducing default arguments in row_to_names (#1004) and round_to_fraction (#1005). @thatlittleboy [ENH] patterns deprecated in favour of importing re.compile . #1007 @samukweku [ENH] Changes to kwargs in encode_categorical , where the values can either be a string or a 1D array. #1021 @samukweku [ENH] Add fill_value and explicit parameters to the complete function. #1019 @samukweku [ENH] Performance improvement for expand_grid . @samukweku [BUG] Make factorize_columns (PR #1028) and truncate_datetime_dataframe (PR #1040) functions non-mutating. @thatlittleboy [BUG] Fix SettingWithCopyWarning and other minor bugs when using truncate_datetime_dataframe , along with further performance improvements (PR #1040). @thatlittleboy [ENH] Performance improvement for conditional_join . @samukweku [ENH] Multiple .value is now supported in pivot_longer . Multiple values_to is also supported, when names_pattern is a list or tuple. names_transform parameter added, for efficient dtype transformation of unpivoted columns. #1034, #1048, #1051 @samukweku [ENH] Add xlsx_cells for reading a spreadsheet as a table of individual cells. #929 @samukweku. [ENH] Let filter_string suit parameters of Series.str.contains Issue #1003 and #1047. @Zeroto521 [ENH] names_glue in pivot_wider now takes a string form, using str.format_map under the hood. levels_order is also deprecated. @samukweku [BUG] Fixed bug in transform_columns which ignored the column_names specification when new_column_names dictionary was provided as an argument, issue #1063. @thatlittleboy [BUG] count_cumulative_unique no longer modifies the column being counted in the output when case_sensitive argument is set to False, issue #1065. @thatlittleboy [BUG] Fix for gcc missing error in dev container [DOC] Added a step in the dev guide to install Remote Container in VS Code. @ashenafiyb [DOC] Convert expand_column and find_replace code examples to doctests, issue #972. @gahjelle [DOC] Convert expand_column code examples to doctests, issue #972. @gahjelle [DOC] Convert get_dupes code examples to doctests, issue #972. @ethompsy [DOC] Convert engineering code examples to doctests, issue #972 @ashenafiyb [DOC] Convert groupby_topk code examples to doctests, issue #972. @ethompsy [DOC] Add doctests to math , issue #972. @gahjelle [DOC] Add doctests to math and ml , issue #972. @gahjelle [DOC] Add doctests to math , ml , and xarray , issue #972. @gahjelle","title":"v0.23.1 - 2022-05-03"},{"location":"CHANGELOG/#v0220-2021-11-21","text":"[BUG] Fix conditional join issue for multiple conditions, where pd.eval fails to evaluate if numexpr is installed. #898 @samukweku [ENH] Added case_when to handle multiple conditionals and replacement values. Issue #736. @robertmitchellv [ENH] Deprecate new_column_names and merge_frame from process_text . Only existing columns are supported. @samukweku [ENH] complete uses pd.merge internally, providing a simpler logic, with some speed improvements in certain cases over pd.reindex . @samukweku [ENH] expand_grid returns a MultiIndex DataFrame, allowing the user to decide how to manipulate the columns. @samukweku [INF] Simplify a bit linting, use pre-commit as the CI linting checker. @Zeroto521 [ENH] Fix bug in pivot_longer for wrong output when names_pattern is a sequence with a single value. Issue #885 @samukweku [ENH] Deprecate aggfunc from pivot_wider ; aggregation can be chained with pandas' groupby . [ENH] As_Categorical deprecated from encode_categorical ; a tuple of (categories, order) suffices for **kwargs. @samukweku [ENH] Deprecate names_sort from pivot_wider .@samukweku [ENH] Add softmax to math module. Issue #902. @loganthomas","title":"v0.22.0 - 2021-11-21"},{"location":"CHANGELOG/#v0212-2021-09-01","text":"[ENH] Fix warning message in coalesce , from bfill/fill; coalesce now uses variable arguments. Issue #882 @samukweku [INF] Add SciPy as explicit dependency in base.in . Issue #895 @ericmjl","title":"v0.21.2 - 2021-09-01"},{"location":"CHANGELOG/#v0211-2021-08-29","text":"[DOC] Fix references and broken links in AUTHORS.rst. @loganthomas [DOC] Updated Broken links in the README and contributing docs. @nvamsikrishna05 [INF] Update pre-commit hooks and remove mutable references. Issue #844. @loganthomas [INF] Add GitHub Release pointer to auto-release script. Issue #818. @loganthomas [INF] Updated black version in github actions code-checks to match pre-commit hooks. @nvamsikrishna05 [ENH] Add reset_index flag to row_to_names function. @fireddd [ENH] Updated label_encode to use pandas factorize instead of scikit-learn LabelEncoder. @nvamsikrishna05 [INF] Removed the scikit-learn package from the dependencies from environment-dev.yml and base.in files. @nvamsikrishna05 [ENH] Add function to remove constant columns. @fireddd [ENH] Added factorize_columns method which will deprecate the label_encode method in future release. @nvamsikrishna05 [DOC] Delete Read the Docs project and remove all readthedocs.io references from the repo. Issue #863. @loganthomas [DOC] Updated various documentation sources to reflect pyjanitor-dev ownership. @loganthomas [INF] Fix isort automatic checks. Issue #845. @loganthomas [ENH] complete function now uses variable args (*args) - @samukweku [ENH] Set expand_column 's sep default is \"|\" , same to pandas.Series.str.get_dummies . Issue #876. @Zeroto521 [ENH] Deprecate limit from fill_direction. fill_direction now uses kwargs. @samukweku [ENH] Added conditional_join function that supports joins on non-equi operators. @samukweku [INF] Speed up pytest via -n (pytest-xdist) option. Issue #881. @Zeroto521 [DOC] Add list mark to keep select_columns 's example same style. @Zeroto521 [ENH] Updated rename_columns to take optional function argument for mapping. @nvamsikrishna05","title":"v0.21.1 - 2021-08-29"},{"location":"CHANGELOG/#v0210-2021-07-16","text":"[ENH] Drop fill_value parameter from complete . Users can use fillna instead. @samukweku [BUG] Fix bug in pivot_longer with single level columns. @samukweku [BUG] Disable exchange rates API until we can find another one to hit. @ericmjl [ENH] Change coalesce to return columns; also use bfill , ffill , which is faster than combine_first @samukweku [ENH] Use eval for string conditions in update_where . @samukweku [ENH] Add clearer error messages for pivot_longer . h/t to @tdhock for the observation. Issue #836 @samukweku [ENH] select_columns now uses variable arguments (*args), to provide a simpler selection without the need for lists. - @samukweku [ENH] encode_categoricals refactored to use generic functions via functools.dispatch . - @samukweku [ENH] Updated convert_excel_date to throw meaningful error when values contain non-numeric. @nvamsikrishna05","title":"v0.21.0 - 2021-07-16"},{"location":"CHANGELOG/#v02014-2021-03-25","text":"[ENH] Add dropna parameter to groupby_agg. @samukweku [ENH] complete adds a by parameter to expose explicit missing values per group, via groupby. @samukweku [ENH] Fix check_column to support single inputs - fixes label_encode . @zbarry","title":"v0.20.14 - 2021-03-25"},{"location":"CHANGELOG/#v02013-2021-02-25","text":"[ENH] Performance improvements to expand_grid . @samukweku [HOTFIX] Add multipledispatch to pip requirements. @ericmjl","title":"v0.20.13 - 2021-02-25"},{"location":"CHANGELOG/#v02012-2021-02-25","text":"[INF] Auto-release GitHub action maintenance. @loganthomas","title":"v0.20.12 - 2021-02-25"},{"location":"CHANGELOG/#v02011-2021-02-24","text":"[INF] Setup auto-release GitHub action. @loganthomas [INF] Deploy darglint package for docstring linting. Issue #745. @loganthomas [ENH] Added optional truncation to clean_names function. Issue #753. @richardqiu [ENH] Added timeseries.flag_jumps() function. Issue #711. @loganthomas [ENH] pivot_longer can handle multiple values in paired columns, and can reshape using a list/tuple of regular expressions in names_pattern . @samukweku [ENH] Replaced default numeric conversion of dataframe with a dtypes parameter, allowing the user to control the data types. - @samukweku [INF] Loosen dependency specifications. Switch to pip-tools for managing dependencies. Issue #760. @MinchinWeb [DOC] added pipenv installation instructions @evan-anderson [ENH] Add pivot_wider function, which is the inverse of the pivot_longer function. @samukweku [INF] Add openpyxl to environment-dev.yml . @samukweku [ENH] Reduce code by reusing existing functions for fill_direction. @samukweku [ENH] Improvements to pivot_longer function, with improved speed and cleaner code. dtypes parameter dropped; user can change dtypes with pandas' astype method, or pyjanitor's change_type method. @samukweku [ENH] Add kwargs to encode_categorical function, to create ordered categorical columns, or categorical columns with explicit categories. @samukweku [ENH] Improvements to complete method. Use pd.merge to handle duplicates and null values. @samukweku [ENH] Add new_column_names parameter to process_text , allowing a user to create a new column name after processing a text column. Also added a merge_frame parameter, allowing dataframe merging, if the result of the text processing is a dataframe.@samukweku [ENH] Add aggfunc parameter to pivot_wider. @samukweku [ENH] Modified the check function in utils to verify if a value is a callable. @samukweku [ENH] Add a base _select_column function, using functools.singledispatch , to allow for flexible columns selection. @samukweku [ENH] pivot_longer and pivot_wider now support janitor.select_columns syntax, allowing for more flexible and dynamic column selection. @samukweku","title":"v0.20.11 - 2021-02-24"},{"location":"CHANGELOG/#v02010","text":"[ENH] Added function sort_timestamps_monotonically to timeseries functions @UGuntupalli [ENH] Added the complete function for converting implicit missing values to explicit ones. @samukweku [ENH] Further simplification of expand_grid. @samukweku [BUGFIX] Added copy() method to original dataframe, to avoid mutation. Issue #729. @samukweku [ENH] Added also method for running functions in chain with no return values. [DOC] Added a timeseries module section to website docs. Issue #742. @loganthomas [ENH] Added a pivot_longer function, a wrapper around pd.melt and similar to tidyr's pivot_longer function. Also added an example notebook. @samukweku [ENH] Fixed code to returns error if fill_value is not a dictionary. @samukweku [INF] Welcome bot (.github/config.yml) for new users added. Issue #739. @samukweku","title":"v0.20.10"},{"location":"CHANGELOG/#v0209","text":"[ENH] Updated groupby_agg function to account for null entries in the by argument. @samukweku [ENH] Added function groupby_topk to janitor functions @mphirke","title":"v0.20.9"},{"location":"CHANGELOG/#v0208","text":"[ENH] Upgraded update_where function to use either the pandas query style, or boolean indexing via the loc method. Also updated find_replace function to use the loc method directly, instead of routing it through the update_where function. @samukweku [INF] Update pandas minimum version to 1.0.0. @hectormz [DOC] Updated the general functions API page to show all available functions. @samukweku [DOC] Fix the few lacking type annotations of functions. @VPerrollaz [DOC] Changed the signature from str to Optional[str] when initialized by None. @VPerrollaz [DOC] Add the Optional type for all signatures of the API. @VPerrollaz [TST] Updated test_expand_grid to account for int dtype difference in Windows OS @samukweku [TST] Make importing pandas testing functions follow uniform pattern. @hectormz [ENH] Added process_text wrapper function for all Pandas string methods. @samukweku [TST] Only skip tests for non-installed libraries on local machine. @hectormz [DOC] Fix minor issues in documentation. @hectormz [ENH] Added fill_direction function for forward/backward fills on missing values for selected columns in a dataframe. @samukweku [ENH] Simpler logic and less lines of code for expand_grid function @samukweku","title":"v0.20.8"},{"location":"CHANGELOG/#v0207","text":"[TST] Add a test for transform_column to check for nonmutation. @VPerrollaz [ENH] Contributed expand_grid function by @samukweku","title":"v0.20.7"},{"location":"CHANGELOG/#v0206","text":"[DOC] Pep8 all examples. @VPerrollaz [TST] Add docstrings to tests @hectormz [INF] Add debug-statements , requirements-txt-fixer , and interrogate to pre-commit . @hectormz [ENH] Upgraded transform_column to use df.assign underneath the hood, and also added option to transform column elementwise (via apply) or columnwise (thus operating on a series). @ericmjl","title":"v0.20.6"},{"location":"CHANGELOG/#v0205","text":"[INF] Replace pycodestyle with flake8 in order to add pandas-vet linter @hectormz [ENH] select_columns() now raises NameError if column label in search_columns_labels is missing from DataFrame columns. @smu095","title":"v0.20.5"},{"location":"CHANGELOG/#v0201","text":"[DOC] Added an example for groupby_agg in general functions @samukweku [ENH] Contributed sort_naturally() function. @ericmjl","title":"v0.20.1"},{"location":"CHANGELOG/#v0200","text":"[DOC] Edited transform_column dest_column_name kwarg description to be clearer on defaults by @evan-anderson. [ENH] Replace apply() in favor of pandas functions in several functions. @hectormz [ENH] Add ecdf() Series function by @ericmjl. [DOC] Update API policy for clarity. @ericmjl [ENH] Enforce string conversion when cleaning names. @ericmjl [ENH] Change find_replace implementation to use keyword arguments to specify columns to perform find and replace on. @ericmjl [ENH] Add jitter() dataframe function by @rahosbach","title":"v0.20.0"},{"location":"CHANGELOG/#v0190","text":"[ENH] Add xarray support and clone_using / convert_datetime_to_number funcs by @zbarry.","title":"v0.19.0"},{"location":"CHANGELOG/#v0183","text":"[ENH] Series toset() functionality #570 @eyaltrabelsi [ENH] Added option to coalesce function to not delete coalesced columns. @gddcunh [ENH] Added functionality to deconcatenate tuple/list/collections in a column to deconcatenate_column @zbarry [ENH] Fix error message when length of new_column_names is wrong @DollofCutty [DOC] Fixed several examples of functional syntax in functions.py . @bdice [DOC] Fix #noqa comments showing up in docs by @hectormz [ENH] Add unionizing a group of dataframes' categoricals. @zbarry [DOC] Fix contributions hyperlinks in AUTHORS.rst and contributions by @hectormz [INF] Add pre-commit hooks to repository by @ericmjl [DOC] Fix formatting code in CONTRIBUTING.rst by @hectormz [DOC] Changed the typing for most \"column_name(s)\" to Hashable rather than enforcing strings, to more closely match Pandas API by @dendrondal [INF] Edited pycodestyle and Black parameters to avoid venvs by @dendrondal","title":"v0.18.3"},{"location":"CHANGELOG/#v0182","text":"[INF] Make requirements.txt smaller @eyaltrabelsi [ENH] Add a reset_index parameter to shuffle @eyaltrabelsi [DOC] Added contribution page link to readme @eyaltrabelsi [DOC] fix example for update_where , provide a bit more detail, and expand the bad_values example notebook to demonstrate its use by @anzelpwj. [INF] Fix pytest marks by @ericmjl (issue #520) [ENH] add example notebook with use of finance submodule methods by @rahosbach [DOC] added a couple of admonitions for Windows users. h/t @anzelpwj for debugging help when a few tests failed for win32 @Ram-N [ENH] Pyjanitor for PySpark @zjpoh [ENH] Add pyspark clean_names @zjpoh [ENH] Convert asserts to raise exceptions by @hectormz [ENH] Add decorator functions for missing and error handling @jiafengkevinchen [DOC] Update README with functional pandas API example. @ericmjl [INF] Move get_features_targets() to new ml.py module by @hectormz [ENH] Add chirality to morgan fingerprints in janitor.chemistry submodule by @Clayton-Springer [INF] import_message suggests python dist. appropriate installs by @hectormz [ENH] Add count_cumulative_unique() method to janitor.functions submodule by @rahosbach [ENH] Add update_where() method to janitor.spark.functions submodule by @zjpoh","title":"v0.18.2"},{"location":"CHANGELOG/#v0181","text":"[ENH] extend find_replace functionality to allow both exact match and regular-expression-based fuzzy match by @shandou [ENH] add preserve_position kwarg to deconcatenate_column with tests by @shandou and @ericmjl [DOC] add contributions that did not leave git traces by @ericmjl [ENH] add inflation adjustment in finance submodule by @rahosbach [DOC] clarified how new functions should be implemented by @shandou [ENH] add optional removal of accents on functions.clean_names, enabled by default by @mralbu [ENH] add camelCase conversion to snake_case on clean_names by @ericmjl, h/t @jtaylor for sharing original [ENH] Added null_flag function which can mark null values in rows. Implemented by @anzelpwj [ENH] add engineering submodule with unit conversion method by @rahosbach [DOC] add PyPI project description [ENH] add example notebook with use of finance submodule methods by @rahosbach For changes that happened prior to v0.18.1, please consult the closed PRs, which can be found here . We thank all contributors who have helped make pyjanitor the package that it is today.","title":"v0.18.1"},{"location":"devguide/","text":"Development Guide For those of you who are interested in contributing code to the project, many previous contributors have wrestled with a variety of ways of getting set up. While we can't cover every single last configuration, we could cover some of the more common cases. Here they are for your benefit! Development Containers with VSCode As of 29 May 2020, development containers are supported! This is the preferred way to get you started up and running, as it creates a uniform setup environment that is much easier for the maintainers to debug, because you are provided with a pre-built and clean development environment free of any assumptions of your own system. You don't have to wrestle with conda wait times if you don't want to! To get started: Fork the repository. Ensure you have Docker running on your local machine. Ensure you have VSCode running on your local machine. In VS Code, Install an extension called Remote - Containers . In Visual Studio Code, click on the quick actions Status Bar item in the lower left corner. Then select \"Remote Containers: Clone Repository In Container Volume\". Enter in the URL of your fork of pyjanitor . VSCode will pull down the prebuilt Docker container, git clone the repository for you inside an isolated Docker volume, and mount the repository directory inside your Docker container. Follow best practices to submit a pull request by making a feature branch. Now, hack away, and submit in your pull request! You shouln't be able to access the cloned repo on your local hard drive. If you do want local access, then clone the repo locally first before selecting \"Remote Containers: Open Folder In Container\". If you find something is broken because a utility is missing in the container, submit a PR with the appropriate build command inserted in the Dockerfile. Care has been taken to document what each step does, so please read the in-line documentation in the Dockerfile carefully. Manual Setup Fork the repository Firstly, begin by forking the pyjanitor repo on GitHub. Then, clone your fork locally: git clone git@github.com:<your_github_username>/pyjanitor.git Setup the conda environment Now, install your cloned repo into a conda environment. Assuming you have conda installed, this is how you set up your fork for local development cd pyjanitor/ # Activate the pyjanitor conda environment source activate pyjanitor-dev # Create your conda environment conda env create -f environment-dev.yml # Install PyJanitor in development mode python setup.py develop # Register current virtual environment as a Jupyter Python kernel python -m ipykernel install --user --name pyjanitor-dev --display-name \"PyJanitor development\" If you plan to write any notebooks, make sure they run correctly inside the environment by selecting the correct kernel from the top right corner of JupyterLab! PyCharm Users For PyCharm users, here are some instructions <PYCHARM_USERS.html> __ to get your Conda environment set up. Install the pre-commit hooks. pre-commit hooks are available to run code formatting checks automagically before git commits happen. If you did not have these installed before, run the following commands: # Update your environment to install pre-commit conda env update -f environment-dev.yml # Install pre-commit hooks pre-commit install Build docs locally You should also be able to preview the docs locally. To do this, from the main pyjanitor directory: python -m mkdocs serve The command above allows you to view the documentation locally in your browser. If you get any errors about importing modules when running mkdocs serve , first activate the development environment: source activate pyjanitor-dev || conda activate pyjanitor-dev Plan out the change you'd like to contribute The old adage rings true: failing to plan means planning to fail. We'd encourage you to flesh out the idea you'd like to contribute on the GitHub issue tracker before embarking on a contribution. Submitting new code, in particular, is one where the maintainers will need more consideration, after all, any new code submitted introduces a new maintenance burden, unless you the contributor would like to join the maintainers team! To kickstart the discussion, submit an issue to the pyjanitor GitHub issue tracker describing your planned changes. The issue tracker also helps us keep track of who is working on what. Create a branch for local development New contributions to pyjanitor should be done in a new branch that you have based off the latest version of the dev branch. To create a new branch: git checkout -b <name-of-your-bugfix-or-feature> dev Now you can make your changes locally. Check your environment To ensure that your environemnt is properly set up, run the following command: python -m pytest -m \"not turtle\" If all tests pass then your environment is setup for development and you are ready to contribute \ud83e\udd73. Check your code When you're done making changes, commit your staged files with a meaningful message. While we have automated checks that run before code is commited via pre-commit and GitHub Actions to run tests before code can be merged, you can still manually run the following commands to check that your changes are properly formatted and that all tests still pass. To do so: Run python -m flake8 --exclude nbconvert_config.py janitor to check code styling problems Run python -m black -c pyproject.toml to format your code. Run python -m interrogate -c pyproject.toml to check your code for missing docstring. Run darglint -v 2 to check quality of your docstrings. Run python -m pytest to run all unit tests. Tip You can run python -m pytest -m \"not turtle\" to run the fast tests. Running test locally When you run tests locally, the tests in chemistry.py , biology.py , spark.py are automatically skipped if you don't have the optional dependencies (e.g. rdkit ) installed. Info pre-commit does not run your tests locally rather all tests are run in continous integration (CI). All tests must pass in CI before the pull request is accepted, and the continuous integration system up on GitHub Actions will help run all of the tests before they are committed to the repository. Commit your changes Now you can commit your changes and push your branch to GitHub: git add . git commit -m \"Your detailed description of your changes.\" git push origin <name-of-your-bugfix-or-feature> Submit a pull request through the GitHub website Congratulations \ud83c\udf89\ud83c\udf89\ud83c\udf89, you've made it to the penultimate step; your code is ready to be checked and reviewed by the maintainers! Head over to the GitHub website and create a pull request. When you are picking out which branch to merge into, a.k.a. the target branch, be sure to select dev (not master ). Fix any remaining issues It's rare, but you might at this point still encounter issues, as the continuous integration (CI) system on GitHub Actions checks your code. Some of these might not be your fault; rather, it might well be the case that your code fell a little bit out of date as others' pull requests are merged into the repository. In any case, if there are any issues, the pipeline will fail out. We check for code style, docstring coverage, test coverage, and doc discovery. If you're comfortable looking at the pipeline logs, feel free to do so; they are open to all to view. Otherwise, one of the dev team members can help you with reviewing the code checks. Code Compatibility pyjanitor supports Python 3.6+, so all contributed code must maintain this compatibility. Tips To run a subset of tests: pytest tests.test_functions","title":"Development Guide"},{"location":"devguide/#development-guide","text":"For those of you who are interested in contributing code to the project, many previous contributors have wrestled with a variety of ways of getting set up. While we can't cover every single last configuration, we could cover some of the more common cases. Here they are for your benefit!","title":"Development Guide"},{"location":"devguide/#development-containers-with-vscode","text":"As of 29 May 2020, development containers are supported! This is the preferred way to get you started up and running, as it creates a uniform setup environment that is much easier for the maintainers to debug, because you are provided with a pre-built and clean development environment free of any assumptions of your own system. You don't have to wrestle with conda wait times if you don't want to! To get started: Fork the repository. Ensure you have Docker running on your local machine. Ensure you have VSCode running on your local machine. In VS Code, Install an extension called Remote - Containers . In Visual Studio Code, click on the quick actions Status Bar item in the lower left corner. Then select \"Remote Containers: Clone Repository In Container Volume\". Enter in the URL of your fork of pyjanitor . VSCode will pull down the prebuilt Docker container, git clone the repository for you inside an isolated Docker volume, and mount the repository directory inside your Docker container. Follow best practices to submit a pull request by making a feature branch. Now, hack away, and submit in your pull request! You shouln't be able to access the cloned repo on your local hard drive. If you do want local access, then clone the repo locally first before selecting \"Remote Containers: Open Folder In Container\". If you find something is broken because a utility is missing in the container, submit a PR with the appropriate build command inserted in the Dockerfile. Care has been taken to document what each step does, so please read the in-line documentation in the Dockerfile carefully.","title":"Development Containers with VSCode"},{"location":"devguide/#manual-setup","text":"","title":"Manual Setup"},{"location":"devguide/#fork-the-repository","text":"Firstly, begin by forking the pyjanitor repo on GitHub. Then, clone your fork locally: git clone git@github.com:<your_github_username>/pyjanitor.git","title":"Fork the repository"},{"location":"devguide/#setup-the-conda-environment","text":"Now, install your cloned repo into a conda environment. Assuming you have conda installed, this is how you set up your fork for local development cd pyjanitor/ # Activate the pyjanitor conda environment source activate pyjanitor-dev # Create your conda environment conda env create -f environment-dev.yml # Install PyJanitor in development mode python setup.py develop # Register current virtual environment as a Jupyter Python kernel python -m ipykernel install --user --name pyjanitor-dev --display-name \"PyJanitor development\" If you plan to write any notebooks, make sure they run correctly inside the environment by selecting the correct kernel from the top right corner of JupyterLab! PyCharm Users For PyCharm users, here are some instructions <PYCHARM_USERS.html> __ to get your Conda environment set up.","title":"Setup the conda environment"},{"location":"devguide/#install-the-pre-commit-hooks","text":"pre-commit hooks are available to run code formatting checks automagically before git commits happen. If you did not have these installed before, run the following commands: # Update your environment to install pre-commit conda env update -f environment-dev.yml # Install pre-commit hooks pre-commit install","title":"Install the pre-commit hooks."},{"location":"devguide/#build-docs-locally","text":"You should also be able to preview the docs locally. To do this, from the main pyjanitor directory: python -m mkdocs serve The command above allows you to view the documentation locally in your browser. If you get any errors about importing modules when running mkdocs serve , first activate the development environment: source activate pyjanitor-dev || conda activate pyjanitor-dev","title":"Build docs locally"},{"location":"devguide/#plan-out-the-change-youd-like-to-contribute","text":"The old adage rings true: failing to plan means planning to fail. We'd encourage you to flesh out the idea you'd like to contribute on the GitHub issue tracker before embarking on a contribution. Submitting new code, in particular, is one where the maintainers will need more consideration, after all, any new code submitted introduces a new maintenance burden, unless you the contributor would like to join the maintainers team! To kickstart the discussion, submit an issue to the pyjanitor GitHub issue tracker describing your planned changes. The issue tracker also helps us keep track of who is working on what.","title":"Plan out the change you'd like to contribute"},{"location":"devguide/#create-a-branch-for-local-development","text":"New contributions to pyjanitor should be done in a new branch that you have based off the latest version of the dev branch. To create a new branch: git checkout -b <name-of-your-bugfix-or-feature> dev Now you can make your changes locally.","title":"Create a branch for local development"},{"location":"devguide/#check-your-environment","text":"To ensure that your environemnt is properly set up, run the following command: python -m pytest -m \"not turtle\" If all tests pass then your environment is setup for development and you are ready to contribute \ud83e\udd73.","title":"Check your environment"},{"location":"devguide/#check-your-code","text":"When you're done making changes, commit your staged files with a meaningful message. While we have automated checks that run before code is commited via pre-commit and GitHub Actions to run tests before code can be merged, you can still manually run the following commands to check that your changes are properly formatted and that all tests still pass. To do so: Run python -m flake8 --exclude nbconvert_config.py janitor to check code styling problems Run python -m black -c pyproject.toml to format your code. Run python -m interrogate -c pyproject.toml to check your code for missing docstring. Run darglint -v 2 to check quality of your docstrings. Run python -m pytest to run all unit tests. Tip You can run python -m pytest -m \"not turtle\" to run the fast tests. Running test locally When you run tests locally, the tests in chemistry.py , biology.py , spark.py are automatically skipped if you don't have the optional dependencies (e.g. rdkit ) installed. Info pre-commit does not run your tests locally rather all tests are run in continous integration (CI). All tests must pass in CI before the pull request is accepted, and the continuous integration system up on GitHub Actions will help run all of the tests before they are committed to the repository.","title":"Check your code"},{"location":"devguide/#commit-your-changes","text":"Now you can commit your changes and push your branch to GitHub: git add . git commit -m \"Your detailed description of your changes.\" git push origin <name-of-your-bugfix-or-feature>","title":"Commit your changes"},{"location":"devguide/#submit-a-pull-request-through-the-github-website","text":"Congratulations \ud83c\udf89\ud83c\udf89\ud83c\udf89, you've made it to the penultimate step; your code is ready to be checked and reviewed by the maintainers! Head over to the GitHub website and create a pull request. When you are picking out which branch to merge into, a.k.a. the target branch, be sure to select dev (not master ).","title":"Submit a pull request through the GitHub website"},{"location":"devguide/#fix-any-remaining-issues","text":"It's rare, but you might at this point still encounter issues, as the continuous integration (CI) system on GitHub Actions checks your code. Some of these might not be your fault; rather, it might well be the case that your code fell a little bit out of date as others' pull requests are merged into the repository. In any case, if there are any issues, the pipeline will fail out. We check for code style, docstring coverage, test coverage, and doc discovery. If you're comfortable looking at the pipeline logs, feel free to do so; they are open to all to view. Otherwise, one of the dev team members can help you with reviewing the code checks.","title":"Fix any remaining issues"},{"location":"devguide/#code-compatibility","text":"pyjanitor supports Python 3.6+, so all contributed code must maintain this compatibility.","title":"Code Compatibility"},{"location":"devguide/#tips","text":"To run a subset of tests: pytest tests.test_functions","title":"Tips"},{"location":"api/biology/","text":"Biology Biology and bioinformatics-oriented data cleaning functions. join_fasta(df, filename, id_col, column_name) Convenience method to join in a FASTA file as a column. This allows us to add the string sequence of a FASTA file as a new column of data in the dataframe. This method only attaches the string representation of the SeqRecord.Seq object from Biopython. Does not attach the full SeqRecord. Alphabet is also not stored, under the assumption that the data scientist has domain knowledge of what kind of sequence is being read in (nucleotide vs. amino acid.) This method mutates the original DataFrame. For more advanced functions, please use phylopandas. Method chaining usage example: >>> import tempfile >>> import pandas as pd >>> import janitor.biology >>> tf = tempfile.NamedTemporaryFile() >>> tf.write('''>SEQUENCE_1 ... MTEITAAMVKELRESTGAGMMDCK ... >SEQUENCE_2 ... SATVSEINSETDFVAKN'''.encode('utf8')) 66 >>> tf.seek(0) 0 >>> df = pd.DataFrame({\"sequence_accession\": ... [\"SEQUENCE_1\", \"SEQUENCE_2\", ]}) >>> df = df.join_fasta( # doctest: +SKIP ... filename=tf.name, ... id_col='sequence_accession', ... column_name='sequence', ... ) >>> df.sequence # doctest: +SKIP 0 MTEITAAMVKELRESTGAGMMDCK 1 SATVSEINSETDFVAKN Name: sequence, dtype: object Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required filename str Path to the FASTA file. required id_col str The column in the DataFrame that houses sequence IDs. required column_name str The name of the new column. required Returns: Type Description DataFrame A pandas DataFrame with new FASTA string sequence column. Source code in janitor/biology.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\") def join_fasta( df: pd.DataFrame, filename: str, id_col: str, column_name: str ) -> pd.DataFrame: \"\"\" Convenience method to join in a FASTA file as a column. This allows us to add the string sequence of a FASTA file as a new column of data in the dataframe. This method only attaches the string representation of the SeqRecord.Seq object from Biopython. Does not attach the full SeqRecord. Alphabet is also not stored, under the assumption that the data scientist has domain knowledge of what kind of sequence is being read in (nucleotide vs. amino acid.) This method mutates the original DataFrame. For more advanced functions, please use phylopandas. Method chaining usage example: >>> import tempfile >>> import pandas as pd >>> import janitor.biology >>> tf = tempfile.NamedTemporaryFile() >>> tf.write('''>SEQUENCE_1 ... MTEITAAMVKELRESTGAGMMDCK ... >SEQUENCE_2 ... SATVSEINSETDFVAKN'''.encode('utf8')) 66 >>> tf.seek(0) 0 >>> df = pd.DataFrame({\"sequence_accession\": ... [\"SEQUENCE_1\", \"SEQUENCE_2\", ]}) >>> df = df.join_fasta( # doctest: +SKIP ... filename=tf.name, ... id_col='sequence_accession', ... column_name='sequence', ... ) >>> df.sequence # doctest: +SKIP 0 MTEITAAMVKELRESTGAGMMDCK 1 SATVSEINSETDFVAKN Name: sequence, dtype: object :param df: A pandas DataFrame. :param filename: Path to the FASTA file. :param id_col: The column in the DataFrame that houses sequence IDs. :param column_name: The name of the new column. :returns: A pandas DataFrame with new FASTA string sequence column. \"\"\" seqrecords = { x.id: x.seq.__str__() for x in SeqIO.parse(filename, \"fasta\") } seq_col = [seqrecords[i] for i in df[id_col]] df[column_name] = seq_col return df","title":"Biology"},{"location":"api/biology/#biology","text":"Biology and bioinformatics-oriented data cleaning functions.","title":"Biology"},{"location":"api/biology/#janitor.biology.join_fasta","text":"Convenience method to join in a FASTA file as a column. This allows us to add the string sequence of a FASTA file as a new column of data in the dataframe. This method only attaches the string representation of the SeqRecord.Seq object from Biopython. Does not attach the full SeqRecord. Alphabet is also not stored, under the assumption that the data scientist has domain knowledge of what kind of sequence is being read in (nucleotide vs. amino acid.) This method mutates the original DataFrame. For more advanced functions, please use phylopandas. Method chaining usage example: >>> import tempfile >>> import pandas as pd >>> import janitor.biology >>> tf = tempfile.NamedTemporaryFile() >>> tf.write('''>SEQUENCE_1 ... MTEITAAMVKELRESTGAGMMDCK ... >SEQUENCE_2 ... SATVSEINSETDFVAKN'''.encode('utf8')) 66 >>> tf.seek(0) 0 >>> df = pd.DataFrame({\"sequence_accession\": ... [\"SEQUENCE_1\", \"SEQUENCE_2\", ]}) >>> df = df.join_fasta( # doctest: +SKIP ... filename=tf.name, ... id_col='sequence_accession', ... column_name='sequence', ... ) >>> df.sequence # doctest: +SKIP 0 MTEITAAMVKELRESTGAGMMDCK 1 SATVSEINSETDFVAKN Name: sequence, dtype: object Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required filename str Path to the FASTA file. required id_col str The column in the DataFrame that houses sequence IDs. required column_name str The name of the new column. required Returns: Type Description DataFrame A pandas DataFrame with new FASTA string sequence column. Source code in janitor/biology.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\") def join_fasta( df: pd.DataFrame, filename: str, id_col: str, column_name: str ) -> pd.DataFrame: \"\"\" Convenience method to join in a FASTA file as a column. This allows us to add the string sequence of a FASTA file as a new column of data in the dataframe. This method only attaches the string representation of the SeqRecord.Seq object from Biopython. Does not attach the full SeqRecord. Alphabet is also not stored, under the assumption that the data scientist has domain knowledge of what kind of sequence is being read in (nucleotide vs. amino acid.) This method mutates the original DataFrame. For more advanced functions, please use phylopandas. Method chaining usage example: >>> import tempfile >>> import pandas as pd >>> import janitor.biology >>> tf = tempfile.NamedTemporaryFile() >>> tf.write('''>SEQUENCE_1 ... MTEITAAMVKELRESTGAGMMDCK ... >SEQUENCE_2 ... SATVSEINSETDFVAKN'''.encode('utf8')) 66 >>> tf.seek(0) 0 >>> df = pd.DataFrame({\"sequence_accession\": ... [\"SEQUENCE_1\", \"SEQUENCE_2\", ]}) >>> df = df.join_fasta( # doctest: +SKIP ... filename=tf.name, ... id_col='sequence_accession', ... column_name='sequence', ... ) >>> df.sequence # doctest: +SKIP 0 MTEITAAMVKELRESTGAGMMDCK 1 SATVSEINSETDFVAKN Name: sequence, dtype: object :param df: A pandas DataFrame. :param filename: Path to the FASTA file. :param id_col: The column in the DataFrame that houses sequence IDs. :param column_name: The name of the new column. :returns: A pandas DataFrame with new FASTA string sequence column. \"\"\" seqrecords = { x.id: x.seq.__str__() for x in SeqIO.parse(filename, \"fasta\") } seq_col = [seqrecords[i] for i in df[id_col]] df[column_name] = seq_col return df","title":"join_fasta()"},{"location":"api/chemistry/","text":"Chemistry Chemistry and cheminformatics-oriented data cleaning functions. maccs_keys_fingerprint(df, mols_column_name) Convert a column of RDKIT mol objects into MACCS Keys Fingerprints. Returns a new dataframe without any of the original data. This is intentional to leave the user with the data requested. This method does not mutate the original DataFrame. Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> maccs = janitor.chemistry.maccs_keys_fingerprint( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols' ... ) >>> len(maccs.columns) 167 Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> maccs = ( ... df.smiles2mol('smiles', 'mols') ... .maccs_keys_fingerprint(mols_column_name='mols') ... ) >>> len(maccs.columns) 167 If you wish to join the maccs keys fingerprints back into the original dataframe, this can be accomplished by doing a join , because the indices are preserved: joined = df.join(maccs) len(joined.columns) 169 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required mols_column_name Hashable The name of the column that has the RDKIT mol objects. required Returns: Type Description DataFrame A new pandas DataFrame of MACCS keys fingerprints. Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(mols_col=\"mols_column_name\") def maccs_keys_fingerprint( df: pd.DataFrame, mols_column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert a column of RDKIT mol objects into MACCS Keys Fingerprints. Returns a new dataframe without any of the original data. This is intentional to leave the user with the data requested. This method does not mutate the original DataFrame. Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> maccs = janitor.chemistry.maccs_keys_fingerprint( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols' ... ) >>> len(maccs.columns) 167 Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> maccs = ( ... df.smiles2mol('smiles', 'mols') ... .maccs_keys_fingerprint(mols_column_name='mols') ... ) >>> len(maccs.columns) 167 If you wish to join the maccs keys fingerprints back into the original dataframe, this can be accomplished by doing a `join`, because the indices are preserved: >>> joined = df.join(maccs) >>> len(joined.columns) 169 :param df: A pandas DataFrame. :param mols_column_name: The name of the column that has the RDKIT mol objects. :returns: A new pandas DataFrame of MACCS keys fingerprints. \"\"\" maccs = [GetMACCSKeysFingerprint(m) for m in df[mols_column_name]] np_maccs = [] for macc in maccs: arr = np.zeros((1,)) DataStructs.ConvertToNumpyArray(macc, arr) np_maccs.append(arr) np_maccs = np.vstack(np_maccs) fmaccs = pd.DataFrame(np_maccs) fmaccs.index = df.index return fmaccs molecular_descriptors(df, mols_column_name) Convert a column of RDKIT mol objects into a Pandas DataFrame of molecular descriptors. Returns a new dataframe without any of the original data. This is intentional to leave the user only with the data requested. This method does not mutate the original DataFrame. The molecular descriptors are from the rdkit.Chem.rdMolDescriptors: Chi0n, Chi0v, Chi1n, Chi1v, Chi2n, Chi2v, Chi3n, Chi3v, Chi4n, Chi4v, ExactMolWt, FractionCSP3, HallKierAlpha, Kappa1, Kappa2, Kappa3, LabuteASA, NumAliphaticCarbocycles, NumAliphaticHeterocycles, NumAliphaticRings, NumAmideBonds, NumAromaticCarbocycles, NumAromaticHeterocycles, NumAromaticRings, NumAtomStereoCenters, NumBridgeheadAtoms, NumHBA, NumHBD, NumHeteroatoms, NumHeterocycles, NumLipinskiHBA, NumLipinskiHBD, NumRings, NumSaturatedCarbocycles, NumSaturatedHeterocycles, NumSaturatedRings, NumSpiroAtoms, NumUnspecifiedAtomStereoCenters, TPSA. Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> mol_desc = ( ... janitor.chemistry.molecular_descriptors( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols' ... ) ... ) >>> mol_desc.TPSA 0 34.14 1 37.30 Name: TPSA, dtype: float64 Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> mol_desc = ( ... df.smiles2mol('smiles', 'mols') ... .molecular_descriptors(mols_column_name='mols') ... ) >>> mol_desc.TPSA 0 34.14 1 37.30 Name: TPSA, dtype: float64 If you wish to join the molecular descriptors back into the original dataframe, this can be accomplished by doing a join , because the indices are preserved: >>> joined = df.join(mol_desc) >>> len(joined.columns) 41 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required mols_column_name Hashable The name of the column that has the RDKIT mol objects. required Returns: Type Description DataFrame A new pandas DataFrame of molecular descriptors. Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(mols_col=\"mols_column_name\") def molecular_descriptors( df: pd.DataFrame, mols_column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert a column of RDKIT mol objects into a Pandas DataFrame of molecular descriptors. Returns a new dataframe without any of the original data. This is intentional to leave the user only with the data requested. This method does not mutate the original DataFrame. The molecular descriptors are from the rdkit.Chem.rdMolDescriptors: ``` Chi0n, Chi0v, Chi1n, Chi1v, Chi2n, Chi2v, Chi3n, Chi3v, Chi4n, Chi4v, ExactMolWt, FractionCSP3, HallKierAlpha, Kappa1, Kappa2, Kappa3, LabuteASA, NumAliphaticCarbocycles, NumAliphaticHeterocycles, NumAliphaticRings, NumAmideBonds, NumAromaticCarbocycles, NumAromaticHeterocycles, NumAromaticRings, NumAtomStereoCenters, NumBridgeheadAtoms, NumHBA, NumHBD, NumHeteroatoms, NumHeterocycles, NumLipinskiHBA, NumLipinskiHBD, NumRings, NumSaturatedCarbocycles, NumSaturatedHeterocycles, NumSaturatedRings, NumSpiroAtoms, NumUnspecifiedAtomStereoCenters, TPSA. ``` Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> mol_desc = ( ... janitor.chemistry.molecular_descriptors( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols' ... ) ... ) >>> mol_desc.TPSA 0 34.14 1 37.30 Name: TPSA, dtype: float64 Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> mol_desc = ( ... df.smiles2mol('smiles', 'mols') ... .molecular_descriptors(mols_column_name='mols') ... ) >>> mol_desc.TPSA 0 34.14 1 37.30 Name: TPSA, dtype: float64 If you wish to join the molecular descriptors back into the original dataframe, this can be accomplished by doing a `join`, because the indices are preserved: >>> joined = df.join(mol_desc) >>> len(joined.columns) 41 :param df: A pandas DataFrame. :param mols_column_name: The name of the column that has the RDKIT mol objects. :returns: A new pandas DataFrame of molecular descriptors. \"\"\" descriptors = [ CalcChi0n, CalcChi0v, CalcChi1n, CalcChi1v, CalcChi2n, CalcChi2v, CalcChi3n, CalcChi3v, CalcChi4n, CalcChi4v, CalcExactMolWt, CalcFractionCSP3, CalcHallKierAlpha, CalcKappa1, CalcKappa2, CalcKappa3, CalcLabuteASA, CalcNumAliphaticCarbocycles, CalcNumAliphaticHeterocycles, CalcNumAliphaticRings, CalcNumAmideBonds, CalcNumAromaticCarbocycles, CalcNumAromaticHeterocycles, CalcNumAromaticRings, CalcNumAtomStereoCenters, CalcNumBridgeheadAtoms, CalcNumHBA, CalcNumHBD, CalcNumHeteroatoms, CalcNumHeterocycles, CalcNumLipinskiHBA, CalcNumLipinskiHBD, CalcNumRings, CalcNumSaturatedCarbocycles, CalcNumSaturatedHeterocycles, CalcNumSaturatedRings, CalcNumSpiroAtoms, CalcNumUnspecifiedAtomStereoCenters, CalcTPSA, ] descriptors_mapping = {f.__name__.strip(\"Calc\"): f for f in descriptors} feats = dict() for name, func in descriptors_mapping.items(): feats[name] = [func(m) for m in df[mols_column_name]] return pd.DataFrame(feats) morgan_fingerprint(df, mols_column_name, radius=3, nbits=2048, kind='counts') Convert a column of RDKIT Mol objects into Morgan Fingerprints. Returns a new dataframe without any of the original data. This is intentional, as Morgan fingerprints are usually high-dimensional features. This method does not mutate the original DataFrame. Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) # For \"counts\" kind >>> morgans = janitor.chemistry.morgan_fingerprint( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='counts' # Defaults to \"counts\" ... ) >>> set(morgans.iloc[0]) {0.0, 1.0, 2.0} # For \"bits\" kind >>> morgans = janitor.chemistry.morgan_fingerprint( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='bits' # Defaults to \"counts\" ... ) >>> set(morgans.iloc[0]) {0.0, 1.0} Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) # For \"counts\" kind >>> morgans = ( ... df.smiles2mol('smiles', 'mols') ... .morgan_fingerprint( ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='counts' # Defaults to \"counts\" ... ) ... ) >>> set(morgans.iloc[0]) {0.0, 1.0, 2.0} # For \"bits\" kind >>> morgans = ( ... df ... .smiles2mol('smiles', 'mols') ... .morgan_fingerprint( ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='bits' # Defaults to \"counts\" ... ) ... ) >>> set(morgans.iloc[0]) {0.0, 1.0} If you wish to join the morgan fingerprints back into the original dataframe, this can be accomplished by doing a join , because the indices are preserved: >>> joined = df.join(morgans) >>> len(joined.columns) 2050 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required mols_column_name str The name of the column that has the RDKIT mol objects required radius int Radius of Morgan fingerprints. Defaults to 3. 3 nbits int The length of the fingerprints. Defaults to 2048. 2048 kind str Whether to return counts or bits. Defaults to counts. 'counts' Returns: Type Description DataFrame A new pandas DataFrame of Morgan fingerprints. Exceptions: Type Description ValueError if kind is not one of \"counts\" or `\"bits\"``. Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(mols_col=\"mols_column_name\") def morgan_fingerprint( df: pd.DataFrame, mols_column_name: str, radius: int = 3, nbits: int = 2048, kind: str = \"counts\", ) -> pd.DataFrame: \"\"\" Convert a column of RDKIT Mol objects into Morgan Fingerprints. Returns a new dataframe without any of the original data. This is intentional, as Morgan fingerprints are usually high-dimensional features. This method does not mutate the original DataFrame. Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) # For \"counts\" kind >>> morgans = janitor.chemistry.morgan_fingerprint( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='counts' # Defaults to \"counts\" ... ) >>> set(morgans.iloc[0]) {0.0, 1.0, 2.0} # For \"bits\" kind >>> morgans = janitor.chemistry.morgan_fingerprint( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='bits' # Defaults to \"counts\" ... ) >>> set(morgans.iloc[0]) {0.0, 1.0} Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) # For \"counts\" kind >>> morgans = ( ... df.smiles2mol('smiles', 'mols') ... .morgan_fingerprint( ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='counts' # Defaults to \"counts\" ... ) ... ) >>> set(morgans.iloc[0]) {0.0, 1.0, 2.0} # For \"bits\" kind >>> morgans = ( ... df ... .smiles2mol('smiles', 'mols') ... .morgan_fingerprint( ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='bits' # Defaults to \"counts\" ... ) ... ) >>> set(morgans.iloc[0]) {0.0, 1.0} If you wish to join the morgan fingerprints back into the original dataframe, this can be accomplished by doing a `join`, because the indices are preserved: >>> joined = df.join(morgans) >>> len(joined.columns) 2050 :param df: A pandas DataFrame. :param mols_column_name: The name of the column that has the RDKIT mol objects :param radius: Radius of Morgan fingerprints. Defaults to 3. :param nbits: The length of the fingerprints. Defaults to 2048. :param kind: Whether to return counts or bits. Defaults to counts. :returns: A new pandas DataFrame of Morgan fingerprints. :raises ValueError: if `kind` is not one of `\"counts\"` or `\"bits\"``. \"\"\" acceptable_kinds = [\"counts\", \"bits\"] if kind not in acceptable_kinds: raise ValueError(f\"`kind` must be one of {acceptable_kinds}\") if kind == \"bits\": fps = [ GetMorganFingerprintAsBitVect(m, radius, nbits, useChirality=True) for m in df[mols_column_name] ] elif kind == \"counts\": fps = [ GetHashedMorganFingerprint(m, radius, nbits, useChirality=True) for m in df[mols_column_name] ] np_fps = [] for fp in fps: arr = np.zeros((1,)) DataStructs.ConvertToNumpyArray(fp, arr) np_fps.append(arr) np_fps = np.vstack(np_fps) fpdf = pd.DataFrame(np_fps) fpdf.index = df.index return fpdf smiles2mol(df, smiles_column_name, mols_column_name, drop_nulls=True, progressbar=None) Convert a column of SMILES strings into RDKit Mol objects. Automatically drops invalid SMILES, as determined by RDKIT. This method mutates the original DataFrame. Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> df = janitor.chemistry.smiles2mol( ... df=df, ... smiles_column_name='smiles', ... mols_column_name='mols' ... ) >>> df.mols[0].GetNumAtoms(), df.mols[0].GetNumBonds() (3, 2) >>> df.mols[1].GetNumAtoms(), df.mols[1].GetNumBonds() (5, 4) Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = df.smiles2mol( ... smiles_column_name='smiles', ... mols_column_name='rdkmol' ... ) >>> df.rdkmol[0].GetNumAtoms(), df.rdkmol[0].GetNumBonds() (3, 2) A progressbar can be optionally used. Pass in \"notebook\" to show a tqdm notebook progressbar. ( ipywidgets must be enabled with your Jupyter installation.) Pass in \"terminal\" to show a tqdm progressbar. Better suited for use with scripts. \"none\" is the default value - progress bar will be not be shown. Parameters: Name Type Description Default df DataFrame pandas DataFrame. required smiles_column_name Hashable Name of column that holds the SMILES strings. required mols_column_name Hashable Name to be given to the new mols column. required drop_nulls bool Whether to drop rows whose mols failed to be constructed. True progressbar Optional[str] Whether to show a progressbar or not. None Returns: Type Description DataFrame A pandas DataFrame with new RDKIT Mol objects column. Exceptions: Type Description ValueError if progressbar is not one of \"notebook\"``, \"terminal\" , or `None . Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(smiles_col=\"smiles_column_name\", mols_col=\"mols_column_name\") def smiles2mol( df: pd.DataFrame, smiles_column_name: Hashable, mols_column_name: Hashable, drop_nulls: bool = True, progressbar: Optional[str] = None, ) -> pd.DataFrame: \"\"\" Convert a column of SMILES strings into RDKit Mol objects. Automatically drops invalid SMILES, as determined by RDKIT. This method mutates the original DataFrame. Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> df = janitor.chemistry.smiles2mol( ... df=df, ... smiles_column_name='smiles', ... mols_column_name='mols' ... ) >>> df.mols[0].GetNumAtoms(), df.mols[0].GetNumBonds() (3, 2) >>> df.mols[1].GetNumAtoms(), df.mols[1].GetNumBonds() (5, 4) Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = df.smiles2mol( ... smiles_column_name='smiles', ... mols_column_name='rdkmol' ... ) >>> df.rdkmol[0].GetNumAtoms(), df.rdkmol[0].GetNumBonds() (3, 2) A progressbar can be optionally used. - Pass in \"notebook\" to show a `tqdm` notebook progressbar. (`ipywidgets` must be enabled with your Jupyter installation.) - Pass in \"terminal\" to show a `tqdm` progressbar. Better suited for use with scripts. - \"none\" is the default value - progress bar will be not be shown. :param df: pandas DataFrame. :param smiles_column_name: Name of column that holds the SMILES strings. :param mols_column_name: Name to be given to the new mols column. :param drop_nulls: Whether to drop rows whose mols failed to be constructed. :param progressbar: Whether to show a progressbar or not. :returns: A pandas DataFrame with new RDKIT Mol objects column. :raises ValueError: if `progressbar` is not one of `\"notebook\"``, `\"terminal\"``, or `None``. \"\"\" valid_progress = [\"notebook\", \"terminal\", None] if progressbar not in valid_progress: raise ValueError(f\"progressbar kwarg must be one of {valid_progress}\") if progressbar is None: df[mols_column_name] = df[smiles_column_name].apply( lambda x: Chem.MolFromSmiles(x) ) else: if progressbar == \"notebook\": tqdmn().pandas(desc=\"mols\") elif progressbar == \"terminal\": tqdm.pandas(desc=\"mols\") df[mols_column_name] = df[smiles_column_name].progress_apply( lambda x: Chem.MolFromSmiles(x) ) if drop_nulls: df = df.dropna(subset=[mols_column_name]) df = df.reset_index(drop=True) return df","title":"Chemistry"},{"location":"api/chemistry/#chemistry","text":"Chemistry and cheminformatics-oriented data cleaning functions.","title":"Chemistry"},{"location":"api/chemistry/#janitor.chemistry.maccs_keys_fingerprint","text":"Convert a column of RDKIT mol objects into MACCS Keys Fingerprints. Returns a new dataframe without any of the original data. This is intentional to leave the user with the data requested. This method does not mutate the original DataFrame. Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> maccs = janitor.chemistry.maccs_keys_fingerprint( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols' ... ) >>> len(maccs.columns) 167 Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> maccs = ( ... df.smiles2mol('smiles', 'mols') ... .maccs_keys_fingerprint(mols_column_name='mols') ... ) >>> len(maccs.columns) 167 If you wish to join the maccs keys fingerprints back into the original dataframe, this can be accomplished by doing a join , because the indices are preserved: joined = df.join(maccs) len(joined.columns) 169 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required mols_column_name Hashable The name of the column that has the RDKIT mol objects. required Returns: Type Description DataFrame A new pandas DataFrame of MACCS keys fingerprints. Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(mols_col=\"mols_column_name\") def maccs_keys_fingerprint( df: pd.DataFrame, mols_column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert a column of RDKIT mol objects into MACCS Keys Fingerprints. Returns a new dataframe without any of the original data. This is intentional to leave the user with the data requested. This method does not mutate the original DataFrame. Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> maccs = janitor.chemistry.maccs_keys_fingerprint( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols' ... ) >>> len(maccs.columns) 167 Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> maccs = ( ... df.smiles2mol('smiles', 'mols') ... .maccs_keys_fingerprint(mols_column_name='mols') ... ) >>> len(maccs.columns) 167 If you wish to join the maccs keys fingerprints back into the original dataframe, this can be accomplished by doing a `join`, because the indices are preserved: >>> joined = df.join(maccs) >>> len(joined.columns) 169 :param df: A pandas DataFrame. :param mols_column_name: The name of the column that has the RDKIT mol objects. :returns: A new pandas DataFrame of MACCS keys fingerprints. \"\"\" maccs = [GetMACCSKeysFingerprint(m) for m in df[mols_column_name]] np_maccs = [] for macc in maccs: arr = np.zeros((1,)) DataStructs.ConvertToNumpyArray(macc, arr) np_maccs.append(arr) np_maccs = np.vstack(np_maccs) fmaccs = pd.DataFrame(np_maccs) fmaccs.index = df.index return fmaccs","title":"maccs_keys_fingerprint()"},{"location":"api/chemistry/#janitor.chemistry.molecular_descriptors","text":"Convert a column of RDKIT mol objects into a Pandas DataFrame of molecular descriptors. Returns a new dataframe without any of the original data. This is intentional to leave the user only with the data requested. This method does not mutate the original DataFrame. The molecular descriptors are from the rdkit.Chem.rdMolDescriptors: Chi0n, Chi0v, Chi1n, Chi1v, Chi2n, Chi2v, Chi3n, Chi3v, Chi4n, Chi4v, ExactMolWt, FractionCSP3, HallKierAlpha, Kappa1, Kappa2, Kappa3, LabuteASA, NumAliphaticCarbocycles, NumAliphaticHeterocycles, NumAliphaticRings, NumAmideBonds, NumAromaticCarbocycles, NumAromaticHeterocycles, NumAromaticRings, NumAtomStereoCenters, NumBridgeheadAtoms, NumHBA, NumHBD, NumHeteroatoms, NumHeterocycles, NumLipinskiHBA, NumLipinskiHBD, NumRings, NumSaturatedCarbocycles, NumSaturatedHeterocycles, NumSaturatedRings, NumSpiroAtoms, NumUnspecifiedAtomStereoCenters, TPSA. Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> mol_desc = ( ... janitor.chemistry.molecular_descriptors( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols' ... ) ... ) >>> mol_desc.TPSA 0 34.14 1 37.30 Name: TPSA, dtype: float64 Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> mol_desc = ( ... df.smiles2mol('smiles', 'mols') ... .molecular_descriptors(mols_column_name='mols') ... ) >>> mol_desc.TPSA 0 34.14 1 37.30 Name: TPSA, dtype: float64 If you wish to join the molecular descriptors back into the original dataframe, this can be accomplished by doing a join , because the indices are preserved: >>> joined = df.join(mol_desc) >>> len(joined.columns) 41 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required mols_column_name Hashable The name of the column that has the RDKIT mol objects. required Returns: Type Description DataFrame A new pandas DataFrame of molecular descriptors. Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(mols_col=\"mols_column_name\") def molecular_descriptors( df: pd.DataFrame, mols_column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert a column of RDKIT mol objects into a Pandas DataFrame of molecular descriptors. Returns a new dataframe without any of the original data. This is intentional to leave the user only with the data requested. This method does not mutate the original DataFrame. The molecular descriptors are from the rdkit.Chem.rdMolDescriptors: ``` Chi0n, Chi0v, Chi1n, Chi1v, Chi2n, Chi2v, Chi3n, Chi3v, Chi4n, Chi4v, ExactMolWt, FractionCSP3, HallKierAlpha, Kappa1, Kappa2, Kappa3, LabuteASA, NumAliphaticCarbocycles, NumAliphaticHeterocycles, NumAliphaticRings, NumAmideBonds, NumAromaticCarbocycles, NumAromaticHeterocycles, NumAromaticRings, NumAtomStereoCenters, NumBridgeheadAtoms, NumHBA, NumHBD, NumHeteroatoms, NumHeterocycles, NumLipinskiHBA, NumLipinskiHBD, NumRings, NumSaturatedCarbocycles, NumSaturatedHeterocycles, NumSaturatedRings, NumSpiroAtoms, NumUnspecifiedAtomStereoCenters, TPSA. ``` Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> mol_desc = ( ... janitor.chemistry.molecular_descriptors( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols' ... ) ... ) >>> mol_desc.TPSA 0 34.14 1 37.30 Name: TPSA, dtype: float64 Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> mol_desc = ( ... df.smiles2mol('smiles', 'mols') ... .molecular_descriptors(mols_column_name='mols') ... ) >>> mol_desc.TPSA 0 34.14 1 37.30 Name: TPSA, dtype: float64 If you wish to join the molecular descriptors back into the original dataframe, this can be accomplished by doing a `join`, because the indices are preserved: >>> joined = df.join(mol_desc) >>> len(joined.columns) 41 :param df: A pandas DataFrame. :param mols_column_name: The name of the column that has the RDKIT mol objects. :returns: A new pandas DataFrame of molecular descriptors. \"\"\" descriptors = [ CalcChi0n, CalcChi0v, CalcChi1n, CalcChi1v, CalcChi2n, CalcChi2v, CalcChi3n, CalcChi3v, CalcChi4n, CalcChi4v, CalcExactMolWt, CalcFractionCSP3, CalcHallKierAlpha, CalcKappa1, CalcKappa2, CalcKappa3, CalcLabuteASA, CalcNumAliphaticCarbocycles, CalcNumAliphaticHeterocycles, CalcNumAliphaticRings, CalcNumAmideBonds, CalcNumAromaticCarbocycles, CalcNumAromaticHeterocycles, CalcNumAromaticRings, CalcNumAtomStereoCenters, CalcNumBridgeheadAtoms, CalcNumHBA, CalcNumHBD, CalcNumHeteroatoms, CalcNumHeterocycles, CalcNumLipinskiHBA, CalcNumLipinskiHBD, CalcNumRings, CalcNumSaturatedCarbocycles, CalcNumSaturatedHeterocycles, CalcNumSaturatedRings, CalcNumSpiroAtoms, CalcNumUnspecifiedAtomStereoCenters, CalcTPSA, ] descriptors_mapping = {f.__name__.strip(\"Calc\"): f for f in descriptors} feats = dict() for name, func in descriptors_mapping.items(): feats[name] = [func(m) for m in df[mols_column_name]] return pd.DataFrame(feats)","title":"molecular_descriptors()"},{"location":"api/chemistry/#janitor.chemistry.morgan_fingerprint","text":"Convert a column of RDKIT Mol objects into Morgan Fingerprints. Returns a new dataframe without any of the original data. This is intentional, as Morgan fingerprints are usually high-dimensional features. This method does not mutate the original DataFrame. Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) # For \"counts\" kind >>> morgans = janitor.chemistry.morgan_fingerprint( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='counts' # Defaults to \"counts\" ... ) >>> set(morgans.iloc[0]) {0.0, 1.0, 2.0} # For \"bits\" kind >>> morgans = janitor.chemistry.morgan_fingerprint( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='bits' # Defaults to \"counts\" ... ) >>> set(morgans.iloc[0]) {0.0, 1.0} Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) # For \"counts\" kind >>> morgans = ( ... df.smiles2mol('smiles', 'mols') ... .morgan_fingerprint( ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='counts' # Defaults to \"counts\" ... ) ... ) >>> set(morgans.iloc[0]) {0.0, 1.0, 2.0} # For \"bits\" kind >>> morgans = ( ... df ... .smiles2mol('smiles', 'mols') ... .morgan_fingerprint( ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='bits' # Defaults to \"counts\" ... ) ... ) >>> set(morgans.iloc[0]) {0.0, 1.0} If you wish to join the morgan fingerprints back into the original dataframe, this can be accomplished by doing a join , because the indices are preserved: >>> joined = df.join(morgans) >>> len(joined.columns) 2050 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required mols_column_name str The name of the column that has the RDKIT mol objects required radius int Radius of Morgan fingerprints. Defaults to 3. 3 nbits int The length of the fingerprints. Defaults to 2048. 2048 kind str Whether to return counts or bits. Defaults to counts. 'counts' Returns: Type Description DataFrame A new pandas DataFrame of Morgan fingerprints. Exceptions: Type Description ValueError if kind is not one of \"counts\" or `\"bits\"``. Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(mols_col=\"mols_column_name\") def morgan_fingerprint( df: pd.DataFrame, mols_column_name: str, radius: int = 3, nbits: int = 2048, kind: str = \"counts\", ) -> pd.DataFrame: \"\"\" Convert a column of RDKIT Mol objects into Morgan Fingerprints. Returns a new dataframe without any of the original data. This is intentional, as Morgan fingerprints are usually high-dimensional features. This method does not mutate the original DataFrame. Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) # For \"counts\" kind >>> morgans = janitor.chemistry.morgan_fingerprint( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='counts' # Defaults to \"counts\" ... ) >>> set(morgans.iloc[0]) {0.0, 1.0, 2.0} # For \"bits\" kind >>> morgans = janitor.chemistry.morgan_fingerprint( ... df=df.smiles2mol('smiles', 'mols'), ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='bits' # Defaults to \"counts\" ... ) >>> set(morgans.iloc[0]) {0.0, 1.0} Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) # For \"counts\" kind >>> morgans = ( ... df.smiles2mol('smiles', 'mols') ... .morgan_fingerprint( ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='counts' # Defaults to \"counts\" ... ) ... ) >>> set(morgans.iloc[0]) {0.0, 1.0, 2.0} # For \"bits\" kind >>> morgans = ( ... df ... .smiles2mol('smiles', 'mols') ... .morgan_fingerprint( ... mols_column_name='mols', ... radius=3, # Defaults to 3 ... nbits=2048, # Defaults to 2048 ... kind='bits' # Defaults to \"counts\" ... ) ... ) >>> set(morgans.iloc[0]) {0.0, 1.0} If you wish to join the morgan fingerprints back into the original dataframe, this can be accomplished by doing a `join`, because the indices are preserved: >>> joined = df.join(morgans) >>> len(joined.columns) 2050 :param df: A pandas DataFrame. :param mols_column_name: The name of the column that has the RDKIT mol objects :param radius: Radius of Morgan fingerprints. Defaults to 3. :param nbits: The length of the fingerprints. Defaults to 2048. :param kind: Whether to return counts or bits. Defaults to counts. :returns: A new pandas DataFrame of Morgan fingerprints. :raises ValueError: if `kind` is not one of `\"counts\"` or `\"bits\"``. \"\"\" acceptable_kinds = [\"counts\", \"bits\"] if kind not in acceptable_kinds: raise ValueError(f\"`kind` must be one of {acceptable_kinds}\") if kind == \"bits\": fps = [ GetMorganFingerprintAsBitVect(m, radius, nbits, useChirality=True) for m in df[mols_column_name] ] elif kind == \"counts\": fps = [ GetHashedMorganFingerprint(m, radius, nbits, useChirality=True) for m in df[mols_column_name] ] np_fps = [] for fp in fps: arr = np.zeros((1,)) DataStructs.ConvertToNumpyArray(fp, arr) np_fps.append(arr) np_fps = np.vstack(np_fps) fpdf = pd.DataFrame(np_fps) fpdf.index = df.index return fpdf","title":"morgan_fingerprint()"},{"location":"api/chemistry/#janitor.chemistry.smiles2mol","text":"Convert a column of SMILES strings into RDKit Mol objects. Automatically drops invalid SMILES, as determined by RDKIT. This method mutates the original DataFrame. Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> df = janitor.chemistry.smiles2mol( ... df=df, ... smiles_column_name='smiles', ... mols_column_name='mols' ... ) >>> df.mols[0].GetNumAtoms(), df.mols[0].GetNumBonds() (3, 2) >>> df.mols[1].GetNumAtoms(), df.mols[1].GetNumBonds() (5, 4) Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = df.smiles2mol( ... smiles_column_name='smiles', ... mols_column_name='rdkmol' ... ) >>> df.rdkmol[0].GetNumAtoms(), df.rdkmol[0].GetNumBonds() (3, 2) A progressbar can be optionally used. Pass in \"notebook\" to show a tqdm notebook progressbar. ( ipywidgets must be enabled with your Jupyter installation.) Pass in \"terminal\" to show a tqdm progressbar. Better suited for use with scripts. \"none\" is the default value - progress bar will be not be shown. Parameters: Name Type Description Default df DataFrame pandas DataFrame. required smiles_column_name Hashable Name of column that holds the SMILES strings. required mols_column_name Hashable Name to be given to the new mols column. required drop_nulls bool Whether to drop rows whose mols failed to be constructed. True progressbar Optional[str] Whether to show a progressbar or not. None Returns: Type Description DataFrame A pandas DataFrame with new RDKIT Mol objects column. Exceptions: Type Description ValueError if progressbar is not one of \"notebook\"``, \"terminal\" , or `None . Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(smiles_col=\"smiles_column_name\", mols_col=\"mols_column_name\") def smiles2mol( df: pd.DataFrame, smiles_column_name: Hashable, mols_column_name: Hashable, drop_nulls: bool = True, progressbar: Optional[str] = None, ) -> pd.DataFrame: \"\"\" Convert a column of SMILES strings into RDKit Mol objects. Automatically drops invalid SMILES, as determined by RDKIT. This method mutates the original DataFrame. Functional usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]}) >>> df = janitor.chemistry.smiles2mol( ... df=df, ... smiles_column_name='smiles', ... mols_column_name='mols' ... ) >>> df.mols[0].GetNumAtoms(), df.mols[0].GetNumBonds() (3, 2) >>> df.mols[1].GetNumAtoms(), df.mols[1].GetNumBonds() (5, 4) Method chaining usage example: >>> import pandas as pd >>> import janitor.chemistry >>> df = df.smiles2mol( ... smiles_column_name='smiles', ... mols_column_name='rdkmol' ... ) >>> df.rdkmol[0].GetNumAtoms(), df.rdkmol[0].GetNumBonds() (3, 2) A progressbar can be optionally used. - Pass in \"notebook\" to show a `tqdm` notebook progressbar. (`ipywidgets` must be enabled with your Jupyter installation.) - Pass in \"terminal\" to show a `tqdm` progressbar. Better suited for use with scripts. - \"none\" is the default value - progress bar will be not be shown. :param df: pandas DataFrame. :param smiles_column_name: Name of column that holds the SMILES strings. :param mols_column_name: Name to be given to the new mols column. :param drop_nulls: Whether to drop rows whose mols failed to be constructed. :param progressbar: Whether to show a progressbar or not. :returns: A pandas DataFrame with new RDKIT Mol objects column. :raises ValueError: if `progressbar` is not one of `\"notebook\"``, `\"terminal\"``, or `None``. \"\"\" valid_progress = [\"notebook\", \"terminal\", None] if progressbar not in valid_progress: raise ValueError(f\"progressbar kwarg must be one of {valid_progress}\") if progressbar is None: df[mols_column_name] = df[smiles_column_name].apply( lambda x: Chem.MolFromSmiles(x) ) else: if progressbar == \"notebook\": tqdmn().pandas(desc=\"mols\") elif progressbar == \"terminal\": tqdm.pandas(desc=\"mols\") df[mols_column_name] = df[smiles_column_name].progress_apply( lambda x: Chem.MolFromSmiles(x) ) if drop_nulls: df = df.dropna(subset=[mols_column_name]) df = df.reset_index(drop=True) return df","title":"smiles2mol()"},{"location":"api/engineering/","text":"Engineering Engineering-specific data cleaning functions. convert_units(df, column_name=None, existing_units=None, to_units=None, dest_column_name=None) Converts a column of numeric values from one unit to another. Functional usage example: Method chaining usage example: Example: >>> import pandas as pd >>> import janitor.engineering >>> df = pd.DataFrame({\"temp_F\": [-40, 112]}) >>> df = df.convert_units( ... column_name='temp_F', ... existing_units='degF', ... to_units='degC', ... dest_column_name='temp_C' ... ) >>> df temp_F temp_C 0 -40 -40.000000 1 112 44.444444 Unit conversion can only take place if the existing_units and to_units are of the same type (e.g., temperature or pressure). The provided unit types can be any unit name or alternate name provided in the unyt package's Listing of Units table . Volume units are not provided natively in unyt . However, exponents are supported, and therefore some volume units can be converted. For example, a volume in cubic centimeters can be converted to cubic meters using existing_units='cm**3' and to_units='m**3' . Note : This method mutates the original DataFrame. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str Name of the column containing numeric values that are to be converted from one set of units to another. None existing_units str The unit type to convert from. None to_units str The unit type to convert to. None dest_column_name str The name of the new column containing the converted values that will be created. None Returns: Type Description DataFrame A pandas DataFrame with a new column of unit-converted values. Exceptions: Type Description TypeError if column is not numeric. Source code in janitor/engineering.py @pf.register_dataframe_method def convert_units( df: pd.DataFrame, column_name: str = None, existing_units: str = None, to_units: str = None, dest_column_name: str = None, ) -> pd.DataFrame: \"\"\" Converts a column of numeric values from one unit to another. Functional usage example: Method chaining usage example: Example: >>> import pandas as pd >>> import janitor.engineering >>> df = pd.DataFrame({\"temp_F\": [-40, 112]}) >>> df = df.convert_units( ... column_name='temp_F', ... existing_units='degF', ... to_units='degC', ... dest_column_name='temp_C' ... ) >>> df temp_F temp_C 0 -40 -40.000000 1 112 44.444444 Unit conversion can only take place if the `existing_units` and `to_units` are of the same type (e.g., temperature or pressure). The provided unit types can be any unit name or alternate name provided in the `unyt` package's [Listing of Units table]( https://unyt.readthedocs.io/en/stable/unit_listing.html#unit-listing). Volume units are not provided natively in `unyt`. However, exponents are supported, and therefore some volume units can be converted. For example, a volume in cubic centimeters can be converted to cubic meters using `existing_units='cm**3'` and `to_units='m**3'`. **Note**: This method mutates the original DataFrame. :param df: A pandas DataFrame. :param column_name: Name of the column containing numeric values that are to be converted from one set of units to another. :param existing_units: The unit type to convert from. :param to_units: The unit type to convert to. :param dest_column_name: The name of the new column containing the converted values that will be created. :returns: A pandas DataFrame with a new column of unit-converted values. :raises TypeError: if column is not numeric. \"\"\" # Check all inputs are correct data type check(\"column_name\", column_name, [str]) check(\"existing_units\", existing_units, [str]) check(\"to_units\", to_units, [str]) check(\"dest_column_name\", dest_column_name, [str]) # Check that column_name is a numeric column if not np.issubdtype(df[column_name].dtype, np.number): raise TypeError(f\"{column_name} must be a numeric column.\") original_vals = df[column_name].to_numpy() * unyt.Unit(existing_units) converted_vals = original_vals.to(to_units) df[dest_column_name] = np.array(converted_vals) return df","title":"Engineering"},{"location":"api/engineering/#engineering","text":"Engineering-specific data cleaning functions.","title":"Engineering"},{"location":"api/engineering/#janitor.engineering.convert_units","text":"Converts a column of numeric values from one unit to another. Functional usage example: Method chaining usage example: Example: >>> import pandas as pd >>> import janitor.engineering >>> df = pd.DataFrame({\"temp_F\": [-40, 112]}) >>> df = df.convert_units( ... column_name='temp_F', ... existing_units='degF', ... to_units='degC', ... dest_column_name='temp_C' ... ) >>> df temp_F temp_C 0 -40 -40.000000 1 112 44.444444 Unit conversion can only take place if the existing_units and to_units are of the same type (e.g., temperature or pressure). The provided unit types can be any unit name or alternate name provided in the unyt package's Listing of Units table . Volume units are not provided natively in unyt . However, exponents are supported, and therefore some volume units can be converted. For example, a volume in cubic centimeters can be converted to cubic meters using existing_units='cm**3' and to_units='m**3' . Note : This method mutates the original DataFrame. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str Name of the column containing numeric values that are to be converted from one set of units to another. None existing_units str The unit type to convert from. None to_units str The unit type to convert to. None dest_column_name str The name of the new column containing the converted values that will be created. None Returns: Type Description DataFrame A pandas DataFrame with a new column of unit-converted values. Exceptions: Type Description TypeError if column is not numeric. Source code in janitor/engineering.py @pf.register_dataframe_method def convert_units( df: pd.DataFrame, column_name: str = None, existing_units: str = None, to_units: str = None, dest_column_name: str = None, ) -> pd.DataFrame: \"\"\" Converts a column of numeric values from one unit to another. Functional usage example: Method chaining usage example: Example: >>> import pandas as pd >>> import janitor.engineering >>> df = pd.DataFrame({\"temp_F\": [-40, 112]}) >>> df = df.convert_units( ... column_name='temp_F', ... existing_units='degF', ... to_units='degC', ... dest_column_name='temp_C' ... ) >>> df temp_F temp_C 0 -40 -40.000000 1 112 44.444444 Unit conversion can only take place if the `existing_units` and `to_units` are of the same type (e.g., temperature or pressure). The provided unit types can be any unit name or alternate name provided in the `unyt` package's [Listing of Units table]( https://unyt.readthedocs.io/en/stable/unit_listing.html#unit-listing). Volume units are not provided natively in `unyt`. However, exponents are supported, and therefore some volume units can be converted. For example, a volume in cubic centimeters can be converted to cubic meters using `existing_units='cm**3'` and `to_units='m**3'`. **Note**: This method mutates the original DataFrame. :param df: A pandas DataFrame. :param column_name: Name of the column containing numeric values that are to be converted from one set of units to another. :param existing_units: The unit type to convert from. :param to_units: The unit type to convert to. :param dest_column_name: The name of the new column containing the converted values that will be created. :returns: A pandas DataFrame with a new column of unit-converted values. :raises TypeError: if column is not numeric. \"\"\" # Check all inputs are correct data type check(\"column_name\", column_name, [str]) check(\"existing_units\", existing_units, [str]) check(\"to_units\", to_units, [str]) check(\"dest_column_name\", dest_column_name, [str]) # Check that column_name is a numeric column if not np.issubdtype(df[column_name].dtype, np.number): raise TypeError(f\"{column_name} must be a numeric column.\") original_vals = df[column_name].to_numpy() * unyt.Unit(existing_units) converted_vals = original_vals.to(to_units) df[dest_column_name] = np.array(converted_vals) return df","title":"convert_units()"},{"location":"api/finance/","text":"Finance Finance-specific data cleaning functions. convert_currency(df, api_key, column_name=None, from_currency=None, to_currency=None, historical_date=None, make_new_column=False) Deprecated function. Source code in janitor/finance.py @pf.register_dataframe_method @deprecated_alias(colname=\"column_name\") def convert_currency( df: pd.DataFrame, api_key: str, column_name: str = None, from_currency: str = None, to_currency: str = None, historical_date: date = None, make_new_column: bool = False, ) -> pd.DataFrame: \"\"\"Deprecated function.\"\"\" raise JanitorError( \"The `convert_currency` function has been temporarily disabled due to \" \"exchangeratesapi.io disallowing free pinging of its API. \" \"(Our tests started to fail due to this issue.) \" \"There is no easy way around this problem \" \"except to find a new API to call on.\" \"Please comment on issue #829 \" \"(https://github.com/pyjanitor-devs/pyjanitor/issues/829) \" \"if you know of an alternative API that we can call on, \" \"otherwise the function will be removed in pyjanitor's 1.0 release.\" ) convert_stock(stock_symbol) This function takes in a stock symbol as a parameter, queries an API for the companies full name and returns it Functional usage example: import janitor.finance janitor.finance.convert_stock(\"aapl\") Parameters: Name Type Description Default stock_symbol str Stock ticker Symbol required Returns: Type Description str Full company name Exceptions: Type Description ConnectionError Internet connection is not available Source code in janitor/finance.py def convert_stock(stock_symbol: str) -> str: \"\"\" This function takes in a stock symbol as a parameter, queries an API for the companies full name and returns it Functional usage example: ```python import janitor.finance janitor.finance.convert_stock(\"aapl\") ``` :param stock_symbol: Stock ticker Symbol :raises ConnectionError: Internet connection is not available :returns: Full company name \"\"\" if is_connected(\"www.google.com\"): stock_symbol = stock_symbol.upper() return get_symbol(stock_symbol) else: raise ConnectionError( \"Connection Error: Client Not Connected to Internet\" ) get_symbol(symbol) This is a helper function to get a companies full name based on the stock symbol. Functional usage example: import janitor.finance janitor.finance.get_symbol(\"aapl\") Parameters: Name Type Description Default symbol str This is our stock symbol that we use to query the api for the companies full name. required Returns: Type Description Company full name Source code in janitor/finance.py def get_symbol(symbol: str): \"\"\" This is a helper function to get a companies full name based on the stock symbol. Functional usage example: ```python import janitor.finance janitor.finance.get_symbol(\"aapl\") ``` :param symbol: This is our stock symbol that we use to query the api for the companies full name. :return: Company full name \"\"\" result = requests.get( \"http://d.yimg.com/autoc.\" + \"finance.yahoo.com/autoc?query={}&region=1&lang=en\".format(symbol) ).json() for x in result[\"ResultSet\"][\"Result\"]: if x[\"symbol\"] == symbol: return x[\"name\"] else: return None inflate_currency(df, column_name=None, country=None, currency_year=None, to_year=None, make_new_column=False) Inflates a column of monetary values from one year to another, based on the currency's country. The provided country can be any economy name or code from the World Bank [list of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). Note : This method mutates the original DataFrame. Method chaining usage example: import pandas as pd import janitor.finance df = pd.DataFrame({\"profit\":[100.10, 200.20, 300.30, 400.40, 500.50]}) df profit 0 100.1 1 200.2 2 300.3 3 400.4 4 500.5 df.inflate_currency( ... column_name='profit', ... country='USA', ... currency_year=2015, ... to_year=2018, ... make_new_column=True ... ) profit profit_2018 0 100.1 106.050596 1 200.2 212.101191 2 300.3 318.151787 3 400.4 424.202382 4 500.5 530.252978 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str Name of the column containing monetary values to inflate. None country str The country associated with the currency being inflated. May be any economy or code from the World Bank [List of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). None currency_year int The currency year to inflate from. The year should be 1960 or later. None to_year int The currency year to inflate to. The year should be 1960 or later. None make_new_column bool Generates new column for inflated currency if True, otherwise, inflates currency in place. False Returns: Type Description DataFrame The dataframe with inflated currency column. Source code in janitor/finance.py @pf.register_dataframe_method def inflate_currency( df: pd.DataFrame, column_name: str = None, country: str = None, currency_year: int = None, to_year: int = None, make_new_column: bool = False, ) -> pd.DataFrame: \"\"\" Inflates a column of monetary values from one year to another, based on the currency's country. The provided country can be any economy name or code from the World Bank [list of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). **Note**: This method mutates the original DataFrame. Method chaining usage example: >>> import pandas as pd >>> import janitor.finance >>> df = pd.DataFrame({\"profit\":[100.10, 200.20, 300.30, 400.40, 500.50]}) >>> df profit 0 100.1 1 200.2 2 300.3 3 400.4 4 500.5 >>> df.inflate_currency( ... column_name='profit', ... country='USA', ... currency_year=2015, ... to_year=2018, ... make_new_column=True ... ) profit profit_2018 0 100.1 106.050596 1 200.2 212.101191 2 300.3 318.151787 3 400.4 424.202382 4 500.5 530.252978 :param df: A pandas DataFrame. :param column_name: Name of the column containing monetary values to inflate. :param country: The country associated with the currency being inflated. May be any economy or code from the World Bank [List of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). :param currency_year: The currency year to inflate from. The year should be 1960 or later. :param to_year: The currency year to inflate to. The year should be 1960 or later. :param make_new_column: Generates new column for inflated currency if True, otherwise, inflates currency in place. :returns: The dataframe with inflated currency column. \"\"\" inflator = _inflate_currency(country, currency_year, to_year) if make_new_column: new_column_name = column_name + \"_\" + str(to_year) df[new_column_name] = df[column_name] * inflator else: df[column_name] = df[column_name] * inflator return df","title":"Finance"},{"location":"api/finance/#finance","text":"Finance-specific data cleaning functions.","title":"Finance"},{"location":"api/finance/#janitor.finance.convert_currency","text":"Deprecated function. Source code in janitor/finance.py @pf.register_dataframe_method @deprecated_alias(colname=\"column_name\") def convert_currency( df: pd.DataFrame, api_key: str, column_name: str = None, from_currency: str = None, to_currency: str = None, historical_date: date = None, make_new_column: bool = False, ) -> pd.DataFrame: \"\"\"Deprecated function.\"\"\" raise JanitorError( \"The `convert_currency` function has been temporarily disabled due to \" \"exchangeratesapi.io disallowing free pinging of its API. \" \"(Our tests started to fail due to this issue.) \" \"There is no easy way around this problem \" \"except to find a new API to call on.\" \"Please comment on issue #829 \" \"(https://github.com/pyjanitor-devs/pyjanitor/issues/829) \" \"if you know of an alternative API that we can call on, \" \"otherwise the function will be removed in pyjanitor's 1.0 release.\" )","title":"convert_currency()"},{"location":"api/finance/#janitor.finance.convert_stock","text":"This function takes in a stock symbol as a parameter, queries an API for the companies full name and returns it Functional usage example: import janitor.finance janitor.finance.convert_stock(\"aapl\") Parameters: Name Type Description Default stock_symbol str Stock ticker Symbol required Returns: Type Description str Full company name Exceptions: Type Description ConnectionError Internet connection is not available Source code in janitor/finance.py def convert_stock(stock_symbol: str) -> str: \"\"\" This function takes in a stock symbol as a parameter, queries an API for the companies full name and returns it Functional usage example: ```python import janitor.finance janitor.finance.convert_stock(\"aapl\") ``` :param stock_symbol: Stock ticker Symbol :raises ConnectionError: Internet connection is not available :returns: Full company name \"\"\" if is_connected(\"www.google.com\"): stock_symbol = stock_symbol.upper() return get_symbol(stock_symbol) else: raise ConnectionError( \"Connection Error: Client Not Connected to Internet\" )","title":"convert_stock()"},{"location":"api/finance/#janitor.finance.get_symbol","text":"This is a helper function to get a companies full name based on the stock symbol. Functional usage example: import janitor.finance janitor.finance.get_symbol(\"aapl\") Parameters: Name Type Description Default symbol str This is our stock symbol that we use to query the api for the companies full name. required Returns: Type Description Company full name Source code in janitor/finance.py def get_symbol(symbol: str): \"\"\" This is a helper function to get a companies full name based on the stock symbol. Functional usage example: ```python import janitor.finance janitor.finance.get_symbol(\"aapl\") ``` :param symbol: This is our stock symbol that we use to query the api for the companies full name. :return: Company full name \"\"\" result = requests.get( \"http://d.yimg.com/autoc.\" + \"finance.yahoo.com/autoc?query={}&region=1&lang=en\".format(symbol) ).json() for x in result[\"ResultSet\"][\"Result\"]: if x[\"symbol\"] == symbol: return x[\"name\"] else: return None","title":"get_symbol()"},{"location":"api/finance/#janitor.finance.inflate_currency","text":"Inflates a column of monetary values from one year to another, based on the currency's country. The provided country can be any economy name or code from the World Bank [list of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). Note : This method mutates the original DataFrame. Method chaining usage example: import pandas as pd import janitor.finance df = pd.DataFrame({\"profit\":[100.10, 200.20, 300.30, 400.40, 500.50]}) df profit 0 100.1 1 200.2 2 300.3 3 400.4 4 500.5 df.inflate_currency( ... column_name='profit', ... country='USA', ... currency_year=2015, ... to_year=2018, ... make_new_column=True ... ) profit profit_2018 0 100.1 106.050596 1 200.2 212.101191 2 300.3 318.151787 3 400.4 424.202382 4 500.5 530.252978 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str Name of the column containing monetary values to inflate. None country str The country associated with the currency being inflated. May be any economy or code from the World Bank [List of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). None currency_year int The currency year to inflate from. The year should be 1960 or later. None to_year int The currency year to inflate to. The year should be 1960 or later. None make_new_column bool Generates new column for inflated currency if True, otherwise, inflates currency in place. False Returns: Type Description DataFrame The dataframe with inflated currency column. Source code in janitor/finance.py @pf.register_dataframe_method def inflate_currency( df: pd.DataFrame, column_name: str = None, country: str = None, currency_year: int = None, to_year: int = None, make_new_column: bool = False, ) -> pd.DataFrame: \"\"\" Inflates a column of monetary values from one year to another, based on the currency's country. The provided country can be any economy name or code from the World Bank [list of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). **Note**: This method mutates the original DataFrame. Method chaining usage example: >>> import pandas as pd >>> import janitor.finance >>> df = pd.DataFrame({\"profit\":[100.10, 200.20, 300.30, 400.40, 500.50]}) >>> df profit 0 100.1 1 200.2 2 300.3 3 400.4 4 500.5 >>> df.inflate_currency( ... column_name='profit', ... country='USA', ... currency_year=2015, ... to_year=2018, ... make_new_column=True ... ) profit profit_2018 0 100.1 106.050596 1 200.2 212.101191 2 300.3 318.151787 3 400.4 424.202382 4 500.5 530.252978 :param df: A pandas DataFrame. :param column_name: Name of the column containing monetary values to inflate. :param country: The country associated with the currency being inflated. May be any economy or code from the World Bank [List of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). :param currency_year: The currency year to inflate from. The year should be 1960 or later. :param to_year: The currency year to inflate to. The year should be 1960 or later. :param make_new_column: Generates new column for inflated currency if True, otherwise, inflates currency in place. :returns: The dataframe with inflated currency column. \"\"\" inflator = _inflate_currency(country, currency_year, to_year) if make_new_column: new_column_name = column_name + \"_\" + str(to_year) df[new_column_name] = df[column_name] * inflator else: df[column_name] = df[column_name] * inflator return df","title":"inflate_currency()"},{"location":"api/functions/","text":"Functions General Functions pyjanitor's general-purpose data cleaning functions. NOTE: Instructions for future contributors: Place the source code of the functions in a file named after the function. Place utility functions in the same file. If you use a utility function from another source file, please refactor it out to janitor.functions.utils . Import the function into this file so that it shows up in the top-level API. Sort the imports in alphabetical order. Try to group related functions together (e.g. see convert_date.py ) Never import utils. add_columns add_column(df, column_name, value, fill_remaining=False) Add a column to the dataframe. Intended to be the method-chaining alternative to: df[column_name] = value Example: Add a column of constant values to the dataframe. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_column(column_name=\"c\", value=1) a b c 0 0 a 1 1 1 b 1 2 2 c 1 Example: Add a column of different values to the dataframe. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_column(column_name=\"c\", value=list(\"efg\")) a b c 0 0 a e 1 1 b f 2 2 c g Example: Add a column using an iterator. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_column(column_name=\"c\", value=range(4, 7)) a b c 0 0 a 4 1 1 b 5 2 2 c 6 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str Name of the new column. Should be a string, in order for the column name to be compatible with the Feather binary format (this is a useful thing to have). required value Union[List[Any], Tuple[Any], Any] Either a single value, or a list/tuple of values. required fill_remaining bool If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. False Returns: Type Description DataFrame A pandas DataFrame with an added column. Exceptions: Type Description ValueError If attempting to add a column that already exists. ValueError If value has more elements that number of rows in the DataFrame. ValueError If attempting to add an iterable of values with a length not equal to the number of DataFrame rows. ValueError If value has length of 0 . Source code in janitor/functions/add_columns.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\") def add_column( df: pd.DataFrame, column_name: str, value: Union[List[Any], Tuple[Any], Any], fill_remaining: bool = False, ) -> pd.DataFrame: \"\"\"Add a column to the dataframe. Intended to be the method-chaining alternative to: ```python df[column_name] = value ``` Example: Add a column of constant values to the dataframe. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_column(column_name=\"c\", value=1) a b c 0 0 a 1 1 1 b 1 2 2 c 1 Example: Add a column of different values to the dataframe. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_column(column_name=\"c\", value=list(\"efg\")) a b c 0 0 a e 1 1 b f 2 2 c g Example: Add a column using an iterator. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_column(column_name=\"c\", value=range(4, 7)) a b c 0 0 a 4 1 1 b 5 2 2 c 6 :param df: A pandas DataFrame. :param column_name: Name of the new column. Should be a string, in order for the column name to be compatible with the Feather binary format (this is a useful thing to have). :param value: Either a single value, or a list/tuple of values. :param fill_remaining: If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. :returns: A pandas DataFrame with an added column. :raises ValueError: If attempting to add a column that already exists. :raises ValueError: If `value` has more elements that number of rows in the DataFrame. :raises ValueError: If attempting to add an iterable of values with a length not equal to the number of DataFrame rows. :raises ValueError: If `value` has length of `0`. \"\"\" check(\"column_name\", column_name, [str]) if column_name in df.columns: raise ValueError( f\"Attempted to add column that already exists: \" f\"{column_name}.\" ) nrows = len(df) if hasattr(value, \"__len__\") and not isinstance( value, (str, bytes, bytearray) ): len_value = len(value) # if `value` is a list, ndarray, etc. if len_value > nrows: raise ValueError( \"`value` has more elements than number of rows \" f\"in your `DataFrame`. vals: {len_value}, \" f\"df: {nrows}\" ) if len_value != nrows and not fill_remaining: raise ValueError( \"Attempted to add iterable of values with length\" \" not equal to number of DataFrame rows\" ) if not len_value: raise ValueError( \"`value` has to be an iterable of minimum length 1\" ) elif fill_remaining: # relevant if a scalar val was passed, yet fill_remaining == True len_value = 1 value = [value] df = df.copy() if fill_remaining: times_to_loop = int(np.ceil(nrows / len_value)) fill_values = list(value) * times_to_loop df[column_name] = fill_values[:nrows] else: df[column_name] = value return df add_columns(df, fill_remaining=False, **kwargs) Add multiple columns to the dataframe. This method does not mutate the original DataFrame. Method to augment add_column with ability to add multiple columns in one go. This replaces the need for multiple add_column calls. Usage is through supplying kwargs where the key is the col name and the values correspond to the values of the new DataFrame column. Values passed can be scalar or iterable (list, ndarray, etc.) Example: Inserting two more columns into a dataframe. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_columns(x=4, y=list(\"def\")) a b x y 0 0 a 4 d 1 1 b 4 e 2 2 c 4 f Parameters: Name Type Description Default df DataFrame A pandas dataframe. required fill_remaining bool If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. (Passed to add_column ) False **kwargs Column, value pairs which are looped through in add_column calls. {} Returns: Type Description DataFrame A pandas DataFrame with added columns. Source code in janitor/functions/add_columns.py @pf.register_dataframe_method def add_columns( df: pd.DataFrame, fill_remaining: bool = False, **kwargs, ) -> pd.DataFrame: \"\"\"Add multiple columns to the dataframe. This method does not mutate the original DataFrame. Method to augment [`add_column`][janitor.functions.add_columns.add_column] with ability to add multiple columns in one go. This replaces the need for multiple [`add_column`][janitor.functions.add_columns.add_column] calls. Usage is through supplying kwargs where the key is the col name and the values correspond to the values of the new DataFrame column. Values passed can be scalar or iterable (list, ndarray, etc.) Example: Inserting two more columns into a dataframe. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_columns(x=4, y=list(\"def\")) a b x y 0 0 a 4 d 1 1 b 4 e 2 2 c 4 f :param df: A pandas dataframe. :param fill_remaining: If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. (Passed to `add_column`) :param **kwargs: Column, value pairs which are looped through in `add_column` calls. :returns: A pandas DataFrame with added columns. \"\"\" # Note: error checking can pretty much be handled in `add_column` for col_name, values in kwargs.items(): df = df.add_column(col_name, values, fill_remaining=fill_remaining) return df also Implementation source for chainable function also . also(df, func, *args, **kwargs) Run a function with side effects. This function allows you to run an arbitrary function in the pyjanitor method chain. Doing so will let you do things like save the dataframe to disk midway while continuing to modify the dataframe afterwards. Example: >>> import pandas as pd >>> import janitor >>> df = ( ... pd.DataFrame({\"a\": [1, 2, 3], \"b\": list(\"abc\")}) ... .query(\"a > 1\") ... .also(lambda df: print(f\"DataFrame shape is: {df.shape}\")) ... .rename_column(old_column_name=\"a\", new_column_name=\"a_new\") ... .also(lambda df: df.to_csv(\"midpoint.csv\")) ... .also( ... lambda df: print(f\"Columns: {df.columns}\") ... ) ... ) DataFrame shape is: (2, 2) Columns: Index(['a_new', 'b'], dtype='object') Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required func Callable A function you would like to run in the method chain. It should take one DataFrame object as a parameter and have no return. If there is a return, it will be ignored. required args Optional arguments for func . () kwargs Optional keyword arguments for func . {} Returns: Type Description DataFrame The input pandas DataFrame, unmodified. Source code in janitor/functions/also.py @pf.register_dataframe_method def also(df: pd.DataFrame, func: Callable, *args, **kwargs) -> pd.DataFrame: \"\"\"Run a function with side effects. This function allows you to run an arbitrary function in the `pyjanitor` method chain. Doing so will let you do things like save the dataframe to disk midway while continuing to modify the dataframe afterwards. Example: >>> import pandas as pd >>> import janitor >>> df = ( ... pd.DataFrame({\"a\": [1, 2, 3], \"b\": list(\"abc\")}) ... .query(\"a > 1\") ... .also(lambda df: print(f\"DataFrame shape is: {df.shape}\")) ... .rename_column(old_column_name=\"a\", new_column_name=\"a_new\") ... .also(lambda df: df.to_csv(\"midpoint.csv\")) ... .also( ... lambda df: print(f\"Columns: {df.columns}\") ... ) ... ) DataFrame shape is: (2, 2) Columns: Index(['a_new', 'b'], dtype='object') :param df: A pandas DataFrame. :param func: A function you would like to run in the method chain. It should take one DataFrame object as a parameter and have no return. If there is a return, it will be ignored. :param args: Optional arguments for `func`. :param kwargs: Optional keyword arguments for `func`. :returns: The input pandas DataFrame, unmodified. \"\"\" # noqa: E501 func(df.copy(), *args, **kwargs) return df bin_numeric bin_numeric(df, from_column_name, to_column_name, bins=5, **kwargs) Generate a new column that labels bins for a specified numeric column. This method does not mutate the original DataFrame. A wrapper around the pandas cut() function to bin data of one column, generating a new column with the results. Example: Binning a numeric column with specific bin edges. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [3, 6, 9, 12, 15]}) >>> df.bin_numeric( ... from_column_name=\"a\", to_column_name=\"a_binned\", ... bins=[0, 5, 11, 15], ... ) a a_binned 0 3 (0, 5] 1 6 (5, 11] 2 9 (5, 11] 3 12 (11, 15] 4 15 (11, 15] Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required from_column_name str The column whose data you want binned. required to_column_name str The new column to be created with the binned data. required bins Union[int, Sequence[float], pandas.core.indexes.interval.IntervalIndex] The binning strategy to be utilized. Read the pd.cut documentation for more details. 5 **kwargs Additional kwargs to pass to pd.cut , except retbins . {} Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError If retbins is passed in as a kwarg. Source code in janitor/functions/bin_numeric.py @pf.register_dataframe_method @deprecated_alias( from_column=\"from_column_name\", to_column=\"to_column_name\", num_bins=\"bins\", ) def bin_numeric( df: pd.DataFrame, from_column_name: str, to_column_name: str, bins: Optional[Union[int, ScalarSequence, pd.IntervalIndex]] = 5, **kwargs, ) -> pd.DataFrame: \"\"\" Generate a new column that labels bins for a specified numeric column. This method does not mutate the original DataFrame. A wrapper around the pandas [`cut()`][pd_cut_docs] function to bin data of one column, generating a new column with the results. [pd_cut_docs]: https://pandas.pydata.org/docs/reference/api/pandas.cut.html Example: Binning a numeric column with specific bin edges. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [3, 6, 9, 12, 15]}) >>> df.bin_numeric( ... from_column_name=\"a\", to_column_name=\"a_binned\", ... bins=[0, 5, 11, 15], ... ) a a_binned 0 3 (0, 5] 1 6 (5, 11] 2 9 (5, 11] 3 12 (11, 15] 4 15 (11, 15] :param df: A pandas DataFrame. :param from_column_name: The column whose data you want binned. :param to_column_name: The new column to be created with the binned data. :param bins: The binning strategy to be utilized. Read the `pd.cut` documentation for more details. :param **kwargs: Additional kwargs to pass to `pd.cut`, except `retbins`. :return: A pandas DataFrame. :raises ValueError: If `retbins` is passed in as a kwarg. \"\"\" if \"retbins\" in kwargs: raise ValueError(\"`retbins` is not an acceptable keyword argument.\") check(\"from_column_name\", from_column_name, [str]) check(\"to_column_name\", to_column_name, [str]) check_column(df, from_column_name) df = df.assign( **{ to_column_name: pd.cut(df[from_column_name], bins=bins, **kwargs), } ) return df case_when case_when(df, *args, *, default=None, column_name) Create a column based on a condition or multiple conditions. Example usage: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"a\": [0, 0, 1, 2, \"hi\"], ... \"b\": [0, 3, 4, 5, \"bye\"], ... \"c\": [6, 7, 8, 9, \"wait\"], ... } ... ) >>> df a b c 0 0 0 6 1 0 3 7 2 1 4 8 3 2 5 9 4 hi bye wait >>> df.case_when( ... ((df.a == 0) & (df.b != 0)) | (df.c == \"wait\"), df.a, ... (df.b == 0) & (df.a == 0), \"x\", ... default = df.c, ... column_name = \"value\", ... ) a b c value 0 0 0 6 x 1 0 3 7 0 2 1 4 8 8 3 2 5 9 9 4 hi bye wait hi Similar to SQL and dplyr's case_when with inspiration from pydatatable if_else function. If your scenario requires direct replacement of values, pandas' replace method or map method should be better suited and more efficient; if the conditions check if a value is within a range of values, pandas' cut or qcut should be more efficient; np.where/np.select are also performant options. This function relies on pd.Series.mask method. When multiple conditions are satisfied, the first one is used. The variable *args parameters takes arguments of the form : condition0 , value0 , condition1 , value1 , ..., default . If condition0 evaluates to True , then assign value0 to column_name , if condition1 evaluates to True , then assign value1 to column_name , and so on. If none of the conditions evaluate to True , assign default to column_name . This function can be likened to SQL's case_when : CASE WHEN condition0 THEN value0 WHEN condition1 THEN value1 --- more conditions ELSE default END AS column_name compared to python's if-elif-else : if condition0: value0 elif condition1: value1 # more elifs else: default Version Changed 0.24.0 Added default parameter. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required args Variable argument of conditions and expected values. Takes the form condition0 , value0 , condition1 , value1 , ... . condition can be a 1-D boolean array, a callable, or a string. If condition is a callable, it should evaluate to a 1-D boolean array. The array should have the same length as the DataFrame. If it is a string, it is computed on the dataframe, via df.eval , and should return a 1-D boolean array. result can be a scalar, a 1-D array, or a callable. If result is a callable, it should evaluate to a 1-D array. For a 1-D array, it should have the same length as the DataFrame. () default Any scalar, 1-D array or callable. This is the element inserted in the output when all conditions evaluate to False. If callable, it should evaluate to a 1-D array. The 1-D array should be the same length as the DataFrame. None column_name str Name of column to assign results to. A new column is created, if it does not already exist in the DataFrame. required Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError if condition/value fails to evaluate. Source code in janitor/functions/case_when.py @pf.register_dataframe_method def case_when( df: pd.DataFrame, *args, default: Any = None, column_name: str ) -> pd.DataFrame: \"\"\" Create a column based on a condition or multiple conditions. Example usage: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"a\": [0, 0, 1, 2, \"hi\"], ... \"b\": [0, 3, 4, 5, \"bye\"], ... \"c\": [6, 7, 8, 9, \"wait\"], ... } ... ) >>> df a b c 0 0 0 6 1 0 3 7 2 1 4 8 3 2 5 9 4 hi bye wait >>> df.case_when( ... ((df.a == 0) & (df.b != 0)) | (df.c == \"wait\"), df.a, ... (df.b == 0) & (df.a == 0), \"x\", ... default = df.c, ... column_name = \"value\", ... ) a b c value 0 0 0 6 x 1 0 3 7 0 2 1 4 8 8 3 2 5 9 9 4 hi bye wait hi Similar to SQL and dplyr's case_when with inspiration from `pydatatable` if_else function. If your scenario requires direct replacement of values, pandas' `replace` method or `map` method should be better suited and more efficient; if the conditions check if a value is within a range of values, pandas' `cut` or `qcut` should be more efficient; `np.where/np.select` are also performant options. This function relies on `pd.Series.mask` method. When multiple conditions are satisfied, the first one is used. The variable `*args` parameters takes arguments of the form : `condition0`, `value0`, `condition1`, `value1`, ..., `default`. If `condition0` evaluates to `True`, then assign `value0` to `column_name`, if `condition1` evaluates to `True`, then assign `value1` to `column_name`, and so on. If none of the conditions evaluate to `True`, assign `default` to `column_name`. This function can be likened to SQL's `case_when`: ```sql CASE WHEN condition0 THEN value0 WHEN condition1 THEN value1 --- more conditions ELSE default END AS column_name ``` compared to python's `if-elif-else`: ```python if condition0: value0 elif condition1: value1 # more elifs else: default ``` !!! abstract \"Version Changed\" - 0.24.0 - Added `default` parameter. :param df: A pandas DataFrame. :param args: Variable argument of conditions and expected values. Takes the form `condition0`, `value0`, `condition1`, `value1`, ... . `condition` can be a 1-D boolean array, a callable, or a string. If `condition` is a callable, it should evaluate to a 1-D boolean array. The array should have the same length as the DataFrame. If it is a string, it is computed on the dataframe, via `df.eval`, and should return a 1-D boolean array. `result` can be a scalar, a 1-D array, or a callable. If `result` is a callable, it should evaluate to a 1-D array. For a 1-D array, it should have the same length as the DataFrame. :param default: scalar, 1-D array or callable. This is the element inserted in the output when all conditions evaluate to False. If callable, it should evaluate to a 1-D array. The 1-D array should be the same length as the DataFrame. :param column_name: Name of column to assign results to. A new column is created, if it does not already exist in the DataFrame. :raises ValueError: if condition/value fails to evaluate. :returns: A pandas DataFrame. \"\"\" # Preliminary checks on the case_when function. # The bare minimum checks are done; the remaining checks # are done within `pd.Series.mask`. check(\"column_name\", column_name, [str]) len_args = len(args) if len_args < 2: raise ValueError( \"At least two arguments are required for the `args` parameter\" ) if len_args % 2: if default is None: warnings.warn( \"The last argument in the variable arguments \" \"has been assigned as the default. \" \"Note however that this will be deprecated \" \"in a future release; use an even number \" \"of boolean conditions and values, \" \"and pass the default argument to the `default` \" \"parameter instead.\", DeprecationWarning, stacklevel=2, ) *args, default = args else: raise ValueError( \"The number of conditions and values do not match. \" f\"There are {len_args - len_args//2} conditions \" f\"and {len_args//2} values.\" ) booleans = [] replacements = [] for index, value in enumerate(args): if index % 2: if callable(value): value = apply_if_callable(value, df) replacements.append(value) else: if callable(value): value = apply_if_callable(value, df) elif isinstance(value, str): value = df.eval(value) booleans.append(value) if callable(default): default = apply_if_callable(default, df) if is_scalar(default): default = pd.Series([default]).repeat(len(df)) if not hasattr(default, \"shape\"): default = pd.Series([*default]) if isinstance(default, pd.Index): arr_ndim = default.nlevels else: arr_ndim = default.ndim if arr_ndim != 1: raise ValueError( \"The argument for the `default` parameter \" \"should either be a 1-D array, a scalar, \" \"or a callable that can evaluate to a 1-D array.\" ) if not isinstance(default, pd.Series): default = pd.Series(default) default.index = df.index # actual computation # ensures value assignment is on a first come basis booleans = booleans[::-1] replacements = replacements[::-1] for index, (condition, value) in enumerate(zip(booleans, replacements)): try: default = default.mask(condition, value) # error `feedoff` idea from SO # https://stackoverflow.com/a/46091127/7175713 except Exception as error: raise ValueError( f\"condition{index} and value{index} failed to evaluate. \" f\"Original error message: {error}\" ) from error return df.assign(**{column_name: default}) change_type change_type(df, column_name, dtype, ignore_exception=False) Change the type of a column. This method does not mutate the original DataFrame. Exceptions that are raised can be ignored. For example, if one has a mixed dtype column that has non-integer strings and integers, and you want to coerce everything to integers, you can optionally ignore the non-integer strings and replace them with NaN or keep the original value. Intended to be the method-chaining alternative to: df[col] = df[col].astype(dtype) Example: Change the type of a column. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": range(3), \"col2\": [\"m\", 5, True]}) >>> df col1 col2 0 0 m 1 1 5 2 2 True >>> df.change_type( ... \"col1\", dtype=str, ... ).change_type( ... \"col2\", dtype=float, ignore_exception=\"fillna\", ... ) col1 col2 0 0 NaN 1 1 5.0 2 2 1.0 Example: Change the type of multiple columns. Change the type of all columns, please use DataFrame.astype instead. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": range(3), \"col2\": [\"m\", 5, True]}) >>> df.change_type(['col1', 'col2'], str) col1 col2 0 0 m 1 1 5 2 2 True Parameters: Name Type Description Default df pd.DataFrame A pandas DataFrame. required column_name Hashable | list[Hashable] | pd.Index The column(s) in the dataframe. required dtype type The datatype to convert to. Should be one of the standard Python types, or a numpy datatype. required ignore_exception bool one of {False, \"fillna\", \"keep_values\"} . False Returns: Type Description pd.DataFrame A pandas DataFrame with changed column types. Exceptions: Type Description ValueError If unknown option provided for ignore_exception . Source code in janitor/functions/change_type.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def change_type( df: pd.DataFrame, column_name: Hashable | list[Hashable] | pd.Index, dtype: type, ignore_exception: bool = False, ) -> pd.DataFrame: \"\"\"Change the type of a column. This method does not mutate the original DataFrame. Exceptions that are raised can be ignored. For example, if one has a mixed dtype column that has non-integer strings and integers, and you want to coerce everything to integers, you can optionally ignore the non-integer strings and replace them with `NaN` or keep the original value. Intended to be the method-chaining alternative to: ```python df[col] = df[col].astype(dtype) ``` Example: Change the type of a column. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": range(3), \"col2\": [\"m\", 5, True]}) >>> df col1 col2 0 0 m 1 1 5 2 2 True >>> df.change_type( ... \"col1\", dtype=str, ... ).change_type( ... \"col2\", dtype=float, ignore_exception=\"fillna\", ... ) col1 col2 0 0 NaN 1 1 5.0 2 2 1.0 Example: Change the type of multiple columns. Change the type of all columns, please use `DataFrame.astype` instead. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": range(3), \"col2\": [\"m\", 5, True]}) >>> df.change_type(['col1', 'col2'], str) col1 col2 0 0 m 1 1 5 2 2 True :param df: A pandas DataFrame. :param column_name: The column(s) in the dataframe. :param dtype: The datatype to convert to. Should be one of the standard Python types, or a numpy datatype. :param ignore_exception: one of `{False, \"fillna\", \"keep_values\"}`. :returns: A pandas DataFrame with changed column types. :raises ValueError: If unknown option provided for `ignore_exception`. \"\"\" df = df.copy() # avoid mutating the original DataFrame if not ignore_exception: df[column_name] = df[column_name].astype(dtype) elif ignore_exception == \"keep_values\": df[column_name] = df[column_name].astype(dtype, errors=\"ignore\") elif ignore_exception == \"fillna\": if isinstance(column_name, Hashable): column_name = [column_name] df[column_name] = df[column_name].applymap(_convert, dtype=dtype) else: raise ValueError(\"Unknown option for ignore_exception\") return df clean_names Functions for cleaning columns names. clean_names(df, strip_underscores=None, case_type='lower', remove_special=False, strip_accents=True, preserve_original_columns=True, enforce_string=True, truncate_limit=None) Clean column names. Takes all column names, converts them to lowercase, then replaces all spaces with underscores. By default, column names are converted to string types. This can be switched off by passing in enforce_string=False . This method does not mutate the original DataFrame. Example usage: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"Aloha\": range(3), ... \"Bell Chart\": range(3), ... \"Animals@#$%^\": range(3) ... } ... ) >>> df Aloha Bell Chart Animals@#$%^ 0 0 0 0 1 1 1 1 2 2 2 2 >>> df.clean_names() aloha bell_chart animals@#$%^ 0 0 0 0 1 1 1 1 2 2 2 2 >>> df.clean_names(remove_special=True) aloha bell_chart animals 0 0 0 0 1 1 1 1 2 2 2 2 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required strip_underscores Union[str, bool] (optional) Removes the outer underscores from all column names. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True. None case_type str (optional) Whether to make columns lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase. 'lower' remove_special bool (optional) Remove special characters from columns. Only letters, numbers and underscores are preserved. False strip_accents bool Whether or not to remove accents from columns names. True preserve_original_columns bool (optional) Preserve original names. This is later retrievable using df.original_columns . True enforce_string bool Whether or not to convert all column names to string type. Defaults to True, but can be turned off. Columns with >1 levels will not be converted by default. True truncate_limit int (optional) Truncates formatted column names to the specified length. Default None does not truncate. None Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/clean_names.py @pf.register_dataframe_method def clean_names( df: pd.DataFrame, strip_underscores: Optional[Union[str, bool]] = None, case_type: str = \"lower\", remove_special: bool = False, strip_accents: bool = True, preserve_original_columns: bool = True, enforce_string: bool = True, truncate_limit: int = None, ) -> pd.DataFrame: \"\"\" Clean column names. Takes all column names, converts them to lowercase, then replaces all spaces with underscores. By default, column names are converted to string types. This can be switched off by passing in `enforce_string=False`. This method does not mutate the original DataFrame. Example usage: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"Aloha\": range(3), ... \"Bell Chart\": range(3), ... \"Animals@#$%^\": range(3) ... } ... ) >>> df Aloha Bell Chart Animals@#$%^ 0 0 0 0 1 1 1 1 2 2 2 2 >>> df.clean_names() aloha bell_chart animals@#$%^ 0 0 0 0 1 1 1 1 2 2 2 2 >>> df.clean_names(remove_special=True) aloha bell_chart animals 0 0 0 0 1 1 1 1 2 2 2 2 :param df: The pandas DataFrame object. :param strip_underscores: (optional) Removes the outer underscores from all column names. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True. :param case_type: (optional) Whether to make columns lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase. :param remove_special: (optional) Remove special characters from columns. Only letters, numbers and underscores are preserved. :param strip_accents: Whether or not to remove accents from columns names. :param preserve_original_columns: (optional) Preserve original names. This is later retrievable using `df.original_columns`. :param enforce_string: Whether or not to convert all column names to string type. Defaults to True, but can be turned off. Columns with >1 levels will not be converted by default. :param truncate_limit: (optional) Truncates formatted column names to the specified length. Default None does not truncate. :returns: A pandas DataFrame. \"\"\" original_column_names = list(df.columns) if enforce_string: df = df.rename(columns=str) df = df.rename(columns=lambda x: _change_case(x, case_type)) df = df.rename(columns=_normalize_1) if remove_special: df = df.rename(columns=_remove_special) if strip_accents: df = df.rename(columns=_strip_accents) df = df.rename(columns=lambda x: re.sub(\"_+\", \"_\", x)) # noqa: PD005 df = _strip_underscores(df, strip_underscores) df = df.rename(columns=lambda x: x[:truncate_limit]) # Store the original column names, if enabled by user if preserve_original_columns: df.__dict__[\"original_columns\"] = original_column_names return df coalesce Function for performing coalesce. coalesce(df, *column_names, *, target_column_name=None, default_value=None) Coalesce two or more columns of data in order of column names provided. Given the variable arguments of column names, coalesce finds and returns the first non-missing value from these columns, for every row in the input dataframe. If all the column values are null for a particular row, then the default_value will be filled in. If target_column_name is not provided, then the first column is coalesced. This method does not mutate the original DataFrame. Example: Use coalesce with 3 columns, \"a\", \"b\" and \"c\". >>> import pandas as pd >>> import numpy as np >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [np.nan, 1, np.nan], ... \"b\": [2, 3, np.nan], ... \"c\": [4, np.nan, np.nan], ... }) >>> df.coalesce(\"a\", \"b\", \"c\") a b c 0 2.0 2.0 4.0 1 1.0 3.0 NaN 2 NaN NaN NaN Example: Provide a target_column_name. >>> df.coalesce(\"a\", \"b\", \"c\", target_column_name=\"new_col\") a b c new_col 0 NaN 2.0 4.0 2.0 1 1.0 3.0 NaN 1.0 2 NaN NaN NaN NaN Example: Provide a default value. >>> import pandas as pd >>> import numpy as np >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [1, np.nan, np.nan], ... \"b\": [2, 3, np.nan], ... }) >>> df.coalesce( ... \"a\", \"b\", ... target_column_name=\"new_col\", ... default_value=-1, ... ) a b new_col 0 1.0 2.0 1.0 1 NaN 3.0 3.0 2 NaN NaN -1.0 This is more syntactic diabetes! For R users, this should look familiar to dplyr 's coalesce function; for Python users, the interface should be more intuitive than the pandas.Series.combine_first method. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names A list of column names. () target_column_name Optional[str] The new column name after combining. If None , then the first column in column_names is updated, with the Null values replaced. None default_value Union[int, float, str] A scalar to replace any remaining nulls after coalescing. None Returns: Type Description DataFrame A pandas DataFrame with coalesced columns. Exceptions: Type Description ValueError if length of column_names is less than 2. Source code in janitor/functions/coalesce.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\", new_column_name=\"target_column_name\") def coalesce( df: pd.DataFrame, *column_names, target_column_name: Optional[str] = None, default_value: Optional[Union[int, float, str]] = None, ) -> pd.DataFrame: \"\"\"Coalesce two or more columns of data in order of column names provided. Given the variable arguments of column names, `coalesce` finds and returns the first non-missing value from these columns, for every row in the input dataframe. If all the column values are null for a particular row, then the `default_value` will be filled in. If `target_column_name` is not provided, then the first column is coalesced. This method does not mutate the original DataFrame. Example: Use `coalesce` with 3 columns, \"a\", \"b\" and \"c\". >>> import pandas as pd >>> import numpy as np >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [np.nan, 1, np.nan], ... \"b\": [2, 3, np.nan], ... \"c\": [4, np.nan, np.nan], ... }) >>> df.coalesce(\"a\", \"b\", \"c\") a b c 0 2.0 2.0 4.0 1 1.0 3.0 NaN 2 NaN NaN NaN Example: Provide a target_column_name. >>> df.coalesce(\"a\", \"b\", \"c\", target_column_name=\"new_col\") a b c new_col 0 NaN 2.0 4.0 2.0 1 1.0 3.0 NaN 1.0 2 NaN NaN NaN NaN Example: Provide a default value. >>> import pandas as pd >>> import numpy as np >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [1, np.nan, np.nan], ... \"b\": [2, 3, np.nan], ... }) >>> df.coalesce( ... \"a\", \"b\", ... target_column_name=\"new_col\", ... default_value=-1, ... ) a b new_col 0 1.0 2.0 1.0 1 NaN 3.0 3.0 2 NaN NaN -1.0 This is more syntactic diabetes! For R users, this should look familiar to `dplyr`'s `coalesce` function; for Python users, the interface should be more intuitive than the `pandas.Series.combine_first` method. :param df: A pandas DataFrame. :param column_names: A list of column names. :param target_column_name: The new column name after combining. If `None`, then the first column in `column_names` is updated, with the Null values replaced. :param default_value: A scalar to replace any remaining nulls after coalescing. :returns: A pandas DataFrame with coalesced columns. :raises ValueError: if length of `column_names` is less than 2. \"\"\" if not column_names: return df if len(column_names) < 2: raise ValueError( \"The number of columns to coalesce should be a minimum of 2.\" ) indices = _select_index([*column_names], df, axis=\"columns\") column_names = df.columns[indices] if target_column_name: check(\"target_column_name\", target_column_name, [str]) if default_value: check(\"default_value\", default_value, [int, float, str]) if target_column_name is None: target_column_name = column_names[0] outcome = df.loc(axis=1)[column_names].bfill(axis=\"columns\").iloc[:, 0] if outcome.hasnans and (default_value is not None): outcome = outcome.fillna(default_value) return df.assign(**{target_column_name: outcome}) collapse_levels Implementation of the collapse_levels function. collapse_levels(df, sep='_') Flatten multi-level column dataframe to a single level. This method mutates the original DataFrame. Given a DataFrame containing multi-level columns, flatten to single-level by string-joining the column labels in each level. After a groupby / aggregate operation where .agg() is passed a list of multiple aggregation functions, a multi-level DataFrame is returned with the name of the function applied in the second level. It is sometimes convenient for later indexing to flatten out this multi-level configuration back into a single level. This function does this through a simple string-joining of all the names across different levels in a single column. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"class\": [\"bird\", \"bird\", \"bird\", \"mammal\", \"mammal\"], ... \"max_speed\": [389, 389, 24, 80, 21], ... \"type\": [\"falcon\", \"falcon\", \"parrot\", \"Lion\", \"Monkey\"], ... }) >>> df class max_speed type 0 bird 389 falcon 1 bird 389 falcon 2 bird 24 parrot 3 mammal 80 Lion 4 mammal 21 Monkey >>> grouped_df = df.groupby(\"class\").agg([\"mean\", \"median\"]) >>> grouped_df # doctest: +NORMALIZE_WHITESPACE max_speed mean median class bird 267.333333 389.0 mammal 50.500000 50.5 >>> grouped_df.collapse_levels(sep=\"_\") # doctest: +NORMALIZE_WHITESPACE max_speed_mean max_speed_median class bird 267.333333 389.0 mammal 50.500000 50.5 Before applying .collapse_levels , the .agg operation returns a multi-level column DataFrame whose columns are (level 1, level 2) : [(\"max_speed\", \"mean\"), (\"max_speed\", \"median\")] .collapse_levels then flattens the column MultiIndex into a single level index with names: [\"max_speed_mean\", \"max_speed_median\"] Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required sep str String separator used to join the column level names. '_' Returns: Type Description DataFrame A pandas DataFrame with single-level column index. Source code in janitor/functions/collapse_levels.py @pf.register_dataframe_method def collapse_levels(df: pd.DataFrame, sep: str = \"_\") -> pd.DataFrame: \"\"\"Flatten multi-level column dataframe to a single level. This method mutates the original DataFrame. Given a DataFrame containing multi-level columns, flatten to single-level by string-joining the column labels in each level. After a `groupby` / `aggregate` operation where `.agg()` is passed a list of multiple aggregation functions, a multi-level DataFrame is returned with the name of the function applied in the second level. It is sometimes convenient for later indexing to flatten out this multi-level configuration back into a single level. This function does this through a simple string-joining of all the names across different levels in a single column. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"class\": [\"bird\", \"bird\", \"bird\", \"mammal\", \"mammal\"], ... \"max_speed\": [389, 389, 24, 80, 21], ... \"type\": [\"falcon\", \"falcon\", \"parrot\", \"Lion\", \"Monkey\"], ... }) >>> df class max_speed type 0 bird 389 falcon 1 bird 389 falcon 2 bird 24 parrot 3 mammal 80 Lion 4 mammal 21 Monkey >>> grouped_df = df.groupby(\"class\").agg([\"mean\", \"median\"]) >>> grouped_df # doctest: +NORMALIZE_WHITESPACE max_speed mean median class bird 267.333333 389.0 mammal 50.500000 50.5 >>> grouped_df.collapse_levels(sep=\"_\") # doctest: +NORMALIZE_WHITESPACE max_speed_mean max_speed_median class bird 267.333333 389.0 mammal 50.500000 50.5 Before applying `.collapse_levels`, the `.agg` operation returns a multi-level column DataFrame whose columns are `(level 1, level 2)`: [(\"max_speed\", \"mean\"), (\"max_speed\", \"median\")] `.collapse_levels` then flattens the column MultiIndex into a single level index with names: [\"max_speed_mean\", \"max_speed_median\"] :param df: A pandas DataFrame. :param sep: String separator used to join the column level names. :returns: A pandas DataFrame with single-level column index. \"\"\" # noqa: E501 check(\"sep\", sep, [str]) # if already single-level, just return the DataFrame if not isinstance(df.columns, pd.MultiIndex): return df df.columns = [ sep.join(str(el) for el in tup if str(el) != \"\") for tup in df # noqa: PD011 ] return df complete complete(df, *columns, *, sort=False, by=None, fill_value=None, explicit=True) It is modeled after tidyr's complete function, and is a wrapper around expand_grid , pd.merge and pd.fillna . In a way, it is the inverse of pd.dropna , as it exposes implicitly missing rows. Combinations of column names or a list/tuple of column names, or even a dictionary of column names and new values are possible. MultiIndex columns are not supported. Example: >>> import pandas as pd >>> import janitor >>> import numpy as np >>> df = pd.DataFrame( ... { ... \"Year\": [1999, 2000, 2004, 1999, 2004], ... \"Taxon\": [ ... \"Saccharina\", ... \"Saccharina\", ... \"Saccharina\", ... \"Agarum\", ... \"Agarum\", ... ], ... \"Abundance\": [4, 5, 2, 1, 8], ... } ... ) >>> df Year Taxon Abundance 0 1999 Saccharina 4 1 2000 Saccharina 5 2 2004 Saccharina 2 3 1999 Agarum 1 4 2004 Agarum 8 Expose missing pairings of Year and Taxon : >>> df.complete(\"Year\", \"Taxon\", sort=True) Year Taxon Abundance 0 1999 Agarum 1.0 1 1999 Saccharina 4.0 2 2000 Agarum NaN 3 2000 Saccharina 5.0 4 2004 Agarum 8.0 5 2004 Saccharina 2.0 Expose missing years from 1999 to 2004 : >>> df.complete( ... {\"Year\": range(df.Year.min(), df.Year.max() + 1)}, ... \"Taxon\", ... sort=True ... ) Year Taxon Abundance 0 1999 Agarum 1.0 1 1999 Saccharina 4.0 2 2000 Agarum NaN 3 2000 Saccharina 5.0 4 2001 Agarum NaN 5 2001 Saccharina NaN 6 2002 Agarum NaN 7 2002 Saccharina NaN 8 2003 Agarum NaN 9 2003 Saccharina NaN 10 2004 Agarum 8.0 11 2004 Saccharina 2.0 Fill missing values: >>> df = pd.DataFrame( ... dict( ... group=(1, 2, 1, 2), ... item_id=(1, 2, 2, 3), ... item_name=(\"a\", \"a\", \"b\", \"b\"), ... value1=(1, np.nan, 3, 4), ... value2=range(4, 8), ... ) ... ) >>> df group item_id item_name value1 value2 0 1 1 a 1.0 4 1 2 2 a NaN 5 2 1 2 b 3.0 6 3 2 3 b 4.0 7 >>> df.complete( ... \"group\", ... (\"item_id\", \"item_name\"), ... fill_value={\"value1\": 0, \"value2\": 99}, ... sort=True ... ) group item_id item_name value1 value2 0 1 1 a 1 4 1 1 2 a 0 99 2 1 2 b 3 6 3 1 3 b 0 99 4 2 1 a 0 99 5 2 2 a 0 5 6 2 2 b 0 99 7 2 3 b 4 7 Limit the fill to only implicit missing values by setting explicit to False : >>> df.complete( ... \"group\", ... (\"item_id\", \"item_name\"), ... fill_value={\"value1\": 0, \"value2\": 99}, ... explicit=False, ... sort=True ... ) group item_id item_name value1 value2 0 1 1 a 1.0 4.0 1 1 2 a 0.0 99.0 2 1 2 b 3.0 6.0 3 1 3 b 0.0 99.0 4 2 1 a 0.0 99.0 5 2 2 a NaN 5.0 6 2 2 b 0.0 99.0 7 2 3 b 4.0 7.0 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required *columns This refers to the columns to be completed. It could be column labels (string type), a list/tuple of column labels, or a dictionary that pairs column labels with new values. () sort bool Sort DataFrame based on *columns. Default is False . False by Union[list, str] label or list of labels to group by. The explicit missing rows are returned per group. None fill_value Union[Dict, Any] Scalar value to use instead of NaN for missing combinations. A dictionary, mapping columns names to a scalar value is also accepted. None explicit bool Determines if only implicitly missing values should be filled ( False ), or all nulls existing in the dataframe ( True ). Default is True . explicit is applicable only if fill_value is not None . True Returns: Type Description DataFrame A pandas DataFrame with explicit missing rows, if any. Source code in janitor/functions/complete.py @pf.register_dataframe_method def complete( df: pd.DataFrame, *columns, sort: bool = False, by: Optional[Union[list, str]] = None, fill_value: Optional[Union[Dict, Any]] = None, explicit: bool = True, ) -> pd.DataFrame: \"\"\" It is modeled after tidyr's `complete` function, and is a wrapper around [`expand_grid`][janitor.functions.expand_grid.expand_grid], `pd.merge` and `pd.fillna`. In a way, it is the inverse of `pd.dropna`, as it exposes implicitly missing rows. Combinations of column names or a list/tuple of column names, or even a dictionary of column names and new values are possible. MultiIndex columns are not supported. Example: >>> import pandas as pd >>> import janitor >>> import numpy as np >>> df = pd.DataFrame( ... { ... \"Year\": [1999, 2000, 2004, 1999, 2004], ... \"Taxon\": [ ... \"Saccharina\", ... \"Saccharina\", ... \"Saccharina\", ... \"Agarum\", ... \"Agarum\", ... ], ... \"Abundance\": [4, 5, 2, 1, 8], ... } ... ) >>> df Year Taxon Abundance 0 1999 Saccharina 4 1 2000 Saccharina 5 2 2004 Saccharina 2 3 1999 Agarum 1 4 2004 Agarum 8 Expose missing pairings of `Year` and `Taxon`: >>> df.complete(\"Year\", \"Taxon\", sort=True) Year Taxon Abundance 0 1999 Agarum 1.0 1 1999 Saccharina 4.0 2 2000 Agarum NaN 3 2000 Saccharina 5.0 4 2004 Agarum 8.0 5 2004 Saccharina 2.0 Expose missing years from 1999 to 2004 : >>> df.complete( ... {\"Year\": range(df.Year.min(), df.Year.max() + 1)}, ... \"Taxon\", ... sort=True ... ) Year Taxon Abundance 0 1999 Agarum 1.0 1 1999 Saccharina 4.0 2 2000 Agarum NaN 3 2000 Saccharina 5.0 4 2001 Agarum NaN 5 2001 Saccharina NaN 6 2002 Agarum NaN 7 2002 Saccharina NaN 8 2003 Agarum NaN 9 2003 Saccharina NaN 10 2004 Agarum 8.0 11 2004 Saccharina 2.0 Fill missing values: >>> df = pd.DataFrame( ... dict( ... group=(1, 2, 1, 2), ... item_id=(1, 2, 2, 3), ... item_name=(\"a\", \"a\", \"b\", \"b\"), ... value1=(1, np.nan, 3, 4), ... value2=range(4, 8), ... ) ... ) >>> df group item_id item_name value1 value2 0 1 1 a 1.0 4 1 2 2 a NaN 5 2 1 2 b 3.0 6 3 2 3 b 4.0 7 >>> df.complete( ... \"group\", ... (\"item_id\", \"item_name\"), ... fill_value={\"value1\": 0, \"value2\": 99}, ... sort=True ... ) group item_id item_name value1 value2 0 1 1 a 1 4 1 1 2 a 0 99 2 1 2 b 3 6 3 1 3 b 0 99 4 2 1 a 0 99 5 2 2 a 0 5 6 2 2 b 0 99 7 2 3 b 4 7 Limit the fill to only implicit missing values by setting explicit to `False`: >>> df.complete( ... \"group\", ... (\"item_id\", \"item_name\"), ... fill_value={\"value1\": 0, \"value2\": 99}, ... explicit=False, ... sort=True ... ) group item_id item_name value1 value2 0 1 1 a 1.0 4.0 1 1 2 a 0.0 99.0 2 1 2 b 3.0 6.0 3 1 3 b 0.0 99.0 4 2 1 a 0.0 99.0 5 2 2 a NaN 5.0 6 2 2 b 0.0 99.0 7 2 3 b 4.0 7.0 :param df: A pandas DataFrame. :param *columns: This refers to the columns to be completed. It could be column labels (string type), a list/tuple of column labels, or a dictionary that pairs column labels with new values. :param sort: Sort DataFrame based on *columns. Default is `False`. :param by: label or list of labels to group by. The explicit missing rows are returned per group. :param fill_value: Scalar value to use instead of NaN for missing combinations. A dictionary, mapping columns names to a scalar value is also accepted. :param explicit: Determines if only implicitly missing values should be filled (`False`), or all nulls existing in the dataframe (`True`). Default is `True`. `explicit` is applicable only if `fill_value` is not `None`. :returns: A pandas DataFrame with explicit missing rows, if any. \"\"\" if not columns: return df df = df.copy() return _computations_complete(df, columns, sort, by, fill_value, explicit) concatenate_columns concatenate_columns(df, column_names, new_column_name, sep='-', ignore_empty=True) Concatenates the set of columns into a single column. Used to quickly generate an index based on a group of columns. This method mutates the original DataFrame. Example: Concatenate two columns row-wise. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [1, 3, 5], \"b\": list(\"xyz\")}) >>> df a b 0 1 x 1 3 y 2 5 z >>> df.concatenate_columns( ... column_names=[\"a\", \"b\"], new_column_name=\"m\", ... ) a b m 0 1 x 1-x 1 3 y 3-y 2 5 z 5-z Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names List[Hashable] A list of columns to concatenate together. required new_column_name Hashable The name of the new column. required sep str The separator between each column's data. '-' ignore_empty bool Ignore null values if exists. True Returns: Type Description DataFrame A pandas DataFrame with concatenated columns. Exceptions: Type Description JanitorError If at least two columns are not provided within column_names . Source code in janitor/functions/concatenate_columns.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def concatenate_columns( df: pd.DataFrame, column_names: List[Hashable], new_column_name: Hashable, sep: str = \"-\", ignore_empty: bool = True, ) -> pd.DataFrame: \"\"\"Concatenates the set of columns into a single column. Used to quickly generate an index based on a group of columns. This method mutates the original DataFrame. Example: Concatenate two columns row-wise. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [1, 3, 5], \"b\": list(\"xyz\")}) >>> df a b 0 1 x 1 3 y 2 5 z >>> df.concatenate_columns( ... column_names=[\"a\", \"b\"], new_column_name=\"m\", ... ) a b m 0 1 x 1-x 1 3 y 3-y 2 5 z 5-z :param df: A pandas DataFrame. :param column_names: A list of columns to concatenate together. :param new_column_name: The name of the new column. :param sep: The separator between each column's data. :param ignore_empty: Ignore null values if exists. :returns: A pandas DataFrame with concatenated columns. :raises JanitorError: If at least two columns are not provided within `column_names`. \"\"\" if len(column_names) < 2: raise JanitorError(\"At least two columns must be specified\") df[new_column_name] = ( df[column_names].astype(str).fillna(\"\").agg(sep.join, axis=1) ) if ignore_empty: def remove_empty_string(x): \"\"\"Ignore empty/null string values from the concatenated output.\"\"\" return sep.join(x for x in x.split(sep) if x) df[new_column_name] = df[new_column_name].transform( remove_empty_string ) return df conditional_join conditional_join(df, right, *conditions, *, how='inner', sort_by_appearance=False, df_columns=None, right_columns=None, keep='all', use_numba=False) The conditional_join function operates similarly to pd.merge , but allows joins on inequality operators, or a combination of equi and non-equi joins. Joins solely on equality are not supported. If the join is solely on equality, pd.merge function covers that; if you are interested in nearest joins, or rolling joins, then pd.merge_asof covers that. There is also pandas' IntervalIndex, which is efficient for range joins, especially if the intervals do not overlap. Column selection in df_columns and right_columns is possible using the select_columns syntax. For strictly non-equi joins, involving either > , < , >= , <= operators, performance could be improved by setting use_numba to True . This assumes that numba is installed. To preserve row order, set sort_by_appearance to True . This function returns rows, if any, where values from df meet the condition(s) for values from right . The conditions are passed in as a variable argument of tuples, where the tuple is of the form (left_on, right_on, op) ; left_on is the column label from df , right_on is the column label from right , while op is the operator. For multiple conditions, the and( & ) operator is used to combine the results of the individual conditions. The operator can be any of == , != , <= , < , >= , > . The join is done only on the columns. MultiIndex columns are not supported. For non-equi joins, only numeric and date columns are supported. Only inner , left , and right joins are supported. If the columns from df and right have nothing in common, a single index column is returned; else, a MultiIndex column is returned. Example: >>> import pandas as pd >>> import janitor >>> df1 = pd.DataFrame({\"value_1\": [2, 5, 7, 1, 3, 4]}) >>> df2 = pd.DataFrame({\"value_2A\": [0, 3, 7, 12, 0, 2, 3, 1], ... \"value_2B\": [1, 5, 9, 15, 1, 4, 6, 3], ... }) >>> df1 value_1 0 2 1 5 2 7 3 1 4 3 5 4 >>> df2 value_2A value_2B 0 0 1 1 3 5 2 7 9 3 12 15 4 0 1 5 2 4 6 3 6 7 1 3 >>> df1.conditional_join( ... df2, ... (\"value_1\", \"value_2A\", \">\"), ... (\"value_1\", \"value_2B\", \"<\") ... ) value_1 value_2A value_2B 0 2 1 3 1 5 3 6 2 3 2 4 3 4 3 5 4 4 3 6 Version Changed 0.24.0 Added df_columns , right_columns , keep and use_numba parameters. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required right Union[pandas.core.frame.DataFrame, pandas.core.series.Series] Named Series or DataFrame to join to. required conditions Variable argument of tuple(s) of the form (left_on, right_on, op) , where left_on is the column label from df , right_on is the column label from right , while op is the operator. The operator can be any of == , != , <= , < , >= , > . For multiple conditions, the and( & ) operator is used to combine the results of the individual conditions. () how Literal['inner', 'left', 'right'] Indicates the type of join to be performed. It can be one of inner , left , right . Full outer join is not supported. Defaults to inner . 'inner' sort_by_appearance bool Default is False . This is useful for scenarios where the user wants the original order maintained. If True and how = left , the row order from the left dataframe is preserved; if True and how = right , the row order from the right dataframe is preserved. False df_columns Optional[Any] Columns to select from df . It can be a single column or a list of columns. It is also possible to rename the output columns via a dictionary. None right_columns Optional[Any] Columns to select from right . It can be a single column or a list of columns. It is also possible to rename the output columns via a dictionary. None keep Literal['first', 'last', 'all'] Choose whether to return the first match, last match or all matches. Default is all . 'all' use_numba bool Use numba, if installed, to accelerate the computation. Applicable only to strictly non-equi joins. Default is False . False Returns: Type Description DataFrame A pandas DataFrame of the two merged Pandas objects. Source code in janitor/functions/conditional_join.py @pf.register_dataframe_method def conditional_join( df: pd.DataFrame, right: Union[pd.DataFrame, pd.Series], *conditions, how: Literal[\"inner\", \"left\", \"right\"] = \"inner\", sort_by_appearance: bool = False, df_columns: Optional[Any] = None, right_columns: Optional[Any] = None, keep: Literal[\"first\", \"last\", \"all\"] = \"all\", use_numba: bool = False, ) -> pd.DataFrame: \"\"\" The conditional_join function operates similarly to `pd.merge`, but allows joins on inequality operators, or a combination of equi and non-equi joins. Joins solely on equality are not supported. If the join is solely on equality, `pd.merge` function covers that; if you are interested in nearest joins, or rolling joins, then `pd.merge_asof` covers that. There is also pandas' IntervalIndex, which is efficient for range joins, especially if the intervals do not overlap. Column selection in `df_columns` and `right_columns` is possible using the [`select_columns`][janitor.functions.select.select_columns] syntax. For strictly non-equi joins, involving either `>`, `<`, `>=`, `<=` operators, performance could be improved by setting `use_numba` to `True`. This assumes that `numba` is installed. To preserve row order, set `sort_by_appearance` to `True`. This function returns rows, if any, where values from `df` meet the condition(s) for values from `right`. The conditions are passed in as a variable argument of tuples, where the tuple is of the form `(left_on, right_on, op)`; `left_on` is the column label from `df`, `right_on` is the column label from `right`, while `op` is the operator. For multiple conditions, the and(`&`) operator is used to combine the results of the individual conditions. The operator can be any of `==`, `!=`, `<=`, `<`, `>=`, `>`. The join is done only on the columns. MultiIndex columns are not supported. For non-equi joins, only numeric and date columns are supported. Only `inner`, `left`, and `right` joins are supported. If the columns from `df` and `right` have nothing in common, a single index column is returned; else, a MultiIndex column is returned. Example: >>> import pandas as pd >>> import janitor >>> df1 = pd.DataFrame({\"value_1\": [2, 5, 7, 1, 3, 4]}) >>> df2 = pd.DataFrame({\"value_2A\": [0, 3, 7, 12, 0, 2, 3, 1], ... \"value_2B\": [1, 5, 9, 15, 1, 4, 6, 3], ... }) >>> df1 value_1 0 2 1 5 2 7 3 1 4 3 5 4 >>> df2 value_2A value_2B 0 0 1 1 3 5 2 7 9 3 12 15 4 0 1 5 2 4 6 3 6 7 1 3 >>> df1.conditional_join( ... df2, ... (\"value_1\", \"value_2A\", \">\"), ... (\"value_1\", \"value_2B\", \"<\") ... ) value_1 value_2A value_2B 0 2 1 3 1 5 3 6 2 3 2 4 3 4 3 5 4 4 3 6 !!! abstract \"Version Changed\" - 0.24.0 - Added `df_columns`, `right_columns`, `keep` and `use_numba` parameters. :param df: A pandas DataFrame. :param right: Named Series or DataFrame to join to. :param conditions: Variable argument of tuple(s) of the form `(left_on, right_on, op)`, where `left_on` is the column label from `df`, `right_on` is the column label from `right`, while `op` is the operator. The operator can be any of `==`, `!=`, `<=`, `<`, `>=`, `>`. For multiple conditions, the and(`&`) operator is used to combine the results of the individual conditions. :param how: Indicates the type of join to be performed. It can be one of `inner`, `left`, `right`. Full outer join is not supported. Defaults to `inner`. :param sort_by_appearance: Default is `False`. This is useful for scenarios where the user wants the original order maintained. If `True` and `how = left`, the row order from the left dataframe is preserved; if `True` and `how = right`, the row order from the right dataframe is preserved. :param df_columns: Columns to select from `df`. It can be a single column or a list of columns. It is also possible to rename the output columns via a dictionary. :param right_columns: Columns to select from `right`. It can be a single column or a list of columns. It is also possible to rename the output columns via a dictionary. :param keep: Choose whether to return the first match, last match or all matches. Default is `all`. :param use_numba: Use numba, if installed, to accelerate the computation. Applicable only to strictly non-equi joins. Default is `False`. :returns: A pandas DataFrame of the two merged Pandas objects. \"\"\" # noqa: E501 return _conditional_join_compute( df, right, conditions, how, sort_by_appearance, df_columns, right_columns, keep, use_numba, ) convert_date convert_excel_date(df, column_name) Convert Excel's serial date format into Python datetime format. This method mutates the original DataFrame. Implementation is also from Stack Overflow Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"date\": [39690, 39690, 37118]}) >>> df date 0 39690 1 39690 2 37118 >>> df.convert_excel_date('date') date 0 2008-08-30 1 2008-08-30 2 2001-08-15 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable A column name. required Returns: Type Description DataFrame A pandas DataFrame with corrected dates. Exceptions: Type Description ValueError if there are non numeric values in the column. Source code in janitor/functions/convert_date.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def convert_excel_date( df: pd.DataFrame, column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert Excel's serial date format into Python datetime format. This method mutates the original DataFrame. Implementation is also from [Stack Overflow](https://stackoverflow.com/questions/38454403/convert-excel-style-date-with-pandas) Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"date\": [39690, 39690, 37118]}) >>> df date 0 39690 1 39690 2 37118 >>> df.convert_excel_date('date') date 0 2008-08-30 1 2008-08-30 2 2001-08-15 :param df: A pandas DataFrame. :param column_name: A column name. :returns: A pandas DataFrame with corrected dates. :raises ValueError: if there are non numeric values in the column. \"\"\" # noqa: E501 if not is_numeric_dtype(df[column_name]): raise ValueError( \"There are non-numeric values in the column. \\ All values must be numeric\" ) df[column_name] = pd.TimedeltaIndex( df[column_name], unit=\"d\" ) + dt.datetime( 1899, 12, 30 ) # noqa: W503 return df convert_matlab_date(df, column_name) Convert Matlab's serial date number into Python datetime format. Implementation is also from Stack Overflow This method mutates the original DataFrame. Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"date\": [737125.0, 737124.815863, 737124.4985, 737124]}) >>> df date 0 737125.000000 1 737124.815863 2 737124.498500 3 737124.000000 >>> df.convert_matlab_date('date') date 0 2018-03-06 00:00:00.000000 1 2018-03-05 19:34:50.563200 2 2018-03-05 11:57:50.399999 3 2018-03-05 00:00:00.000000 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable A column name. required Returns: Type Description DataFrame A pandas DataFrame with corrected dates. Source code in janitor/functions/convert_date.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def convert_matlab_date( df: pd.DataFrame, column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert Matlab's serial date number into Python datetime format. Implementation is also from [Stack Overflow](https://stackoverflow.com/questions/13965740/converting-matlabs-datenum-format-to-python) This method mutates the original DataFrame. Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"date\": [737125.0, 737124.815863, 737124.4985, 737124]}) >>> df date 0 737125.000000 1 737124.815863 2 737124.498500 3 737124.000000 >>> df.convert_matlab_date('date') date 0 2018-03-06 00:00:00.000000 1 2018-03-05 19:34:50.563200 2 2018-03-05 11:57:50.399999 3 2018-03-05 00:00:00.000000 :param df: A pandas DataFrame. :param column_name: A column name. :returns: A pandas DataFrame with corrected dates. \"\"\" # noqa: E501 days = pd.Series([dt.timedelta(v % 1) for v in df[column_name]]) df[column_name] = ( df[column_name].astype(int).apply(dt.datetime.fromordinal) + days - dt.timedelta(days=366) ) return df convert_unix_date(df, column_name) Convert unix epoch time into Python datetime format. Note that this ignores local tz and convert all timestamps to naive datetime based on UTC! This method mutates the original DataFrame. Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"date\": [1651510462, 53394822, 1126233195]}) >>> df date 0 1651510462 1 53394822 2 1126233195 >>> df.convert_unix_date('date') date 0 2022-05-02 16:54:22 1 1971-09-10 23:53:42 2 2005-09-09 02:33:15 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable A column name. required Returns: Type Description DataFrame A pandas DataFrame with corrected dates. Source code in janitor/functions/convert_date.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def convert_unix_date(df: pd.DataFrame, column_name: Hashable) -> pd.DataFrame: \"\"\" Convert unix epoch time into Python datetime format. Note that this ignores local tz and convert all timestamps to naive datetime based on UTC! This method mutates the original DataFrame. Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"date\": [1651510462, 53394822, 1126233195]}) >>> df date 0 1651510462 1 53394822 2 1126233195 >>> df.convert_unix_date('date') date 0 2022-05-02 16:54:22 1 1971-09-10 23:53:42 2 2005-09-09 02:33:15 :param df: A pandas DataFrame. :param column_name: A column name. :returns: A pandas DataFrame with corrected dates. \"\"\" try: df[column_name] = pd.to_datetime(df[column_name], unit=\"s\") except OutOfBoundsDatetime: # Indicates time is in milliseconds. df[column_name] = pd.to_datetime(df[column_name], unit=\"ms\") return df count_cumulative_unique Implementation of count_cumulative_unique. count_cumulative_unique(df, column_name, dest_column_name, case_sensitive=True) Generates a running total of cumulative unique values in a given column. A new column will be created containing a running count of unique values in the specified column. If case_sensitive is True , then the case of any letters will matter (i.e., a != A ); otherwise, the case of any letters will not matter. This method does not mutate the original DataFrame. Examples: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"letters\": list(\"aabABb\"), ... \"numbers\": range(4, 10), ... }) >>> df letters numbers 0 a 4 1 a 5 2 b 6 3 A 7 4 B 8 5 b 9 >>> df.count_cumulative_unique( ... column_name=\"letters\", ... dest_column_name=\"letters_unique_count\", ... ) letters numbers letters_unique_count 0 a 4 1 1 a 5 1 2 b 6 2 3 A 7 3 4 B 8 4 5 b 9 4 Example: Cumulative counts, ignoring casing. >>> df.count_cumulative_unique( ... column_name=\"letters\", ... dest_column_name=\"letters_unique_count\", ... case_sensitive=False, ... ) letters numbers letters_unique_count 0 a 4 1 1 a 5 1 2 b 6 2 3 A 7 2 4 B 8 2 5 b 9 2 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Name of the column containing values from which a running count of unique values will be created. required dest_column_name str The name of the new column containing the cumulative count of unique values that will be created. required case_sensitive bool Whether or not uppercase and lowercase letters will be considered equal. Only valid with string-like columns. True Returns: Type Description DataFrame A pandas DataFrame with a new column containing a cumulative count of unique values from another column. Exceptions: Type Description TypeError If case_sensitive is False when counting a non-string column_name . Source code in janitor/functions/count_cumulative_unique.py @pf.register_dataframe_method def count_cumulative_unique( df: pd.DataFrame, column_name: Hashable, dest_column_name: str, case_sensitive: bool = True, ) -> pd.DataFrame: \"\"\"Generates a running total of cumulative unique values in a given column. A new column will be created containing a running count of unique values in the specified column. If `case_sensitive` is `True`, then the case of any letters will matter (i.e., `a != A`); otherwise, the case of any letters will not matter. This method does not mutate the original DataFrame. Examples: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"letters\": list(\"aabABb\"), ... \"numbers\": range(4, 10), ... }) >>> df letters numbers 0 a 4 1 a 5 2 b 6 3 A 7 4 B 8 5 b 9 >>> df.count_cumulative_unique( ... column_name=\"letters\", ... dest_column_name=\"letters_unique_count\", ... ) letters numbers letters_unique_count 0 a 4 1 1 a 5 1 2 b 6 2 3 A 7 3 4 B 8 4 5 b 9 4 Example: Cumulative counts, ignoring casing. >>> df.count_cumulative_unique( ... column_name=\"letters\", ... dest_column_name=\"letters_unique_count\", ... case_sensitive=False, ... ) letters numbers letters_unique_count 0 a 4 1 1 a 5 1 2 b 6 2 3 A 7 2 4 B 8 2 5 b 9 2 :param df: A pandas DataFrame. :param column_name: Name of the column containing values from which a running count of unique values will be created. :param dest_column_name: The name of the new column containing the cumulative count of unique values that will be created. :param case_sensitive: Whether or not uppercase and lowercase letters will be considered equal. Only valid with string-like columns. :returns: A pandas DataFrame with a new column containing a cumulative count of unique values from another column. :raises TypeError: If `case_sensitive` is False when counting a non-string `column_name`. \"\"\" check_column(df, column_name) check_column(df, dest_column_name, present=False) counter = df[column_name] if not case_sensitive: try: # Make it so that the the same uppercase and lowercase # letter are treated as one unique value counter = counter.str.lower() except (AttributeError, TypeError) as e: # AttributeError is raised by pandas when .str is used on # non-string types, e.g. int. # TypeError is raised by pandas when .str.lower is used on a # forbidden string type, e.g. bytes. raise TypeError( \"case_sensitive=False can only be used with a string-like \" f\"type. Column {column_name} is {counter.dtype} type.\" ) from e counter = ( counter.groupby(counter, sort=False).cumcount().to_numpy(copy=False) ) counter = np.cumsum(counter == 0) return df.assign(**{dest_column_name: counter}) currency_column_to_numeric currency_column_to_numeric(df, column_name, cleaning_style=None, cast_non_numeric=None, fill_all_non_numeric=None, remove_non_numeric=False) Convert currency column to numeric. This method does not mutate the original DataFrame. This method allows one to take a column containing currency values, inadvertently imported as a string, and cast it as a float. This is usually the case when reading CSV files that were modified in Excel. Empty strings (i.e. '' ) are retained as NaN values. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a_col\": [\" 24.56\", \"-\", \"(12.12)\", \"1,000,000\"], ... \"d_col\": [\"\", \"foo\", \"1.23 dollars\", \"-1,000 yen\"], ... }) >>> df # doctest: +NORMALIZE_WHITESPACE a_col d_col 0 24.56 1 - foo 2 (12.12) 1.23 dollars 3 1,000,000 -1,000 yen The default cleaning style. >>> df.currency_column_to_numeric(\"d_col\") a_col d_col 0 24.56 NaN 1 - NaN 2 (12.12) 1.23 3 1,000,000 -1000.00 The accounting cleaning style. >>> df.currency_column_to_numeric(\"a_col\", cleaning_style=\"accounting\") # doctest: +NORMALIZE_WHITESPACE a_col d_col 0 24.56 1 0.00 foo 2 -12.12 1.23 dollars 3 1000000.00 -1,000 yen Valid cleaning styles are: None : Default cleaning is applied. Empty strings are always retained as NaN . Numbers, - , . are extracted and the resulting string is cast to a float. 'accounting' : Replaces numbers in parentheses with negatives, removes commas. Parameters: Name Type Description Default df DataFrame The pandas DataFrame. required column_name str The column containing currency values to modify. required cleaning_style Optional[str] What style of cleaning to perform. None cast_non_numeric Optional[dict] A dict of how to coerce certain strings to numeric type. For example, if there are values of 'REORDER' in the DataFrame, {'REORDER': 0} will cast all instances of 'REORDER' to 0. Only takes effect in the default cleaning style. None fill_all_non_numeric Union[float, int] Similar to cast_non_numeric , but fills all strings to the same value. For example, fill_all_non_numeric=1 , will make everything that doesn't coerce to a currency 1 . Only takes effect in the default cleaning style. None remove_non_numeric bool If set to True, rows of df that contain non-numeric values in the column_name column will be removed. Only takes effect in the default cleaning style. False Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError If cleaning_style is not one of the accepted styles. Source code in janitor/functions/currency_column_to_numeric.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\", type=\"cleaning_style\") def currency_column_to_numeric( df: pd.DataFrame, column_name: str, cleaning_style: Optional[str] = None, cast_non_numeric: Optional[dict] = None, fill_all_non_numeric: Optional[Union[float, int]] = None, remove_non_numeric: bool = False, ) -> pd.DataFrame: \"\"\"Convert currency column to numeric. This method does not mutate the original DataFrame. This method allows one to take a column containing currency values, inadvertently imported as a string, and cast it as a float. This is usually the case when reading CSV files that were modified in Excel. Empty strings (i.e. `''`) are retained as `NaN` values. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a_col\": [\" 24.56\", \"-\", \"(12.12)\", \"1,000,000\"], ... \"d_col\": [\"\", \"foo\", \"1.23 dollars\", \"-1,000 yen\"], ... }) >>> df # doctest: +NORMALIZE_WHITESPACE a_col d_col 0 24.56 1 - foo 2 (12.12) 1.23 dollars 3 1,000,000 -1,000 yen The default cleaning style. >>> df.currency_column_to_numeric(\"d_col\") a_col d_col 0 24.56 NaN 1 - NaN 2 (12.12) 1.23 3 1,000,000 -1000.00 The accounting cleaning style. >>> df.currency_column_to_numeric(\"a_col\", cleaning_style=\"accounting\") # doctest: +NORMALIZE_WHITESPACE a_col d_col 0 24.56 1 0.00 foo 2 -12.12 1.23 dollars 3 1000000.00 -1,000 yen Valid cleaning styles are: - `None`: Default cleaning is applied. Empty strings are always retained as `NaN`. Numbers, `-`, `.` are extracted and the resulting string is cast to a float. - `'accounting'`: Replaces numbers in parentheses with negatives, removes commas. :param df: The pandas DataFrame. :param column_name: The column containing currency values to modify. :param cleaning_style: What style of cleaning to perform. :param cast_non_numeric: A dict of how to coerce certain strings to numeric type. For example, if there are values of 'REORDER' in the DataFrame, `{'REORDER': 0}` will cast all instances of 'REORDER' to 0. Only takes effect in the default cleaning style. :param fill_all_non_numeric: Similar to `cast_non_numeric`, but fills all strings to the same value. For example, `fill_all_non_numeric=1`, will make everything that doesn't coerce to a currency `1`. Only takes effect in the default cleaning style. :param remove_non_numeric: If set to True, rows of `df` that contain non-numeric values in the `column_name` column will be removed. Only takes effect in the default cleaning style. :raises ValueError: If `cleaning_style` is not one of the accepted styles. :returns: A pandas DataFrame. \"\"\" # noqa: E501 check(\"column_name\", column_name, [str]) check_column(df, column_name) column_series = df[column_name] if cleaning_style == \"accounting\": df.loc[:, column_name] = df[column_name].apply( _clean_accounting_column ) return df if cleaning_style is not None: raise ValueError( \"`cleaning_style` is expected to be one of ('accounting', None). \" f\"Got {cleaning_style!r} instead.\" ) if cast_non_numeric: check(\"cast_non_numeric\", cast_non_numeric, [dict]) _make_cc_patrial = partial( _currency_column_to_numeric, cast_non_numeric=cast_non_numeric, ) column_series = column_series.apply(_make_cc_patrial) if remove_non_numeric: df = df.loc[column_series != \"\", :] # _replace_empty_string_with_none is applied here after the check on # remove_non_numeric since \"\" is our indicator that a string was coerced # in the original column column_series = _replace_empty_string_with_none(column_series) if fill_all_non_numeric is not None: check(\"fill_all_non_numeric\", fill_all_non_numeric, [int, float]) column_series = column_series.fillna(fill_all_non_numeric) column_series = _replace_original_empty_string_with_none(column_series) df = df.assign(**{column_name: pd.to_numeric(column_series)}) return df deconcatenate_column Implementation of deconcatenating columns. deconcatenate_column(df, column_name, sep=None, new_column_names=None, autoname=None, preserve_position=False) De-concatenates a single column into multiple columns. The column to de-concatenate can be either a collection (list, tuple, ...) which can be separated out with pd.Series.tolist() , or a string to slice based on sep . To determine this behaviour automatically, the first element in the column specified is inspected. If it is a string, then sep must be specified. Else, the function assumes that it is an iterable type (e.g. list or tuple ), and will attempt to deconcatenate by splitting the list. Given a column with string values, this is the inverse of the concatenate_columns function. Used to quickly split columns out of a single column. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"m\": [\"1-x\", \"2-y\", \"3-z\"]}) >>> df m 0 1-x 1 2-y 2 3-z >>> df.deconcatenate_column(\"m\", sep=\"-\", autoname=\"col\") m col1 col2 0 1-x 1 x 1 2-y 2 y 2 3-z 3 z The keyword argument preserve_position takes True or False boolean that controls whether the new_column_names will take the original position of the to-be-deconcatenated column_name : When preserve_position=False (default), df.columns change from [..., column_name, ...] to [..., column_name, ..., new_column_names] . In other words, the deconcatenated new columns are appended to the right of the original dataframe and the original column_name is NOT dropped. When preserve_position=True , df.column change from [..., column_name, ...] to [..., new_column_names, ...] . In other words, the deconcatenated new column will REPLACE the original column_name at its original position, and column_name itself is dropped. The keyword argument autoname accepts a base string and then automatically creates numbered column names based off the base string. For example, if col is passed in as the argument to autoname , and 4 columns are created, then the resulting columns will be named col1, col2, col3, col4 . Numbering is always 1-indexed, not 0-indexed, in order to make the column names human-friendly. This method does not mutate the original DataFrame. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column to split. required sep Optional[str] The separator delimiting the column's data. None new_column_names Union[List[str], Tuple[str]] A list of new column names post-splitting. None autoname str A base name for automatically naming the new columns. Takes precedence over new_column_names if both are provided. None preserve_position bool Boolean for whether or not to preserve original position of the column upon de-concatenation. False Returns: Type Description DataFrame A pandas DataFrame with a deconcatenated column. Exceptions: Type Description ValueError If column_name is not present in the DataFrame. ValueError If sep is not provided and the column values are of type str . ValueError If either new_column_names or autoname is not supplied. JanitorError If incorrect number of names is provided within new_column_names . Source code in janitor/functions/deconcatenate_column.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def deconcatenate_column( df: pd.DataFrame, column_name: Hashable, sep: Optional[str] = None, new_column_names: Optional[Union[List[str], Tuple[str]]] = None, autoname: str = None, preserve_position: bool = False, ) -> pd.DataFrame: \"\"\"De-concatenates a single column into multiple columns. The column to de-concatenate can be either a collection (list, tuple, ...) which can be separated out with `pd.Series.tolist()`, or a string to slice based on `sep`. To determine this behaviour automatically, the first element in the column specified is inspected. If it is a string, then `sep` must be specified. Else, the function assumes that it is an iterable type (e.g. `list` or `tuple`), and will attempt to deconcatenate by splitting the list. Given a column with string values, this is the inverse of the [`concatenate_columns`][janitor.functions.concatenate_columns.concatenate_columns] function. Used to quickly split columns out of a single column. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"m\": [\"1-x\", \"2-y\", \"3-z\"]}) >>> df m 0 1-x 1 2-y 2 3-z >>> df.deconcatenate_column(\"m\", sep=\"-\", autoname=\"col\") m col1 col2 0 1-x 1 x 1 2-y 2 y 2 3-z 3 z The keyword argument `preserve_position` takes `True` or `False` boolean that controls whether the `new_column_names` will take the original position of the to-be-deconcatenated `column_name`: - When `preserve_position=False` (default), `df.columns` change from `[..., column_name, ...]` to `[..., column_name, ..., new_column_names]`. In other words, the deconcatenated new columns are appended to the right of the original dataframe and the original `column_name` is NOT dropped. - When `preserve_position=True`, `df.column` change from `[..., column_name, ...]` to `[..., new_column_names, ...]`. In other words, the deconcatenated new column will REPLACE the original `column_name` at its original position, and `column_name` itself is dropped. The keyword argument `autoname` accepts a base string and then automatically creates numbered column names based off the base string. For example, if `col` is passed in as the argument to `autoname`, and 4 columns are created, then the resulting columns will be named `col1, col2, col3, col4`. Numbering is always 1-indexed, not 0-indexed, in order to make the column names human-friendly. This method does not mutate the original DataFrame. :param df: A pandas DataFrame. :param column_name: The column to split. :param sep: The separator delimiting the column's data. :param new_column_names: A list of new column names post-splitting. :param autoname: A base name for automatically naming the new columns. Takes precedence over `new_column_names` if both are provided. :param preserve_position: Boolean for whether or not to preserve original position of the column upon de-concatenation. :returns: A pandas DataFrame with a deconcatenated column. :raises ValueError: If `column_name` is not present in the DataFrame. :raises ValueError: If `sep` is not provided and the column values are of type `str`. :raises ValueError: If either `new_column_names` or `autoname` is not supplied. :raises JanitorError: If incorrect number of names is provided within `new_column_names`. \"\"\" # noqa: E501 if column_name not in df.columns: raise ValueError(f\"column name {column_name} not present in DataFrame\") if isinstance(df[column_name].iloc[0], str): if sep is None: raise ValueError( \"`sep` must be specified if the column values \" \"are of type `str`.\" ) df_deconcat = df[column_name].str.split(sep, expand=True) else: df_deconcat = pd.DataFrame( df[column_name].to_list(), columns=new_column_names, index=df.index ) if new_column_names is None and autoname is None: raise ValueError( \"One of `new_column_names` or `autoname` must be supplied.\" ) if autoname: new_column_names = [ f\"{autoname}{i}\" for i in range(1, df_deconcat.shape[1] + 1) ] if not len(new_column_names) == df_deconcat.shape[1]: raise JanitorError( f\"You need to provide {len(df_deconcat.shape[1])} names \" \"to `new_column_names`\" ) df_deconcat.columns = new_column_names df_new = pd.concat([df, df_deconcat], axis=1) if preserve_position: df_original = df.copy() cols = list(df_original.columns) index_original = cols.index(column_name) for i, col_new in enumerate(new_column_names): cols.insert(index_original + i, col_new) df_new = df_new.select_columns(cols).drop(columns=column_name) return df_new drop_constant_columns Implementation of drop_constant_columns. drop_constant_columns(df) Finds and drops the constant columns from a Pandas DataFrame. Example: >>> import pandas as pd >>> import janitor >>> data_dict = { ... \"a\": [1, 1, 1], ... \"b\": [1, 2, 3], ... \"c\": [1, 1, 1], ... \"d\": [\"rabbit\", \"leopard\", \"lion\"], ... \"e\": [\"Cambridge\", \"Shanghai\", \"Basel\"] ... } >>> df = pd.DataFrame(data_dict) >>> df a b c d e 0 1 1 1 rabbit Cambridge 1 1 2 1 leopard Shanghai 2 1 3 1 lion Basel >>> df.drop_constant_columns() b d e 0 1 rabbit Cambridge 1 2 leopard Shanghai 2 3 lion Basel Parameters: Name Type Description Default df DataFrame Input Pandas DataFrame required Returns: Type Description DataFrame The Pandas DataFrame with the constant columns dropped. Source code in janitor/functions/drop_constant_columns.py @pf.register_dataframe_method def drop_constant_columns(df: pd.DataFrame) -> pd.DataFrame: \"\"\" Finds and drops the constant columns from a Pandas DataFrame. Example: >>> import pandas as pd >>> import janitor >>> data_dict = { ... \"a\": [1, 1, 1], ... \"b\": [1, 2, 3], ... \"c\": [1, 1, 1], ... \"d\": [\"rabbit\", \"leopard\", \"lion\"], ... \"e\": [\"Cambridge\", \"Shanghai\", \"Basel\"] ... } >>> df = pd.DataFrame(data_dict) >>> df a b c d e 0 1 1 1 rabbit Cambridge 1 1 2 1 leopard Shanghai 2 1 3 1 lion Basel >>> df.drop_constant_columns() b d e 0 1 rabbit Cambridge 1 2 leopard Shanghai 2 3 lion Basel :param df: Input Pandas DataFrame :returns: The Pandas DataFrame with the constant columns dropped. \"\"\" # Find the constant columns constant_columns = [] for col in df.columns: if len(df[col].unique()) == 1: constant_columns.append(col) # Drop constant columns from df and return it return df.drop(labels=constant_columns, axis=1) drop_duplicate_columns Implementation for drop_duplicate_columns. drop_duplicate_columns(df, column_name, nth_index=0) Remove a duplicated column specified by column_name . Specifying nth_index=0 will remove the first column, nth_index=1 will remove the second column, and so on and so forth. The corresponding tidyverse R's library is: select(-<column_name>_<nth_index + 1>) Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": range(2, 5), ... \"b\": range(3, 6), ... \"A\": range(4, 7), ... \"a*\": range(6, 9), ... }).clean_names(remove_special=True) >>> df a b a a 0 2 3 4 6 1 3 4 5 7 2 4 5 6 8 >>> df.drop_duplicate_columns(column_name=\"a\", nth_index=1) a b a 0 2 3 6 1 3 4 7 2 4 5 8 Parameters: Name Type Description Default df DataFrame A pandas DataFrame required column_name Hashable Name of duplicated columns. required nth_index int Among the duplicated columns, select the nth column to drop. 0 Returns: Type Description DataFrame A pandas DataFrame Source code in janitor/functions/drop_duplicate_columns.py @pf.register_dataframe_method def drop_duplicate_columns( df: pd.DataFrame, column_name: Hashable, nth_index: int = 0 ) -> pd.DataFrame: \"\"\"Remove a duplicated column specified by `column_name`. Specifying `nth_index=0` will remove the first column, `nth_index=1` will remove the second column, and so on and so forth. The corresponding tidyverse R's library is: `select(-<column_name>_<nth_index + 1>)` Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": range(2, 5), ... \"b\": range(3, 6), ... \"A\": range(4, 7), ... \"a*\": range(6, 9), ... }).clean_names(remove_special=True) >>> df a b a a 0 2 3 4 6 1 3 4 5 7 2 4 5 6 8 >>> df.drop_duplicate_columns(column_name=\"a\", nth_index=1) a b a 0 2 3 6 1 3 4 7 2 4 5 8 :param df: A pandas DataFrame :param column_name: Name of duplicated columns. :param nth_index: Among the duplicated columns, select the nth column to drop. :return: A pandas DataFrame \"\"\" col_indexes = [ col_idx for col_idx, col_name in enumerate(df.columns) if col_name == column_name ] # Select the column to remove based on nth_index. removed_col_idx = col_indexes[nth_index] # Filter out columns except for the one to be removed. filtered_cols = [ c_i for c_i, _ in enumerate(df.columns) if c_i != removed_col_idx ] return df.iloc[:, filtered_cols] dropnotnull dropnotnull(df, column_name) Drop rows that do not have null values in the given column. This method does not mutate the original DataFrame. Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [1., np.NaN, 3.], \"b\": [None, \"y\", \"z\"]}) >>> df a b 0 1.0 None 1 NaN y 2 3.0 z >>> df.dropnotnull(\"a\") a b 1 NaN y >>> df.dropnotnull(\"b\") a b 0 1.0 None Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column name to drop rows from. required Returns: Type Description DataFrame A pandas DataFrame with dropped rows. Source code in janitor/functions/dropnotnull.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def dropnotnull(df: pd.DataFrame, column_name: Hashable) -> pd.DataFrame: \"\"\"Drop rows that do *not* have null values in the given column. This method does not mutate the original DataFrame. Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [1., np.NaN, 3.], \"b\": [None, \"y\", \"z\"]}) >>> df a b 0 1.0 None 1 NaN y 2 3.0 z >>> df.dropnotnull(\"a\") a b 1 NaN y >>> df.dropnotnull(\"b\") a b 0 1.0 None :param df: A pandas DataFrame. :param column_name: The column name to drop rows from. :returns: A pandas DataFrame with dropped rows. \"\"\" return df[pd.isna(df[column_name])] encode_categorical encode_categorical(df, column_names=None, **kwargs) Encode the specified columns with Pandas' category dtype . It is syntactic sugar around pd.Categorical . This method does not mutate the original DataFrame. Simply pass a string, or a sequence of column names to column_names ; alternatively, you can pass kwargs, where the keys are the column names and the values can either be None, sort , appearance or a 1-D array-like object. None: column is cast to an unordered categorical. sort : column is cast to an ordered categorical, with the order defined by the sort-order of the categories. appearance : column is cast to an ordered categorical, with the order defined by the order of appearance in the original column. 1d-array-like object: column is cast to an ordered categorical, with the categories and order as specified in the input array. column_names and kwargs parameters cannot be used at the same time. Example: Using column_names >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df foo bar 0 b 4 1 b 5 2 a 6 3 c 7 4 b 8 >>> df.dtypes foo object bar int64 dtype: object >>> enc_df = df.encode_categorical(column_names=\"foo\") >>> enc_df.dtypes foo category bar int64 dtype: object >>> enc_df[\"foo\"].cat.categories Index(['a', 'b', 'c'], dtype='object') >>> enc_df[\"foo\"].cat.ordered False Example: Using kwargs to specify an ordered categorical. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df.dtypes foo object bar int64 dtype: object >>> enc_df = df.encode_categorical(foo=\"appearance\") >>> enc_df.dtypes foo category bar int64 dtype: object >>> enc_df[\"foo\"].cat.categories Index(['b', 'a', 'c'], dtype='object') >>> enc_df[\"foo\"].cat.ordered True Parameters: Name Type Description Default df DataFrame A pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] A column name or an iterable (list or tuple) of column names. None **kwargs A mapping from column name to either None , 'sort' or 'appearance' , or a 1-D array. This is useful in creating categorical columns that are ordered, or if the user needs to explicitly specify the categories. {} Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError If both column_names and kwargs are provided. Source code in janitor/functions/encode_categorical.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def encode_categorical( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable] = None, **kwargs, ) -> pd.DataFrame: \"\"\"Encode the specified columns with Pandas' [category dtype][cat]. [cat]: http://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html It is syntactic sugar around `pd.Categorical`. This method does not mutate the original DataFrame. Simply pass a string, or a sequence of column names to `column_names`; alternatively, you can pass kwargs, where the keys are the column names and the values can either be None, `sort`, `appearance` or a 1-D array-like object. - None: column is cast to an unordered categorical. - `sort`: column is cast to an ordered categorical, with the order defined by the sort-order of the categories. - `appearance`: column is cast to an ordered categorical, with the order defined by the order of appearance in the original column. - 1d-array-like object: column is cast to an ordered categorical, with the categories and order as specified in the input array. `column_names` and `kwargs` parameters cannot be used at the same time. Example: Using `column_names` >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df foo bar 0 b 4 1 b 5 2 a 6 3 c 7 4 b 8 >>> df.dtypes foo object bar int64 dtype: object >>> enc_df = df.encode_categorical(column_names=\"foo\") >>> enc_df.dtypes foo category bar int64 dtype: object >>> enc_df[\"foo\"].cat.categories Index(['a', 'b', 'c'], dtype='object') >>> enc_df[\"foo\"].cat.ordered False Example: Using `kwargs` to specify an ordered categorical. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df.dtypes foo object bar int64 dtype: object >>> enc_df = df.encode_categorical(foo=\"appearance\") >>> enc_df.dtypes foo category bar int64 dtype: object >>> enc_df[\"foo\"].cat.categories Index(['b', 'a', 'c'], dtype='object') >>> enc_df[\"foo\"].cat.ordered True :param df: A pandas DataFrame object. :param column_names: A column name or an iterable (list or tuple) of column names. :param **kwargs: A mapping from column name to either `None`, `'sort'` or `'appearance'`, or a 1-D array. This is useful in creating categorical columns that are ordered, or if the user needs to explicitly specify the categories. :returns: A pandas DataFrame. :raises ValueError: If both `column_names` and `kwargs` are provided. \"\"\" # noqa: E501 if all((column_names, kwargs)): raise ValueError( \"Only one of `column_names` or `kwargs` can be provided.\" ) # column_names deal with only category dtype (unordered) # kwargs takes care of scenarios where user wants an ordered category # or user supplies specific categories to create the categorical if column_names is not None: check(\"column_names\", column_names, [list, tuple, Hashable]) if isinstance(column_names, Hashable): column_names = [column_names] check_column(df, column_names) dtypes = {col: \"category\" for col in column_names} return df.astype(dtypes) return _computations_as_categorical(df, **kwargs) expand_column Implementation for expand_column. expand_column(df, column_name, sep='|', concat=True) Expand a categorical column with multiple labels into dummy-coded columns. Super sugary syntax that wraps :py:meth: pandas.Series.str.get_dummies . This method does not mutate the original DataFrame. Functional usage syntax: >>> import pandas as pd >>> df = pd.DataFrame( ... { ... \"col1\": [\"A, B\", \"B, C, D\", \"E, F\", \"A, E, F\"], ... \"col2\": [1, 2, 3, 4], ... } ... ) >>> df = expand_column( ... df, ... column_name=\"col1\", ... sep=\", \" # note space in sep ... ) >>> df col1 col2 A B C D E F 0 A, B 1 1 1 0 0 0 0 1 B, C, D 2 0 1 1 1 0 0 2 E, F 3 0 0 0 0 1 1 3 A, E, F 4 1 0 0 0 1 1 Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = ( ... pd.DataFrame( ... { ... \"col1\": [\"A, B\", \"B, C, D\", \"E, F\", \"A, E, F\"], ... \"col2\": [1, 2, 3, 4], ... } ... ) ... .expand_column( ... column_name='col1', ... sep=', ' ... ) ... ) >>> df col1 col2 A B C D E F 0 A, B 1 1 1 0 0 0 0 1 B, C, D 2 0 1 1 1 0 0 2 E, F 3 0 0 0 0 1 1 3 A, E, F 4 1 0 0 0 1 1 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Which column to expand. required sep str The delimiter, same to :py:meth: ~pandas.Series.str.get_dummies 's sep , default as | . '|' concat bool Whether to return the expanded column concatenated to the original dataframe ( concat=True ), or to return it standalone ( concat=False ). True Returns: Type Description DataFrame A pandas DataFrame with an expanded column. Source code in janitor/functions/expand_column.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def expand_column( df: pd.DataFrame, column_name: Hashable, sep: str = \"|\", concat: bool = True, ) -> pd.DataFrame: \"\"\"Expand a categorical column with multiple labels into dummy-coded columns. Super sugary syntax that wraps :py:meth:`pandas.Series.str.get_dummies`. This method does not mutate the original DataFrame. Functional usage syntax: >>> import pandas as pd >>> df = pd.DataFrame( ... { ... \"col1\": [\"A, B\", \"B, C, D\", \"E, F\", \"A, E, F\"], ... \"col2\": [1, 2, 3, 4], ... } ... ) >>> df = expand_column( ... df, ... column_name=\"col1\", ... sep=\", \" # note space in sep ... ) >>> df col1 col2 A B C D E F 0 A, B 1 1 1 0 0 0 0 1 B, C, D 2 0 1 1 1 0 0 2 E, F 3 0 0 0 0 1 1 3 A, E, F 4 1 0 0 0 1 1 Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = ( ... pd.DataFrame( ... { ... \"col1\": [\"A, B\", \"B, C, D\", \"E, F\", \"A, E, F\"], ... \"col2\": [1, 2, 3, 4], ... } ... ) ... .expand_column( ... column_name='col1', ... sep=', ' ... ) ... ) >>> df col1 col2 A B C D E F 0 A, B 1 1 1 0 0 0 0 1 B, C, D 2 0 1 1 1 0 0 2 E, F 3 0 0 0 0 1 1 3 A, E, F 4 1 0 0 0 1 1 :param df: A pandas DataFrame. :param column_name: Which column to expand. :param sep: The delimiter, same to :py:meth:`~pandas.Series.str.get_dummies`'s `sep`, default as `|`. :param concat: Whether to return the expanded column concatenated to the original dataframe (`concat=True`), or to return it standalone (`concat=False`). :returns: A pandas DataFrame with an expanded column. \"\"\" # noqa: E501 expanded_df = df[column_name].str.get_dummies(sep=sep) if concat: df = df.join(expanded_df) return df return expanded_df expand_grid expand_grid(df=None, df_key=None, *, others=None) Creates a DataFrame from a cartesian combination of all inputs. It is not restricted to DataFrame; it can work with any list-like structure that is 1 or 2 dimensional. If method-chaining to a DataFrame, a string argument to df_key parameter must be provided. Data types are preserved in this function, including pandas' extension array dtypes. The output will always be a DataFrame, usually with a MultiIndex column, with the keys of the others dictionary serving as the top level columns. If a DataFrame with MultiIndex columns is part of the arguments in others , the columns are flattened, before the final DataFrame is generated. If a pandas Series/DataFrame is passed, and has a labeled index, or a MultiIndex index, the index is discarded; the final DataFrame will have a RangeIndex. The MultiIndexed DataFrame can be flattened using pyjanitor's collapse_levels method; the user can also decide to drop any of the levels, via pandas' droplevel method. Example: >>> import pandas as pd >>> import janitor as jn >>> df = pd.DataFrame({\"x\": [1, 2], \"y\": [2, 1]}) >>> data = {\"z\": [1, 2, 3]} >>> df.expand_grid(df_key=\"df\", others=data) df z x y 0 0 1 2 1 1 1 2 2 2 1 2 3 3 2 1 1 4 2 1 2 5 2 1 3 Expand_grid works with non-pandas objects: >>> data = {\"x\": [1, 2, 3], \"y\": [1, 2]} >>> jn.expand_grid(others=data) x y 0 0 0 1 1 1 1 2 2 2 1 3 2 2 4 3 1 5 3 2 Parameters: Name Type Description Default df Optional[pandas.core.frame.DataFrame] A pandas DataFrame. None df_key Optional[str] name of key for the dataframe. It becomes part of the column names of the dataframe. None others Optional[Dict] A dictionary that contains the data to be combined with the dataframe. If no dataframe exists, all inputs in others will be combined to create a DataFrame. None Returns: Type Description DataFrame A pandas DataFrame of the cartesian product. Exceptions: Type Description KeyError if there is a DataFrame and df_key is not provided. Source code in janitor/functions/expand_grid.py @pf.register_dataframe_method def expand_grid( df: Optional[pd.DataFrame] = None, df_key: Optional[str] = None, *, others: Optional[Dict] = None, ) -> pd.DataFrame: \"\"\" Creates a DataFrame from a cartesian combination of all inputs. It is not restricted to DataFrame; it can work with any list-like structure that is 1 or 2 dimensional. If method-chaining to a DataFrame, a string argument to `df_key` parameter must be provided. Data types are preserved in this function, including pandas' extension array dtypes. The output will always be a DataFrame, usually with a MultiIndex column, with the keys of the `others` dictionary serving as the top level columns. If a DataFrame with MultiIndex columns is part of the arguments in `others`, the columns are flattened, before the final DataFrame is generated. If a pandas Series/DataFrame is passed, and has a labeled index, or a MultiIndex index, the index is discarded; the final DataFrame will have a RangeIndex. The MultiIndexed DataFrame can be flattened using pyjanitor's [`collapse_levels`][janitor.functions.collapse_levels.collapse_levels] method; the user can also decide to drop any of the levels, via pandas' `droplevel` method. Example: >>> import pandas as pd >>> import janitor as jn >>> df = pd.DataFrame({\"x\": [1, 2], \"y\": [2, 1]}) >>> data = {\"z\": [1, 2, 3]} >>> df.expand_grid(df_key=\"df\", others=data) df z x y 0 0 1 2 1 1 1 2 2 2 1 2 3 3 2 1 1 4 2 1 2 5 2 1 3 Expand_grid works with non-pandas objects: >>> data = {\"x\": [1, 2, 3], \"y\": [1, 2]} >>> jn.expand_grid(others=data) x y 0 0 0 1 1 1 1 2 2 2 1 3 2 2 4 3 1 5 3 2 :param df: A pandas DataFrame. :param df_key: name of key for the dataframe. It becomes part of the column names of the dataframe. :param others: A dictionary that contains the data to be combined with the dataframe. If no dataframe exists, all inputs in `others` will be combined to create a DataFrame. :returns: A pandas DataFrame of the cartesian product. :raises KeyError: if there is a DataFrame and `df_key` is not provided. \"\"\" if not others: if df is not None: return df return check(\"others\", others, [dict]) # if there is a DataFrame, for the method chaining, # it must have a key, to create a name value pair if df is not None: df = df.copy() if not df_key: raise KeyError( \"Using `expand_grid` as part of a \" \"DataFrame method chain requires that \" \"a string argument be provided for \" \"the `df_key` parameter. \" ) check(\"df_key\", df_key, [str]) others = {**{df_key: df}, **others} return _computations_expand_grid(others) factorize_columns Implementation of the factorize_columns function factorize_columns(df, column_names, suffix='_enc', **kwargs) Converts labels into numerical data. This method will create a new column with the string _enc appended after the original column's name. This can be overriden with the suffix parameter. Internally, this method uses pandas factorize method. It takes in an optional suffix and keyword arguments also. An empty string as suffix will override the existing column. This method does not mutate the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df foo bar 0 b 4 1 b 5 2 a 6 3 c 7 4 b 8 >>> df.factorize_columns(column_names=\"foo\") foo bar foo_enc 0 b 4 0 1 b 5 0 2 a 6 1 3 c 7 2 4 b 8 0 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] A column name or an iterable (list or tuple) of column names. required suffix str Suffix to be used for the new column. An empty string suffix means, it will override the existing column. '_enc' **kwargs Keyword arguments. It takes any of the keyword arguments, which the pandas factorize method takes like sort , na_sentinel , size_hint . {} Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/factorize_columns.py @pf.register_dataframe_method def factorize_columns( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable], suffix: str = \"_enc\", **kwargs, ) -> pd.DataFrame: \"\"\" Converts labels into numerical data. This method will create a new column with the string `_enc` appended after the original column's name. This can be overriden with the suffix parameter. Internally, this method uses pandas `factorize` method. It takes in an optional suffix and keyword arguments also. An empty string as suffix will override the existing column. This method does not mutate the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df foo bar 0 b 4 1 b 5 2 a 6 3 c 7 4 b 8 >>> df.factorize_columns(column_names=\"foo\") foo bar foo_enc 0 b 4 0 1 b 5 0 2 a 6 1 3 c 7 2 4 b 8 0 :param df: The pandas DataFrame object. :param column_names: A column name or an iterable (list or tuple) of column names. :param suffix: Suffix to be used for the new column. An empty string suffix means, it will override the existing column. :param **kwargs: Keyword arguments. It takes any of the keyword arguments, which the pandas factorize method takes like `sort`, `na_sentinel`, `size_hint`. :returns: A pandas DataFrame. \"\"\" df = _factorize(df.copy(), column_names, suffix, **kwargs) return df fill fill_direction(df, **kwargs) Provide a method-chainable function for filling missing values in selected columns. It is a wrapper for pd.Series.ffill and pd.Series.bfill , and pairs the column name with one of up , down , updown , and downup . Example: >>> import pandas as pd >>> import janitor as jn >>> df = pd.DataFrame( ... { ... 'col1': [1, 2, 3, 4], ... 'col2': [None, 5, 6, 7], ... 'col3': [8, 9, 10, None], ... 'col4': [None, None, 11, None], ... 'col5': [None, 12, 13, None] ... } ... ) >>> df col1 col2 col3 col4 col5 0 1 NaN 8.0 NaN NaN 1 2 5.0 9.0 NaN 12.0 2 3 6.0 10.0 11.0 13.0 3 4 7.0 NaN NaN NaN >>> df.fill_direction( ... col2 = 'up', ... col3 = 'down', ... col4 = 'downup', ... col5 = 'updown' ... ) col1 col2 col3 col4 col5 0 1 5.0 8.0 11.0 12.0 1 2 5.0 9.0 11.0 12.0 2 3 6.0 10.0 11.0 13.0 3 4 7.0 10.0 11.0 13.0 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required kwargs Key - value pairs of columns and directions. Directions can be either down , up , updown (fill up then down) and downup (fill down then up). {} Returns: Type Description DataFrame A pandas DataFrame with modified column(s). Exceptions: Type Description ValueError if direction supplied is not one of down , up , updown , or downup . Source code in janitor/functions/fill.py @pf.register_dataframe_method def fill_direction(df: pd.DataFrame, **kwargs) -> pd.DataFrame: \"\"\" Provide a method-chainable function for filling missing values in selected columns. It is a wrapper for `pd.Series.ffill` and `pd.Series.bfill`, and pairs the column name with one of `up`, `down`, `updown`, and `downup`. Example: >>> import pandas as pd >>> import janitor as jn >>> df = pd.DataFrame( ... { ... 'col1': [1, 2, 3, 4], ... 'col2': [None, 5, 6, 7], ... 'col3': [8, 9, 10, None], ... 'col4': [None, None, 11, None], ... 'col5': [None, 12, 13, None] ... } ... ) >>> df col1 col2 col3 col4 col5 0 1 NaN 8.0 NaN NaN 1 2 5.0 9.0 NaN 12.0 2 3 6.0 10.0 11.0 13.0 3 4 7.0 NaN NaN NaN >>> df.fill_direction( ... col2 = 'up', ... col3 = 'down', ... col4 = 'downup', ... col5 = 'updown' ... ) col1 col2 col3 col4 col5 0 1 5.0 8.0 11.0 12.0 1 2 5.0 9.0 11.0 12.0 2 3 6.0 10.0 11.0 13.0 3 4 7.0 10.0 11.0 13.0 :param df: A pandas DataFrame. :param kwargs: Key - value pairs of columns and directions. Directions can be either `down`, `up`, `updown` (fill up then down) and `downup` (fill down then up). :returns: A pandas DataFrame with modified column(s). :raises ValueError: if direction supplied is not one of `down`, `up`, `updown`, or `downup`. \"\"\" if not kwargs: return df fill_types = {fill.name for fill in _FILLTYPE} for column_name, fill_type in kwargs.items(): check(\"column_name\", column_name, [str]) check(\"fill_type\", fill_type, [str]) if fill_type.upper() not in fill_types: raise ValueError( \"\"\" fill_type should be one of up, down, updown, or downup. \"\"\" ) check_column(df, kwargs) new_values = {} for column_name, fill_type in kwargs.items(): direction = _FILLTYPE[f\"{fill_type.upper()}\"].value if len(direction) == 1: direction = methodcaller(direction[0]) output = direction(df[column_name]) else: direction = [methodcaller(entry) for entry in direction] output = _chain_func(df[column_name], *direction) new_values[column_name] = output return df.assign(**new_values) fill_empty(df, column_names, value) Fill NaN values in specified columns with a given value. Super sugary syntax that wraps pandas.DataFrame.fillna . This method mutates the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... 'col1': [1, 2, 3], ... 'col2': [None, 4, None ], ... 'col3': [None, 5, 6] ... } ... ) >>> df col1 col2 col3 0 1 NaN NaN 1 2 4.0 5.0 2 3 NaN 6.0 >>> df.fill_empty(column_names = 'col2', value = 0) col1 col2 col3 0 1 0.0 NaN 1 2 4.0 5.0 2 3 0.0 6.0 >>> df.fill_empty(column_names = ['col2', 'col3'], value = 0) col1 col2 col3 0 1 0.0 0.0 1 2 4.0 5.0 2 3 0.0 6.0 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names Union[str, Iterable[str], Hashable] column_names: A column name or an iterable (list or tuple) of column names. If a single column name is passed in, then only that column will be filled; if a list or tuple is passed in, then those columns will all be filled with the same value. required value The value that replaces the NaN values. required Returns: Type Description DataFrame A pandas DataFrame with NaN values filled. Source code in janitor/functions/fill.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def fill_empty( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable], value ) -> pd.DataFrame: \"\"\" Fill `NaN` values in specified columns with a given value. Super sugary syntax that wraps `pandas.DataFrame.fillna`. This method mutates the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... 'col1': [1, 2, 3], ... 'col2': [None, 4, None ], ... 'col3': [None, 5, 6] ... } ... ) >>> df col1 col2 col3 0 1 NaN NaN 1 2 4.0 5.0 2 3 NaN 6.0 >>> df.fill_empty(column_names = 'col2', value = 0) col1 col2 col3 0 1 0.0 NaN 1 2 4.0 5.0 2 3 0.0 6.0 >>> df.fill_empty(column_names = ['col2', 'col3'], value = 0) col1 col2 col3 0 1 0.0 0.0 1 2 4.0 5.0 2 3 0.0 6.0 :param df: A pandas DataFrame. :param column_names: column_names: A column name or an iterable (list or tuple) of column names. If a single column name is passed in, then only that column will be filled; if a list or tuple is passed in, then those columns will all be filled with the same value. :param value: The value that replaces the `NaN` values. :returns: A pandas DataFrame with `NaN` values filled. \"\"\" check_column(df, column_names) return _fill_empty(df, column_names, value=value) filter filter_column_isin(df, column_name, iterable, complement=False) Filter a dataframe for values in a column that exist in the given iterable. This method does not mutate the original DataFrame. Assumes exact matching; fuzzy matching not implemented. Example: Filter the dataframe to retain rows for which names are exactly James or John . >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"names\": [\"Jane\", \"Jeremy\", \"John\"], \"foo\": list(\"xyz\")}) >>> df names foo 0 Jane x 1 Jeremy y 2 John z >>> df.filter_column_isin(column_name=\"names\", iterable=[\"James\", \"John\"]) names foo 2 John z This is the method-chaining alternative to: df = df[df[\"names\"].isin([\"James\", \"John\"])] If complement=True , then we will only get rows for which the names are neither James nor John . Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column on which to filter. required iterable Iterable An iterable. Could be a list, tuple, another pandas Series. required complement bool Whether to return the complement of the selection or not. False Returns: Type Description DataFrame A filtered pandas DataFrame. Exceptions: Type Description ValueError If iterable does not have a length of 1 or greater. Source code in janitor/functions/filter.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def filter_column_isin( df: pd.DataFrame, column_name: Hashable, iterable: Iterable, complement: bool = False, ) -> pd.DataFrame: \"\"\"Filter a dataframe for values in a column that exist in the given iterable. This method does not mutate the original DataFrame. Assumes exact matching; fuzzy matching not implemented. Example: Filter the dataframe to retain rows for which `names` are exactly `James` or `John`. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"names\": [\"Jane\", \"Jeremy\", \"John\"], \"foo\": list(\"xyz\")}) >>> df names foo 0 Jane x 1 Jeremy y 2 John z >>> df.filter_column_isin(column_name=\"names\", iterable=[\"James\", \"John\"]) names foo 2 John z This is the method-chaining alternative to: ```python df = df[df[\"names\"].isin([\"James\", \"John\"])] ``` If `complement=True`, then we will only get rows for which the names are neither `James` nor `John`. :param df: A pandas DataFrame. :param column_name: The column on which to filter. :param iterable: An iterable. Could be a list, tuple, another pandas Series. :param complement: Whether to return the complement of the selection or not. :returns: A filtered pandas DataFrame. :raises ValueError: If `iterable` does not have a length of `1` or greater. \"\"\" # noqa: E501 if len(iterable) == 0: raise ValueError( \"`iterable` kwarg must be given an iterable of length 1 \" \"or greater.\" ) criteria = df[column_name].isin(iterable) if complement: return df[~criteria] return df[criteria] filter_date(df, column_name, start_date=None, end_date=None, years=None, months=None, days=None, column_date_options=None, format=None) Filter a date-based column based on certain criteria. This method does not mutate the original DataFrame. Dates may be finicky and this function builds on top of the magic from the pandas to_datetime function that is able to parse dates well. Additional options to parse the date type of your column may be found at the official pandas documentation . Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": range(5, 9), ... \"dt\": [\"2021-11-12\", \"2021-12-15\", \"2022-01-03\", \"2022-01-09\"], ... }) >>> df a dt 0 5 2021-11-12 1 6 2021-12-15 2 7 2022-01-03 3 8 2022-01-09 >>> df.filter_date(\"dt\", start_date=\"2021-12-01\", end_date=\"2022-01-05\") a dt 1 6 2021-12-15 2 7 2022-01-03 >>> df.filter_date(\"dt\", years=[2021], months=[12]) a dt 1 6 2021-12-15 Note This method will cast your column to a Timestamp! Note This only affects the format of the start_date and end_date parameters. If there's an issue with the format of the DataFrame being parsed, you would pass {'format': your_format} to column_date_options . Parameters: Name Type Description Default df DataFrame The dataframe to filter on. required column_name Hashable The column which to apply the fraction transformation. required start_date Optional[datetime.date] The beginning date to use to filter the DataFrame. None end_date Optional[datetime.date] The end date to use to filter the DataFrame. None years Optional[List] The years to use to filter the DataFrame. None months Optional[List] The months to use to filter the DataFrame. None days Optional[List] The days to use to filter the DataFrame. None column_date_options Optional[Dict] Special options to use when parsing the date column in the original DataFrame. The options may be found at the official Pandas documentation. None format Optional[str] If you're using a format for start_date or end_date that is not recognized natively by pandas' to_datetime function, you may supply the format yourself. Python date and time formats may be found here . None Returns: Type Description DataFrame A filtered pandas DataFrame. Source code in janitor/functions/filter.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\", start=\"start_date\", end=\"end_date\") def filter_date( df: pd.DataFrame, column_name: Hashable, start_date: Optional[dt.date] = None, end_date: Optional[dt.date] = None, years: Optional[List] = None, months: Optional[List] = None, days: Optional[List] = None, column_date_options: Optional[Dict] = None, format: Optional[str] = None, # skipcq: PYL-W0622 ) -> pd.DataFrame: \"\"\"Filter a date-based column based on certain criteria. This method does not mutate the original DataFrame. Dates may be finicky and this function builds on top of the *magic* from the pandas `to_datetime` function that is able to parse dates well. Additional options to parse the date type of your column may be found at the official pandas [documentation][datetime]. [datetime]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": range(5, 9), ... \"dt\": [\"2021-11-12\", \"2021-12-15\", \"2022-01-03\", \"2022-01-09\"], ... }) >>> df a dt 0 5 2021-11-12 1 6 2021-12-15 2 7 2022-01-03 3 8 2022-01-09 >>> df.filter_date(\"dt\", start_date=\"2021-12-01\", end_date=\"2022-01-05\") a dt 1 6 2021-12-15 2 7 2022-01-03 >>> df.filter_date(\"dt\", years=[2021], months=[12]) a dt 1 6 2021-12-15 !!!note This method will cast your column to a Timestamp! !!!note This only affects the format of the `start_date` and `end_date` parameters. If there's an issue with the format of the DataFrame being parsed, you would pass `{'format': your_format}` to `column_date_options`. :param df: The dataframe to filter on. :param column_name: The column which to apply the fraction transformation. :param start_date: The beginning date to use to filter the DataFrame. :param end_date: The end date to use to filter the DataFrame. :param years: The years to use to filter the DataFrame. :param months: The months to use to filter the DataFrame. :param days: The days to use to filter the DataFrame. :param column_date_options: Special options to use when parsing the date column in the original DataFrame. The options may be found at the official Pandas documentation. :param format: If you're using a format for `start_date` or `end_date` that is not recognized natively by pandas' `to_datetime` function, you may supply the format yourself. Python date and time formats may be found [here](http://strftime.org/). :returns: A filtered pandas DataFrame. \"\"\" # noqa: E501 def _date_filter_conditions(conditions): \"\"\"Taken from: https://stackoverflow.com/a/13616382.\"\"\" return reduce(np.logical_and, conditions) if column_date_options: df.loc[:, column_name] = pd.to_datetime( df.loc[:, column_name], **column_date_options ) else: df.loc[:, column_name] = pd.to_datetime(df.loc[:, column_name]) _filter_list = [] if start_date: start_date = pd.to_datetime(start_date, format=format) _filter_list.append(df.loc[:, column_name] >= start_date) if end_date: end_date = pd.to_datetime(end_date, format=format) _filter_list.append(df.loc[:, column_name] <= end_date) if years: _filter_list.append(df.loc[:, column_name].dt.year.isin(years)) if months: _filter_list.append(df.loc[:, column_name].dt.month.isin(months)) if days: _filter_list.append(df.loc[:, column_name].dt.day.isin(days)) if start_date and end_date and start_date > end_date: warnings.warn( f\"Your start date of {start_date} is after your end date of \" f\"{end_date}. Is this intended?\" ) return df.loc[_date_filter_conditions(_filter_list), :] filter_on(df, criteria, complement=False) Return a dataframe filtered on a particular criteria. This method does not mutate the original DataFrame. This is super-sugary syntax that wraps the pandas .query() API, enabling users to use strings to quickly specify filters for filtering their dataframe. The intent is that filter_on as a verb better matches the intent of a pandas user than the verb query . This is intended to be the method-chaining equivalent of the following: df = df[df[\"score\"] < 3] Example: Filter students who failed an exam (scored less than 50). >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"student_id\": [\"S1\", \"S2\", \"S3\"], ... \"score\": [40, 60, 85], ... }) >>> df student_id score 0 S1 40 1 S2 60 2 S3 85 >>> df.filter_on(\"score < 50\", complement=False) student_id score 0 S1 40 Credit to Brant Peterson for the name. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required criteria str A filtering criteria that returns an array or Series of booleans, on which pandas can filter on. required complement bool Whether to return the complement of the filter or not. If set to True, then the rows for which the criteria is False are retained instead. False Returns: Type Description DataFrame A filtered pandas DataFrame. Source code in janitor/functions/filter.py @pf.register_dataframe_method def filter_on( df: pd.DataFrame, criteria: str, complement: bool = False, ) -> pd.DataFrame: \"\"\"Return a dataframe filtered on a particular criteria. This method does not mutate the original DataFrame. This is super-sugary syntax that wraps the pandas `.query()` API, enabling users to use strings to quickly specify filters for filtering their dataframe. The intent is that `filter_on` as a verb better matches the intent of a pandas user than the verb `query`. This is intended to be the method-chaining equivalent of the following: ```python df = df[df[\"score\"] < 3] ``` Example: Filter students who failed an exam (scored less than 50). >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"student_id\": [\"S1\", \"S2\", \"S3\"], ... \"score\": [40, 60, 85], ... }) >>> df student_id score 0 S1 40 1 S2 60 2 S3 85 >>> df.filter_on(\"score < 50\", complement=False) student_id score 0 S1 40 Credit to Brant Peterson for the name. :param df: A pandas DataFrame. :param criteria: A filtering criteria that returns an array or Series of booleans, on which pandas can filter on. :param complement: Whether to return the complement of the filter or not. If set to True, then the rows for which the criteria is False are retained instead. :returns: A filtered pandas DataFrame. \"\"\" if complement: return df.query(f\"not ({criteria})\") return df.query(criteria) filter_string(df, column_name, search_string, complement=False, case=True, flags=0, na=None, regex=True) Filter a string-based column according to whether it contains a substring. This is super sugary syntax that builds on top of pandas.Series.str.contains . It is meant to be the method-chaining equivalent of the following: df = df[df[column_name].str.contains(search_string)]] This method does not mutate the original DataFrame. Example: Retain rows whose column values contain a particular substring. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": range(3, 6), \"b\": [\"bear\", \"peeL\", \"sail\"]}) >>> df a b 0 3 bear 1 4 peeL 2 5 sail >>> df.filter_string(column_name=\"b\", search_string=\"ee\") a b 1 4 peeL >>> df.filter_string(column_name=\"b\", search_string=\"L\", case=False) a b 1 4 peeL 2 5 sail Example: Filter names does not contain '.' (disable regex mode). >>> import pandas as pd >>> import janitor >>> df = pd.Series([\"JoseChen\", \"Brian.Salvi\"], name=\"Name\").to_frame() >>> df Name 0 JoseChen 1 Brian.Salvi >>> df.filter_string(column_name=\"Name\", search_string=\".\", regex=False, complement=True) Name 0 JoseChen Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column to filter. The column should contain strings. required search_string str A regex pattern or a (sub-)string to search. required complement bool Whether to return the complement of the filter or not. If set to True, then the rows for which the string search fails are retained instead. False case bool If True, case sensitive. True flags int Flags to pass through to the re module, e.g. re.IGNORECASE. 0 na Fill value for missing values. The default depends on dtype of the array. For object-dtype, numpy.nan is used. For StringDtype , pandas.NA is used. None regex bool If True, assumes search_string is a regular expression. If False, treats the search_string as a literal string. True Returns: Type Description DataFrame A filtered pandas DataFrame. Source code in janitor/functions/filter.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def filter_string( df: pd.DataFrame, column_name: Hashable, search_string: str, complement: bool = False, case: bool = True, flags: int = 0, na=None, regex: bool = True, ) -> pd.DataFrame: \"\"\"Filter a string-based column according to whether it contains a substring. This is super sugary syntax that builds on top of `pandas.Series.str.contains`. It is meant to be the method-chaining equivalent of the following: ```python df = df[df[column_name].str.contains(search_string)]] ``` This method does not mutate the original DataFrame. Example: Retain rows whose column values contain a particular substring. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": range(3, 6), \"b\": [\"bear\", \"peeL\", \"sail\"]}) >>> df a b 0 3 bear 1 4 peeL 2 5 sail >>> df.filter_string(column_name=\"b\", search_string=\"ee\") a b 1 4 peeL >>> df.filter_string(column_name=\"b\", search_string=\"L\", case=False) a b 1 4 peeL 2 5 sail Example: Filter names does not contain `'.'` (disable regex mode). >>> import pandas as pd >>> import janitor >>> df = pd.Series([\"JoseChen\", \"Brian.Salvi\"], name=\"Name\").to_frame() >>> df Name 0 JoseChen 1 Brian.Salvi >>> df.filter_string(column_name=\"Name\", search_string=\".\", regex=False, complement=True) Name 0 JoseChen :param df: A pandas DataFrame. :param column_name: The column to filter. The column should contain strings. :param search_string: A regex pattern or a (sub-)string to search. :param complement: Whether to return the complement of the filter or not. If set to True, then the rows for which the string search fails are retained instead. :param case: If True, case sensitive. :param flags: Flags to pass through to the re module, e.g. re.IGNORECASE. :param na: Fill value for missing values. The default depends on dtype of the array. For object-dtype, `numpy.nan` is used. For `StringDtype`, `pandas.NA` is used. :param regex: If True, assumes `search_string` is a regular expression. If False, treats the `search_string` as a literal string. :returns: A filtered pandas DataFrame. \"\"\" # noqa: E501 criteria = df[column_name].str.contains( pat=search_string, case=case, flags=flags, na=na, regex=regex, ) if complement: return df[~criteria] return df[criteria] find_replace Implementation for find_replace. find_replace(df, match='exact', **mappings) Perform a find-and-replace action on provided columns. Depending on use case, users can choose either exact, full-value matching, or regular-expression-based fuzzy matching (hence allowing substring matching in the latter case). For strings, the matching is always case sensitive. For instance, given a DataFrame containing orders at a coffee shop: >>> df = pd.DataFrame({ ... \"customer\": [\"Mary\", \"Tom\", \"Lila\"], ... \"order\": [\"ice coffee\", \"lemonade\", \"regular coffee\"] ... }) >>> df customer order 0 Mary ice coffee 1 Tom lemonade 2 Lila regular coffee Our task is to replace values ice coffee and regular coffee of the order column into latte . Example 1 - exact matching (functional usage): >>> df = find_replace( ... df, ... match=\"exact\", ... order={\"ice coffee\": \"latte\", \"regular coffee\": \"latte\"}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte Example 1 - exact matching (method chaining): >>> df = df.find_replace( ... match=\"exact\", ... order={\"ice coffee\": \"latte\", \"regular coffee\": \"latte\"}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte Example 2 - Regular-expression-based matching (functional usage): >>> df = find_replace( ... df, ... match='regex', ... order={'coffee$': 'latte'}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte Example 2 - Regular-expression-based matching (method chaining usage): >>> df = df.find_replace( ... match='regex', ... order={'coffee$': 'latte'}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte To perform a find and replace on the entire DataFrame, pandas' df.replace() function provides the appropriate functionality. You can find more detail on the replace docs. This function only works with column names that have no spaces or punctuation in them. For example, a column name item_name would work with find_replace , because it is a contiguous string that can be parsed correctly, but item name would not be parsed correctly by the Python interpreter. If you have column names that might not be compatible, we recommend calling on clean_names() as the first method call. If, for whatever reason, that is not possible, then _find_replace is available as a function that you can do a pandas pipe call on. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required match str Whether or not to perform an exact match or not. Valid values are \"exact\" or \"regex\". 'exact' mappings keyword arguments corresponding to column names that have dictionaries passed in indicating what to find (keys) and what to replace with (values). {} Returns: Type Description DataFrame A pandas DataFrame with replaced values. Source code in janitor/functions/find_replace.py @pf.register_dataframe_method def find_replace( df: pd.DataFrame, match: str = \"exact\", **mappings ) -> pd.DataFrame: \"\"\" Perform a find-and-replace action on provided columns. Depending on use case, users can choose either exact, full-value matching, or regular-expression-based fuzzy matching (hence allowing substring matching in the latter case). For strings, the matching is always case sensitive. For instance, given a DataFrame containing orders at a coffee shop: >>> df = pd.DataFrame({ ... \"customer\": [\"Mary\", \"Tom\", \"Lila\"], ... \"order\": [\"ice coffee\", \"lemonade\", \"regular coffee\"] ... }) >>> df customer order 0 Mary ice coffee 1 Tom lemonade 2 Lila regular coffee Our task is to replace values `ice coffee` and `regular coffee` of the `order` column into `latte`. Example 1 - exact matching (functional usage): >>> df = find_replace( ... df, ... match=\"exact\", ... order={\"ice coffee\": \"latte\", \"regular coffee\": \"latte\"}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte Example 1 - exact matching (method chaining): >>> df = df.find_replace( ... match=\"exact\", ... order={\"ice coffee\": \"latte\", \"regular coffee\": \"latte\"}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte Example 2 - Regular-expression-based matching (functional usage): >>> df = find_replace( ... df, ... match='regex', ... order={'coffee$': 'latte'}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte Example 2 - Regular-expression-based matching (method chaining usage): >>> df = df.find_replace( ... match='regex', ... order={'coffee$': 'latte'}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte To perform a find and replace on the entire DataFrame, pandas' `df.replace()` function provides the appropriate functionality. You can find more detail on the [replace] docs. [replace]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html This function only works with column names that have no spaces or punctuation in them. For example, a column name `item_name` would work with `find_replace`, because it is a contiguous string that can be parsed correctly, but `item name` would not be parsed correctly by the Python interpreter. If you have column names that might not be compatible, we recommend calling on `clean_names()` as the first method call. If, for whatever reason, that is not possible, then `_find_replace` is available as a function that you can do a pandas [pipe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pipe.html) call on. :param df: A pandas DataFrame. :param match: Whether or not to perform an exact match or not. Valid values are \"exact\" or \"regex\". :param mappings: keyword arguments corresponding to column names that have dictionaries passed in indicating what to find (keys) and what to replace with (values). :returns: A pandas DataFrame with replaced values. \"\"\" # noqa: E501 for column_name, mapper in mappings.items(): df = _find_replace(df, column_name, mapper, match=match) return df flag_nulls flag_nulls(df, column_name='null_flag', columns=None) Creates a new column to indicate whether you have null values in a given row. If the columns parameter is not set, looks across the entire DataFrame, otherwise will look only in the columns you set. This method does not mutate the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [\"w\", \"x\", None, \"z\"], \"b\": [5, None, 7, 8], ... }) >>> df.flag_nulls() a b null_flag 0 w 5.0 0 1 x NaN 1 2 None 7.0 1 3 z 8.0 0 >>> df.flag_nulls(columns=\"b\") a b null_flag 0 w 5.0 0 1 x NaN 1 2 None 7.0 0 3 z 8.0 0 Parameters: Name Type Description Default df DataFrame Input pandas DataFrame. required column_name Optional[Hashable] Name for the output column. 'null_flag' columns Union[str, Iterable[str], Hashable] List of columns to look at for finding null values. If you only want to look at one column, you can simply give its name. If set to None (default), all DataFrame columns are used. None Returns: Type Description DataFrame Input dataframe with the null flag column. Exceptions: Type Description ValueError if column_name is already present in the DataFrame. ValueError if any column within columns is not present in the DataFrame. Source code in janitor/functions/flag_nulls.py @pf.register_dataframe_method def flag_nulls( df: pd.DataFrame, column_name: Optional[Hashable] = \"null_flag\", columns: Optional[Union[str, Iterable[str], Hashable]] = None, ) -> pd.DataFrame: \"\"\"Creates a new column to indicate whether you have null values in a given row. If the columns parameter is not set, looks across the entire DataFrame, otherwise will look only in the columns you set. This method does not mutate the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [\"w\", \"x\", None, \"z\"], \"b\": [5, None, 7, 8], ... }) >>> df.flag_nulls() a b null_flag 0 w 5.0 0 1 x NaN 1 2 None 7.0 1 3 z 8.0 0 >>> df.flag_nulls(columns=\"b\") a b null_flag 0 w 5.0 0 1 x NaN 1 2 None 7.0 0 3 z 8.0 0 :param df: Input pandas DataFrame. :param column_name: Name for the output column. :param columns: List of columns to look at for finding null values. If you only want to look at one column, you can simply give its name. If set to None (default), all DataFrame columns are used. :returns: Input dataframe with the null flag column. :raises ValueError: if `column_name` is already present in the DataFrame. :raises ValueError: if any column within `columns` is not present in the DataFrame. <!-- # noqa: DAR402 --> \"\"\" # Sort out columns input if isinstance(columns, str): columns = [columns] elif columns is None: columns = df.columns elif not isinstance(columns, Iterable): # catches other hashable types columns = [columns] # Input sanitation checks check_column(df, columns) check_column(df, [column_name], present=False) # This algorithm works best for n_rows >> n_cols. See issue #501 null_array = np.zeros(len(df)) for col in columns: null_array = np.logical_or(null_array, pd.isna(df[col])) df = df.copy() df[column_name] = null_array.astype(int) return df get_dupes Implementation of the get_dupes function get_dupes(df, column_names=None) Return all duplicate rows. This method does not mutate the original DataFrame. Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"], ... \"quantity\": [100, 100, 75, 200, 75], ... }) >>> df item quantity 0 shoe 100 1 shoe 100 2 bag 75 3 shoe 200 4 bag 75 >>> df.get_dupes() item quantity 0 shoe 100 1 shoe 100 2 bag 75 4 bag 75 Optional column_names usage: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"], ... \"quantity\": [100, 100, 75, 200, 75], ... }) >>> df item quantity 0 shoe 100 1 shoe 100 2 bag 75 3 shoe 200 4 bag 75 >>> df.get_dupes(column_names=[\"item\"]) item quantity 0 shoe 100 1 shoe 100 2 bag 75 3 shoe 200 4 bag 75 >>> df.get_dupes(column_names=[\"quantity\"]) item quantity 0 shoe 100 1 shoe 100 2 bag 75 4 bag 75 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] (optional) A column name or an iterable (list or tuple) of column names. Following pandas API, this only considers certain columns for identifying duplicates. Defaults to using all columns. None Returns: Type Description DataFrame The duplicate rows, as a pandas DataFrame. Source code in janitor/functions/get_dupes.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def get_dupes( df: pd.DataFrame, column_names: Optional[Union[str, Iterable[str], Hashable]] = None, ) -> pd.DataFrame: \"\"\" Return all duplicate rows. This method does not mutate the original DataFrame. Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"], ... \"quantity\": [100, 100, 75, 200, 75], ... }) >>> df item quantity 0 shoe 100 1 shoe 100 2 bag 75 3 shoe 200 4 bag 75 >>> df.get_dupes() item quantity 0 shoe 100 1 shoe 100 2 bag 75 4 bag 75 Optional `column_names` usage: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"], ... \"quantity\": [100, 100, 75, 200, 75], ... }) >>> df item quantity 0 shoe 100 1 shoe 100 2 bag 75 3 shoe 200 4 bag 75 >>> df.get_dupes(column_names=[\"item\"]) item quantity 0 shoe 100 1 shoe 100 2 bag 75 3 shoe 200 4 bag 75 >>> df.get_dupes(column_names=[\"quantity\"]) item quantity 0 shoe 100 1 shoe 100 2 bag 75 4 bag 75 :param df: The pandas DataFrame object. :param column_names: (optional) A column name or an iterable (list or tuple) of column names. Following pandas API, this only considers certain columns for identifying duplicates. Defaults to using all columns. :returns: The duplicate rows, as a pandas DataFrame. \"\"\" dupes = df.duplicated(subset=column_names, keep=False) return df[dupes == True] # noqa: E712 groupby_agg groupby_agg(df, by, new_column_name, agg_column_name, agg, dropna=True) Shortcut for assigning a groupby-transform to a new column. This method does not mutate the original DataFrame. Intended to be the method-chaining equivalent of: df = df.assign(...=df.groupby(...)[...].transform(...)) Example: Basic usage. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"], ... \"quantity\": [100, 120, 75, 200, 25], ... }) >>> df.groupby_agg( ... by=\"item\", ... agg=\"mean\", ... agg_column_name=\"quantity\", ... new_column_name=\"avg_quantity\", ... ) item quantity avg_quantity 0 shoe 100 140.0 1 shoe 120 140.0 2 bag 75 50.0 3 shoe 200 140.0 4 bag 25 50.0 Example: Set dropna=False to compute the aggregation, treating the null values in the by column as an isolated \"group\". >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"x\": [\"a\", \"a\", None, \"b\"], \"y\": [9, 9, 9, 9], ... }) >>> df.groupby_agg( ... by=\"x\", ... agg=\"count\", ... agg_column_name=\"y\", ... new_column_name=\"y_count\", ... dropna=False, ... ) x y y_count 0 a 9 2 1 a 9 2 2 None 9 1 3 b 9 1 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required by Union[List, Callable, str] Column(s) to groupby on, will be passed into DataFrame.groupby . required new_column_name str Name of the aggregation output column. required agg_column_name str Name of the column to aggregate over. required agg Union[Callable, str] How to aggregate. required dropna bool Whether or not to include null values, if present in the by column(s). Default is True (null values in by are assigned NaN in the new column). True Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/groupby_agg.py @pf.register_dataframe_method @deprecated_alias(new_column=\"new_column_name\", agg_column=\"agg_column_name\") def groupby_agg( df: pd.DataFrame, by: Union[List, Callable, str], new_column_name: str, agg_column_name: str, agg: Union[Callable, str], dropna: bool = True, ) -> pd.DataFrame: \"\"\"Shortcut for assigning a groupby-transform to a new column. This method does not mutate the original DataFrame. Intended to be the method-chaining equivalent of: ```python df = df.assign(...=df.groupby(...)[...].transform(...)) ``` Example: Basic usage. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"], ... \"quantity\": [100, 120, 75, 200, 25], ... }) >>> df.groupby_agg( ... by=\"item\", ... agg=\"mean\", ... agg_column_name=\"quantity\", ... new_column_name=\"avg_quantity\", ... ) item quantity avg_quantity 0 shoe 100 140.0 1 shoe 120 140.0 2 bag 75 50.0 3 shoe 200 140.0 4 bag 25 50.0 Example: Set `dropna=False` to compute the aggregation, treating the null values in the `by` column as an isolated \"group\". >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"x\": [\"a\", \"a\", None, \"b\"], \"y\": [9, 9, 9, 9], ... }) >>> df.groupby_agg( ... by=\"x\", ... agg=\"count\", ... agg_column_name=\"y\", ... new_column_name=\"y_count\", ... dropna=False, ... ) x y y_count 0 a 9 2 1 a 9 2 2 None 9 1 3 b 9 1 :param df: A pandas DataFrame. :param by: Column(s) to groupby on, will be passed into `DataFrame.groupby`. :param new_column_name: Name of the aggregation output column. :param agg_column_name: Name of the column to aggregate over. :param agg: How to aggregate. :param dropna: Whether or not to include null values, if present in the `by` column(s). Default is True (null values in `by` are assigned NaN in the new column). :returns: A pandas DataFrame. \"\"\" # noqa: E501 return df.assign( **{ new_column_name: df.groupby(by, dropna=dropna)[ agg_column_name ].transform(agg), } ) groupby_topk Implementation of the groupby_topk function groupby_topk(df, by, column, k, dropna=True, ascending=True, ignore_index=True) Return top k rows from a groupby of a set of columns. Returns a DataFrame that has the top k values per column , grouped by by . Under the hood it uses nlargest/nsmallest , for numeric columns, which avoids sorting the entire dataframe, and is usually more performant. For non-numeric columns, pd.sort_values is used. No sorting is done to the by column(s); the order is maintained in the final output. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"age\": [20, 23, 22, 43, 21], ... \"id\": [1, 4, 6, 2, 5], ... \"result\": [\"pass\", \"pass\", \"fail\", \"pass\", \"fail\"], ... } ... ) >>> df age id result 0 20 1 pass 1 23 4 pass 2 22 6 fail 3 43 2 pass 4 21 5 fail Ascending top 3: >>> df.groupby_topk(by=\"result\", column=\"age\", k=3) age id result 0 20 1 pass 1 23 4 pass 2 43 2 pass 3 21 5 fail 4 22 6 fail Descending top 2: >>> df.groupby_topk( ... by=\"result\", column=\"age\", k=2, ascending=False, ignore_index=False ... ) age id result 3 43 2 pass 1 23 4 pass 2 22 6 fail 4 21 5 fail Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required by Union[list, Hashable] Column name(s) to group input DataFrame df by. required column Hashable Name of the column that determines k rows to return. required k int Number of top rows to return for each group. required dropna bool If True , and NA values exist in by , the NA values are not used in the groupby computation to get the relevant k rows. If False , and NA values exist in by , then the NA values are used in the groupby computation to get the relevant k rows. The default is True . True ascending bool Default is True . If True , the smallest top k rows, determined by column are returned; if False, the largest top k rows, determined by column` are returned. True ignore_index bool Default True . If True , the original index is ignored. If False , the original index for the top k rows is retained. True Returns: Type Description DataFrame A pandas DataFrame with top k rows per column , grouped by by . Exceptions: Type Description ValueError if k is less than 1. Source code in janitor/functions/groupby_topk.py @pf.register_dataframe_method @deprecated_alias(groupby_column_name=\"by\", sort_column_name=\"column\") def groupby_topk( df: pd.DataFrame, by: Union[list, Hashable], column: Hashable, k: int, dropna: bool = True, ascending: bool = True, ignore_index: bool = True, ) -> pd.DataFrame: \"\"\" Return top `k` rows from a groupby of a set of columns. Returns a DataFrame that has the top `k` values per `column`, grouped by `by`. Under the hood it uses `nlargest/nsmallest`, for numeric columns, which avoids sorting the entire dataframe, and is usually more performant. For non-numeric columns, `pd.sort_values` is used. No sorting is done to the `by` column(s); the order is maintained in the final output. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"age\": [20, 23, 22, 43, 21], ... \"id\": [1, 4, 6, 2, 5], ... \"result\": [\"pass\", \"pass\", \"fail\", \"pass\", \"fail\"], ... } ... ) >>> df age id result 0 20 1 pass 1 23 4 pass 2 22 6 fail 3 43 2 pass 4 21 5 fail Ascending top 3: >>> df.groupby_topk(by=\"result\", column=\"age\", k=3) age id result 0 20 1 pass 1 23 4 pass 2 43 2 pass 3 21 5 fail 4 22 6 fail Descending top 2: >>> df.groupby_topk( ... by=\"result\", column=\"age\", k=2, ascending=False, ignore_index=False ... ) age id result 3 43 2 pass 1 23 4 pass 2 22 6 fail 4 21 5 fail :param df: A pandas DataFrame. :param by: Column name(s) to group input DataFrame `df` by. :param column: Name of the column that determines `k` rows to return. :param k: Number of top rows to return for each group. :param dropna: If `True`, and `NA` values exist in `by`, the `NA` values are not used in the groupby computation to get the relevant `k` rows. If `False`, and `NA` values exist in `by`, then the `NA` values are used in the groupby computation to get the relevant `k` rows. The default is `True`. :param ascending: Default is `True`. If `True`, the smallest top `k` rows, determined by `column` are returned; if `False, the largest top `k` rows, determined by `column` are returned. :param ignore_index: Default `True`. If `True`, the original index is ignored. If `False`, the original index for the top `k` rows is retained. :returns: A pandas DataFrame with top `k` rows per `column`, grouped by `by`. :raises ValueError: if `k` is less than 1. \"\"\" # noqa: E501 if isinstance(by, Hashable): by = [by] check(\"by\", by, [Hashable, list]) check_column(df, [column]) check_column(df, by) if k < 1: raise ValueError( \"Numbers of rows per group \" \"to be returned must be greater than 0.\" ) indices = df.groupby(by=by, dropna=dropna, sort=False, observed=True) indices = indices[column] try: if ascending: indices = indices.nsmallest(n=k) else: indices = indices.nlargest(n=k) except TypeError: indices = indices.apply( lambda d: d.sort_values(ascending=ascending).head(k) ) indices = indices.index.get_level_values(-1) if ignore_index: return df.loc[indices].reset_index(drop=True) return df.loc[indices] impute Implementation of impute function impute(df, column_name, value=None, statistic_column_name=None) Method-chainable imputation of values in a column. This method mutates the original DataFrame. Underneath the hood, this function calls the .fillna() method available to every pandas.Series object. Either one of value or statistic_column_name should be provided. If value is provided, then all null values in the selected column will take on the value provided. If statistic_column_name is provided, then all null values in the selected column will take on the summary statistic value of other non-null values. Currently supported statistics include: mean (also aliased by average ) median mode minimum (also aliased by min ) maximum (also aliased by max ) Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [1, 2, 3], ... \"sales\": np.nan, ... \"score\": [np.nan, 3, 2], ... }) >>> df a sales score 0 1 NaN NaN 1 2 NaN 3.0 2 3 NaN 2.0 Imputing null values with 0 (using the value parameter): >>> df.impute(column_name=\"sales\", value=0.0) a sales score 0 1 0.0 NaN 1 2 0.0 3.0 2 3 0.0 2.0 Imputing null values with median (using the statistic_column_name parameter): >>> df.impute(column_name=\"score\", statistic_column_name=\"median\") a sales score 0 1 0.0 2.5 1 2 0.0 3.0 2 3 0.0 2.0 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The name of the column on which to impute values. required value Optional[Any] The value used for imputation, passed into .fillna method of the underlying pandas Series. None statistic_column_name Optional[str] The column statistic to impute. None Returns: Type Description DataFrame An imputed pandas DataFrame. Exceptions: Type Description ValueError If both value and statistic_column_name are provided. KeyError If statistic_column_name is not one of mean , average , median , mode , minimum , min , maximum , or max . Source code in janitor/functions/impute.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") @deprecated_alias(statistic=\"statistic_column_name\") def impute( df: pd.DataFrame, column_name: Hashable, value: Optional[Any] = None, statistic_column_name: Optional[str] = None, ) -> pd.DataFrame: \"\"\" Method-chainable imputation of values in a column. This method mutates the original DataFrame. Underneath the hood, this function calls the `.fillna()` method available to every `pandas.Series` object. Either one of `value` or `statistic_column_name` should be provided. If `value` is provided, then all null values in the selected column will take on the value provided. If `statistic_column_name` is provided, then all null values in the selected column will take on the summary statistic value of other non-null values. Currently supported statistics include: - `mean` (also aliased by `average`) - `median` - `mode` - `minimum` (also aliased by `min`) - `maximum` (also aliased by `max`) Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [1, 2, 3], ... \"sales\": np.nan, ... \"score\": [np.nan, 3, 2], ... }) >>> df a sales score 0 1 NaN NaN 1 2 NaN 3.0 2 3 NaN 2.0 Imputing null values with 0 (using the `value` parameter): >>> df.impute(column_name=\"sales\", value=0.0) a sales score 0 1 0.0 NaN 1 2 0.0 3.0 2 3 0.0 2.0 Imputing null values with median (using the `statistic_column_name` parameter): >>> df.impute(column_name=\"score\", statistic_column_name=\"median\") a sales score 0 1 0.0 2.5 1 2 0.0 3.0 2 3 0.0 2.0 :param df: A pandas DataFrame. :param column_name: The name of the column on which to impute values. :param value: The value used for imputation, passed into `.fillna` method of the underlying pandas Series. :param statistic_column_name: The column statistic to impute. :returns: An imputed pandas DataFrame. :raises ValueError: If both `value` and `statistic_column_name` are provided. :raises KeyError: If `statistic_column_name` is not one of `mean`, `average`, `median`, `mode`, `minimum`, `min`, `maximum`, or `max`. \"\"\" # Firstly, we check that only one of `value` or `statistic` are provided. if value is not None and statistic_column_name is not None: raise ValueError( \"Only one of `value` or `statistic_column_name` should be \" \"provided.\" ) # If statistic is provided, then we compute the relevant summary statistic # from the other data. funcs = { \"mean\": np.mean, \"average\": np.mean, # aliased \"median\": np.median, \"mode\": ss.mode, \"minimum\": np.min, \"min\": np.min, # aliased \"maximum\": np.max, \"max\": np.max, # aliased } if statistic_column_name is not None: # Check that the statistic keyword argument is one of the approved. if statistic_column_name not in funcs: raise KeyError( f\"`statistic_column_name` must be one of {funcs.keys()}.\" ) value = funcs[statistic_column_name]( df[column_name].dropna().to_numpy() ) # special treatment for mode, because scipy stats mode returns a # moderesult object. if statistic_column_name == \"mode\": value = value.mode[0] # The code is architected this way - if `value` is not provided but # statistic is, we then overwrite the None value taken on by `value`, and # use it to set the imputation column. if value is not None: df[column_name] = df[column_name].fillna(value) return df jitter Implementation of the jitter function. jitter(df, column_name, dest_column_name, scale, clip=None, random_state=None) Adds Gaussian noise (jitter) to the values of a column. Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [3, 4, 5, np.nan]}) >>> df a 0 3.0 1 4.0 2 5.0 3 NaN >>> df.jitter(\"a\", dest_column_name=\"a_jit\", scale=1, random_state=42) a a_jit 0 3.0 3.496714 1 4.0 3.861736 2 5.0 5.647689 3 NaN NaN A new column will be created containing the values of the original column with Gaussian noise added. For each value in the column, a Gaussian distribution is created having a location (mean) equal to the value and a scale (standard deviation) equal to scale . A random value is then sampled from this distribution, which is the jittered value. If a tuple is supplied for clip , then any values of the new column less than clip[0] will be set to clip[0] , and any values greater than clip[1] will be set to clip[1] . Additionally, if a numeric value is supplied for random_state , this value will be used to set the random seed used for sampling. NaN values are ignored in this method. This method mutates the original DataFrame. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Name of the column containing values to add Gaussian jitter to. required dest_column_name str The name of the new column containing the jittered values that will be created. required scale number A positive value multiplied by the original column value to determine the scale (standard deviation) of the Gaussian distribution to sample from. (A value of zero results in no jittering.) required clip Optional[Iterable[numpy.number]] An iterable of two values (minimum and maximum) to clip the jittered values to, default to None. None random_state Optional[numpy.number] An integer or 1-d array value used to set the random seed, default to None. None Returns: Type Description DataFrame A pandas DataFrame with a new column containing Gaussian-jittered values from another column. Exceptions: Type Description TypeError If column_name is not numeric. ValueError If scale is not a numerical value greater than 0 . ValueError If clip is not an iterable of length 2 . ValueError If clip[0] is greater than clip[1] . Source code in janitor/functions/jitter.py @pf.register_dataframe_method def jitter( df: pd.DataFrame, column_name: Hashable, dest_column_name: str, scale: np.number, clip: Optional[Iterable[np.number]] = None, random_state: Optional[np.number] = None, ) -> pd.DataFrame: \"\"\" Adds Gaussian noise (jitter) to the values of a column. Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [3, 4, 5, np.nan]}) >>> df a 0 3.0 1 4.0 2 5.0 3 NaN >>> df.jitter(\"a\", dest_column_name=\"a_jit\", scale=1, random_state=42) a a_jit 0 3.0 3.496714 1 4.0 3.861736 2 5.0 5.647689 3 NaN NaN A new column will be created containing the values of the original column with Gaussian noise added. For each value in the column, a Gaussian distribution is created having a location (mean) equal to the value and a scale (standard deviation) equal to `scale`. A random value is then sampled from this distribution, which is the jittered value. If a tuple is supplied for `clip`, then any values of the new column less than `clip[0]` will be set to `clip[0]`, and any values greater than `clip[1]` will be set to `clip[1]`. Additionally, if a numeric value is supplied for `random_state`, this value will be used to set the random seed used for sampling. NaN values are ignored in this method. This method mutates the original DataFrame. :param df: A pandas DataFrame. :param column_name: Name of the column containing values to add Gaussian jitter to. :param dest_column_name: The name of the new column containing the jittered values that will be created. :param scale: A positive value multiplied by the original column value to determine the scale (standard deviation) of the Gaussian distribution to sample from. (A value of zero results in no jittering.) :param clip: An iterable of two values (minimum and maximum) to clip the jittered values to, default to None. :param random_state: An integer or 1-d array value used to set the random seed, default to None. :returns: A pandas DataFrame with a new column containing Gaussian-jittered values from another column. :raises TypeError: If `column_name` is not numeric. :raises ValueError: If `scale` is not a numerical value greater than `0`. :raises ValueError: If `clip` is not an iterable of length `2`. :raises ValueError: If `clip[0]` is greater than `clip[1]`. \"\"\" # Check types check(\"scale\", scale, [int, float]) # Check that `column_name` is a numeric column if not np.issubdtype(df[column_name].dtype, np.number): raise TypeError(f\"{column_name} must be a numeric column.\") if scale <= 0: raise ValueError(\"`scale` must be a numeric value greater than 0.\") values = df[column_name] if random_state is not None: np.random.seed(random_state) result = np.random.normal(loc=values, scale=scale) if clip: # Ensure `clip` has length 2 if len(clip) != 2: raise ValueError(\"`clip` must be an iterable of length 2.\") # Ensure the values in `clip` are ordered as min, max if clip[1] < clip[0]: raise ValueError( \"`clip[0]` must be less than or equal to `clip[1]`.\" ) result = np.clip(result, *clip) df[dest_column_name] = result return df join_apply Implementation of the join_apply function join_apply(df, func, new_column_name) Join the result of applying a function across dataframe rows. This method does not mutate the original DataFrame. This is a convenience function that allows us to apply arbitrary functions that take any combination of information from any of the columns. The only requirement is that the function signature takes in a row from the DataFrame. Example: Sum the result of two columns into a new column. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\":[1, 2, 3], \"b\": [2, 3, 4]}) >>> df a b 0 1 2 1 2 3 2 3 4 >>> df.join_apply( ... func=lambda x: 2 * x[\"a\"] + x[\"b\"], ... new_column_name=\"2a+b\", ... ) a b 2a+b 0 1 2 4 1 2 3 7 2 3 4 10 Example: Incorporating conditionals in func . >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [20, 30, 40]}) >>> df a b 0 1 20 1 2 30 2 3 40 >>> def take_a_if_even(x): ... if x[\"a\"] % 2 == 0: ... return x[\"a\"] ... else: ... return x[\"b\"] >>> df.join_apply(take_a_if_even, \"a_if_even\") a b a_if_even 0 1 20 20 1 2 30 2 2 3 40 40 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required func Callable A function that is applied elementwise across all rows of the DataFrame. required new_column_name str Name of the resulting column. required Returns: Type Description DataFrame A pandas DataFrame with new column appended. Source code in janitor/functions/join_apply.py @pf.register_dataframe_method def join_apply( df: pd.DataFrame, func: Callable, new_column_name: str, ) -> pd.DataFrame: \"\"\" Join the result of applying a function across dataframe rows. This method does not mutate the original DataFrame. This is a convenience function that allows us to apply arbitrary functions that take any combination of information from any of the columns. The only requirement is that the function signature takes in a row from the DataFrame. Example: Sum the result of two columns into a new column. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\":[1, 2, 3], \"b\": [2, 3, 4]}) >>> df a b 0 1 2 1 2 3 2 3 4 >>> df.join_apply( ... func=lambda x: 2 * x[\"a\"] + x[\"b\"], ... new_column_name=\"2a+b\", ... ) a b 2a+b 0 1 2 4 1 2 3 7 2 3 4 10 Example: Incorporating conditionals in `func`. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [20, 30, 40]}) >>> df a b 0 1 20 1 2 30 2 3 40 >>> def take_a_if_even(x): ... if x[\"a\"] % 2 == 0: ... return x[\"a\"] ... else: ... return x[\"b\"] >>> df.join_apply(take_a_if_even, \"a_if_even\") a b a_if_even 0 1 20 20 1 2 30 2 2 3 40 40 :param df: A pandas DataFrame. :param func: A function that is applied elementwise across all rows of the DataFrame. :param new_column_name: Name of the resulting column. :returns: A pandas DataFrame with new column appended. \"\"\" df = df.copy().join(df.apply(func, axis=1).rename(new_column_name)) return df label_encode Implementation of label_encode function label_encode(df, column_names) Convert labels into numerical data. This method will create a new column with the string _enc appended after the original column's name. Consider this to be syntactic sugar. This function uses the factorize pandas function under the hood. This method behaves differently from encode_categorical . This method creates a new column of numeric data. encode_categorical replaces the dtype of the original column with a categorical dtype. This method mutates the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df foo bar 0 b 4 1 b 5 2 a 6 3 c 7 4 b 8 >>> df.label_encode(column_names=\"foo\") foo bar foo_enc 0 b 4 0 1 b 5 0 2 a 6 1 3 c 7 2 4 b 8 0 Note This function will be deprecated in a 1.x release. Please use factorize_columns instead. Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] A column name or an iterable (list or tuple) of column names. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/label_encode.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def label_encode( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable], ) -> pd.DataFrame: \"\"\" Convert labels into numerical data. This method will create a new column with the string `_enc` appended after the original column's name. Consider this to be syntactic sugar. This function uses the `factorize` pandas function under the hood. This method behaves differently from [`encode_categorical`][janitor.functions.encode_categorical.encode_categorical]. This method creates a new column of numeric data. [`encode_categorical`][janitor.functions.encode_categorical.encode_categorical] replaces the dtype of the original column with a *categorical* dtype. This method mutates the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df foo bar 0 b 4 1 b 5 2 a 6 3 c 7 4 b 8 >>> df.label_encode(column_names=\"foo\") foo bar foo_enc 0 b 4 0 1 b 5 0 2 a 6 1 3 c 7 2 4 b 8 0 !!!note This function will be deprecated in a 1.x release. Please use [`factorize_columns`][janitor.functions.factorize_columns.factorize_columns] instead. :param df: The pandas DataFrame object. :param column_names: A column name or an iterable (list or tuple) of column names. :returns: A pandas DataFrame. \"\"\" # noqa: E501 warnings.warn( \"`label_encode` will be deprecated in a 1.x release. \" \"Please use `factorize_columns` instead.\" ) df = _factorize(df, column_names, \"_enc\") return df limit_column_characters Implementation of limit_column_characters. limit_column_characters(df, column_length, col_separator='_') Truncate column sizes to a specific length. This method mutates the original DataFrame. Method chaining will truncate all columns to a given length and append a given separator character with the index of duplicate columns, except for the first distinct column name. Example: >>> import pandas as pd >>> import janitor >>> data_dict = { ... \"really_long_name\": [9, 8, 7], ... \"another_really_long_name\": [2, 4, 6], ... \"another_really_longer_name\": list(\"xyz\"), ... \"this_is_getting_out_of_hand\": list(\"pqr\"), ... } >>> df = pd.DataFrame(data_dict) >>> df # doctest: +SKIP really_long_name another_really_long_name another_really_longer_name this_is_getting_out_of_hand 0 9 2 x p 1 8 4 y q 2 7 6 z r >>> df.limit_column_characters(7) really_ another another_1 this_is 0 9 2 x p 1 8 4 y q 2 7 6 z r Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_length int Character length for which to truncate all columns. The column separator value and number for duplicate column name does not contribute. Therefore, if all columns are truncated to 10 characters, the first distinct column will be 10 characters and the remaining will be 12 characters (assuming a column separator of one character). required col_separator str The separator to use for counting distinct column values, for example, '_' or '.' . Supply an empty string (i.e. '' ) to remove the separator. '_' Returns: Type Description DataFrame A pandas DataFrame with truncated column lengths. Source code in janitor/functions/limit_column_characters.py @pf.register_dataframe_method def limit_column_characters( df: pd.DataFrame, column_length: int, col_separator: str = \"_\", ) -> pd.DataFrame: \"\"\"Truncate column sizes to a specific length. This method mutates the original DataFrame. Method chaining will truncate all columns to a given length and append a given separator character with the index of duplicate columns, except for the first distinct column name. Example: >>> import pandas as pd >>> import janitor >>> data_dict = { ... \"really_long_name\": [9, 8, 7], ... \"another_really_long_name\": [2, 4, 6], ... \"another_really_longer_name\": list(\"xyz\"), ... \"this_is_getting_out_of_hand\": list(\"pqr\"), ... } >>> df = pd.DataFrame(data_dict) >>> df # doctest: +SKIP really_long_name another_really_long_name another_really_longer_name this_is_getting_out_of_hand 0 9 2 x p 1 8 4 y q 2 7 6 z r >>> df.limit_column_characters(7) really_ another another_1 this_is 0 9 2 x p 1 8 4 y q 2 7 6 z r :param df: A pandas DataFrame. :param column_length: Character length for which to truncate all columns. The column separator value and number for duplicate column name does not contribute. Therefore, if all columns are truncated to 10 characters, the first distinct column will be 10 characters and the remaining will be 12 characters (assuming a column separator of one character). :param col_separator: The separator to use for counting distinct column values, for example, `'_'` or `'.'`. Supply an empty string (i.e. `''`) to remove the separator. :returns: A pandas DataFrame with truncated column lengths. \"\"\" # noqa: E501 check(\"column_length\", column_length, [int]) check(\"col_separator\", col_separator, [str]) col_names = df.columns col_names = [col_name[:column_length] for col_name in col_names] col_name_set = set(col_names) col_name_count = {} # If no columns are duplicates, we can skip the loops below. if len(col_name_set) == len(col_names): df.columns = col_names return df for col_name_to_check in col_name_set: count = 0 for idx, col_name in enumerate(col_names): if col_name_to_check == col_name: col_name_count[idx] = count count += 1 final_col_names = [] for idx, col_name in enumerate(col_names): if col_name_count[idx] > 0: col_name_to_append = ( col_name + col_separator + str(col_name_count[idx]) ) final_col_names.append(col_name_to_append) else: final_col_names.append(col_name) df.columns = final_col_names return df min_max_scale min_max_scale(df, feature_range=(0, 1), column_name=None, jointly=False) Scales DataFrame to between a minimum and maximum value. One can optionally set a new target minimum and maximum value using the feature_range keyword argument. If column_name is specified, then only that column(s) of data is scaled. Otherwise, the entire dataframe is scaled. If jointly is True , the column_names provided entire dataframe will be regnozied as the one to jointly scale. Otherwise, each column of data will be scaled separately. Example: Basic usage. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'a':[1, 2], 'b':[0, 1]}) >>> df.min_max_scale() a b 0 0.0 0.0 1 1.0 1.0 >>> df.min_max_scale(jointly=True) a b 0 0.5 0.0 1 1.0 0.5 Example: Setting custom minimum and maximum. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'a':[1, 2], 'b':[0, 1]}) >>> df.min_max_scale(feature_range=(0, 100)) a b 0 0.0 0.0 1 100.0 100.0 >>> df.min_max_scale(feature_range=(0, 100), jointly=True) a b 0 50.0 0.0 1 100.0 50.0 Example: Apply min-max to the selected columns. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'a':[1, 2], 'b':[0, 1], 'c': [1, 0]}) >>> df.min_max_scale( ... feature_range=(0, 100), ... column_name=[\"a\", \"c\"], ... ) a b c 0 0.0 0 100.0 1 100.0 1 0.0 >>> df.min_max_scale( ... feature_range=(0, 100), ... column_name=[\"a\", \"c\"], ... jointly=True, ... ) a b c 0 50.0 0 50.0 1 100.0 1 0.0 >>> df.min_max_scale(feature_range=(0, 100), column_name='a') a b c 0 0.0 0 1 1 100.0 1 0 The aforementioned example might be applied to something like scaling the isoelectric points of amino acids. While technically they range from approx 3-10, we can also think of them on the pH scale which ranges from 1 to 14. Hence, 3 gets scaled not to 0 but approx. 0.15 instead, while 10 gets scaled to approx. 0.69 instead. Version Changed 0.24.0 Deleted old_min , old_max , new_min , and new_max options. Added feature_range , and jointly options. Parameters: Name Type Description Default df pd.DataFrame A pandas DataFrame. required feature_range tuple[int | float, int | float] (optional) Desired range of transformed data. (0, 1) column_name str | int | list[str | int] | pd.Index (optional) The column on which to perform scaling. None jointly bool (bool) Scale the entire data if Ture. False Returns: Type Description pd.DataFrame A pandas DataFrame with scaled data. Exceptions: Type Description ValueError if feature_range isn't tuple type. ValueError if the length of feature_range isn't equal to two. ValueError if the element of feature_range isn't number type. ValueError if feature_range[1] <= feature_range[0] . Source code in janitor/functions/min_max_scale.py @pf.register_dataframe_method @deprecated_kwargs( \"old_min\", \"old_max\", \"new_min\", \"new_max\", message=( \"The keyword argument {argument!r} of {func_name!r} is deprecated. \" \"Please use 'feature_range' instead.\" ), ) @deprecated_alias(col_name=\"column_name\") def min_max_scale( df: pd.DataFrame, feature_range: tuple[int | float, int | float] = (0, 1), column_name: str | int | list[str | int] | pd.Index = None, jointly: bool = False, ) -> pd.DataFrame: \"\"\" Scales DataFrame to between a minimum and maximum value. One can optionally set a new target **minimum** and **maximum** value using the `feature_range` keyword argument. If `column_name` is specified, then only that column(s) of data is scaled. Otherwise, the entire dataframe is scaled. If `jointly` is `True`, the `column_names` provided entire dataframe will be regnozied as the one to jointly scale. Otherwise, each column of data will be scaled separately. Example: Basic usage. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'a':[1, 2], 'b':[0, 1]}) >>> df.min_max_scale() a b 0 0.0 0.0 1 1.0 1.0 >>> df.min_max_scale(jointly=True) a b 0 0.5 0.0 1 1.0 0.5 Example: Setting custom minimum and maximum. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'a':[1, 2], 'b':[0, 1]}) >>> df.min_max_scale(feature_range=(0, 100)) a b 0 0.0 0.0 1 100.0 100.0 >>> df.min_max_scale(feature_range=(0, 100), jointly=True) a b 0 50.0 0.0 1 100.0 50.0 Example: Apply min-max to the selected columns. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'a':[1, 2], 'b':[0, 1], 'c': [1, 0]}) >>> df.min_max_scale( ... feature_range=(0, 100), ... column_name=[\"a\", \"c\"], ... ) a b c 0 0.0 0 100.0 1 100.0 1 0.0 >>> df.min_max_scale( ... feature_range=(0, 100), ... column_name=[\"a\", \"c\"], ... jointly=True, ... ) a b c 0 50.0 0 50.0 1 100.0 1 0.0 >>> df.min_max_scale(feature_range=(0, 100), column_name='a') a b c 0 0.0 0 1 1 100.0 1 0 The aforementioned example might be applied to something like scaling the isoelectric points of amino acids. While technically they range from approx 3-10, we can also think of them on the pH scale which ranges from 1 to 14. Hence, 3 gets scaled not to 0 but approx. 0.15 instead, while 10 gets scaled to approx. 0.69 instead. !!! summary \"Version Changed\" - 0.24.0 - Deleted `old_min`, `old_max`, `new_min`, and `new_max` options. - Added `feature_range`, and `jointly` options. :param df: A pandas DataFrame. :param feature_range: (optional) Desired range of transformed data. :param column_name: (optional) The column on which to perform scaling. :param jointly: (bool) Scale the entire data if Ture. :returns: A pandas DataFrame with scaled data. :raises ValueError: if `feature_range` isn't tuple type. :raises ValueError: if the length of `feature_range` isn't equal to two. :raises ValueError: if the element of `feature_range` isn't number type. :raises ValueError: if `feature_range[1]` <= `feature_range[0]`. \"\"\" if not ( isinstance(feature_range, (tuple, list)) and len(feature_range) == 2 and all((isinstance(i, (int, float))) for i in feature_range) and feature_range[1] > feature_range[0] ): raise ValueError( \"`feature_range` should be a range type contains number element, \" \"the first element must be greater than the second one\" ) if column_name is not None: df = df.copy() # Avoid to change the original DataFrame. old_feature_range = df[column_name].pipe(_min_max_value, jointly) df[column_name] = df[column_name].pipe( _apply_min_max, *old_feature_range, *feature_range, ) else: old_feature_range = df.pipe(_min_max_value, jointly) df = df.pipe( _apply_min_max, *old_feature_range, *feature_range, ) return df move Implementation of move. move(df, source, target, position='before', axis=0) Moves a column or row to a position adjacent to another column or row in the dataframe. This operation does not reset the index of the dataframe. User must explicitly do so. This function does not apply to multilevel dataframes, and the dataframe must have unique column names or indices. Example: Moving a row >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [2, 4, 6, 8], \"b\": list(\"wxyz\")}) >>> df a b 0 2 w 1 4 x 2 6 y 3 8 z >>> df.move(source=0, target=3, position=\"before\", axis=0) a b 1 4 x 2 6 y 0 2 w 3 8 z Example: Moving a column >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [2, 4, 6], \"b\": [1, 3, 5], \"c\": [7, 8, 9]}) >>> df a b c 0 2 1 7 1 4 3 8 2 6 5 9 >>> df.move(source=\"a\", target=\"c\", position=\"after\", axis=1) b c a 0 1 7 2 1 3 8 4 2 5 9 6 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required source Union[int, str] Column or row to move. required target Union[int, str] Column or row to move adjacent to. required position str Specifies whether the Series is moved to before or after the adjacent Series. Values can be either before or after ; defaults to before . 'before' axis int Axis along which the function is applied. 0 to move a row, 1 to move a column. 0 Returns: Type Description DataFrame The dataframe with the Series moved. Exceptions: Type Description ValueError If axis is not 0 or 1 . ValueError If position is not before or after . ValueError If source row or column is not in dataframe. ValueError If target row or column is not in dataframe. Source code in janitor/functions/move.py @pf.register_dataframe_method def move( df: pd.DataFrame, source: Union[int, str], target: Union[int, str], position: str = \"before\", axis: int = 0, ) -> pd.DataFrame: \"\"\" Moves a column or row to a position adjacent to another column or row in the dataframe. This operation does not reset the index of the dataframe. User must explicitly do so. This function does not apply to multilevel dataframes, and the dataframe must have unique column names or indices. Example: Moving a row >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [2, 4, 6, 8], \"b\": list(\"wxyz\")}) >>> df a b 0 2 w 1 4 x 2 6 y 3 8 z >>> df.move(source=0, target=3, position=\"before\", axis=0) a b 1 4 x 2 6 y 0 2 w 3 8 z Example: Moving a column >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [2, 4, 6], \"b\": [1, 3, 5], \"c\": [7, 8, 9]}) >>> df a b c 0 2 1 7 1 4 3 8 2 6 5 9 >>> df.move(source=\"a\", target=\"c\", position=\"after\", axis=1) b c a 0 1 7 2 1 3 8 4 2 5 9 6 :param df: The pandas DataFrame object. :param source: Column or row to move. :param target: Column or row to move adjacent to. :param position: Specifies whether the Series is moved to before or after the adjacent Series. Values can be either `before` or `after`; defaults to `before`. :param axis: Axis along which the function is applied. 0 to move a row, 1 to move a column. :returns: The dataframe with the Series moved. :raises ValueError: If `axis` is not `0` or `1`. :raises ValueError: If `position` is not `before` or `after`. :raises ValueError: If `source` row or column is not in dataframe. :raises ValueError: If `target` row or column is not in dataframe. \"\"\" if axis not in [0, 1]: raise ValueError(f\"Invalid axis '{axis}'. Can only be 0 or 1.\") if position not in [\"before\", \"after\"]: raise ValueError( f\"Invalid position '{position}'. Can only be 'before' or 'after'.\" ) df = df.copy() if axis == 0: names = list(df.index) if source not in names: raise ValueError(f\"Source row '{source}' not in dataframe.\") if target not in names: raise ValueError(f\"Target row '{target}' not in dataframe.\") names.remove(source) pos = names.index(target) if position == \"after\": pos += 1 names.insert(pos, source) df = df.loc[names, :] else: names = list(df.columns) if source not in names: raise ValueError(f\"Source column '{source}' not in dataframe.\") if target not in names: raise ValueError(f\"Target column '{target}' not in dataframe.\") names.remove(source) pos = names.index(target) if position == \"after\": pos += 1 names.insert(pos, source) df = df.loc[:, names] return df pivot pivot_longer(df, index=None, column_names=None, names_to=None, values_to='value', column_level=None, names_sep=None, names_pattern=None, names_transform=None, dropna=False, sort_by_appearance=False, ignore_index=True) Unpivots a DataFrame from wide to long format. This method does not mutate the original DataFrame. It is modeled after the pivot_longer function in R's tidyr package, and also takes inspiration from R's data.table package. This function is useful to massage a DataFrame into a format where one or more columns are considered measured variables, and all other columns are considered as identifier variables. All measured variables are unpivoted (and typically duplicated) along the row axis. Column selection in index and column_names is possible using the select_columns syntax. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"Sepal.Length\": [5.1, 5.9], ... \"Sepal.Width\": [3.5, 3.0], ... \"Petal.Length\": [1.4, 5.1], ... \"Petal.Width\": [0.2, 1.8], ... \"Species\": [\"setosa\", \"virginica\"], ... } ... ) >>> df Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 5.1 3.5 1.4 0.2 setosa 1 5.9 3.0 5.1 1.8 virginica Replicate pandas' melt: >>> df.pivot_longer(index = 'Species') Species variable value 0 setosa Sepal.Length 5.1 1 virginica Sepal.Length 5.9 2 setosa Sepal.Width 3.5 3 virginica Sepal.Width 3.0 4 setosa Petal.Length 1.4 5 virginica Petal.Length 5.1 6 setosa Petal.Width 0.2 7 virginica Petal.Width 1.8 Split the column labels into parts: >>> df.pivot_longer( ... index = 'Species', ... names_to = ('part', 'dimension'), ... names_sep = '.', ... sort_by_appearance = True, ... ) Species part dimension value 0 setosa Sepal Length 5.1 1 setosa Sepal Width 3.5 2 setosa Petal Length 1.4 3 setosa Petal Width 0.2 4 virginica Sepal Length 5.9 5 virginica Sepal Width 3.0 6 virginica Petal Length 5.1 7 virginica Petal Width 1.8 Retain parts of the column names as headers: >>> df.pivot_longer( ... index = 'Species', ... names_to = ('part', '.value'), ... names_sep = '.', ... sort_by_appearance = True, ... ) Species part Length Width 0 setosa Sepal 5.1 3.5 1 setosa Petal 1.4 0.2 2 virginica Sepal 5.9 3.0 3 virginica Petal 5.1 1.8 Split the column labels based on regex: >>> df = pd.DataFrame({\"id\": [1], \"new_sp_m5564\": [2], \"newrel_f65\": [3]}) >>> df id new_sp_m5564 newrel_f65 0 1 2 3 >>> df.pivot_longer( ... index = 'id', ... names_to = ('diagnosis', 'gender', 'age'), ... names_pattern = r\"new_?(.+)_(.)(\\d+)\", ... ) id diagnosis gender age value 0 1 sp m 5564 2 1 1 rel f 65 3 Convert the dtypes of specific columns with names_transform : >>> result = (df ... .pivot_longer( ... index = 'id', ... names_to = ('diagnosis', 'gender', 'age'), ... names_pattern = r\"new_?(.+)_(.)(\\d+)\", ... names_transform = {'gender': 'category', 'age':'int'}) ... ) >>> result.dtypes id int64 gender category age int64 value int64 dtype: object Use multiple .value to reshape dataframe: >>> df = pd.DataFrame( ... [ ... { ... \"x_1_mean\": 10, ... \"x_2_mean\": 20, ... \"y_1_mean\": 30, ... \"y_2_mean\": 40, ... \"unit\": 50, ... } ... ] ... ) >>> df x_1_mean x_2_mean y_1_mean y_2_mean unit 0 10 20 30 40 50 >>> df.pivot_longer( ... index=\"unit\", ... names_to=(\".value\", \"time\", \".value\"), ... names_pattern=r\"(x|y)_([0-9])(_mean)\", ... ) unit time x_mean y_mean 0 50 1 10 30 1 50 2 20 40 Multiple values_to: >>> df = pd.DataFrame( ... { ... \"City\": [\"Houston\", \"Austin\", \"Hoover\"], ... \"State\": [\"Texas\", \"Texas\", \"Alabama\"], ... \"Name\": [\"Aria\", \"Penelope\", \"Niko\"], ... \"Mango\": [4, 10, 90], ... \"Orange\": [10, 8, 14], ... \"Watermelon\": [40, 99, 43], ... \"Gin\": [16, 200, 34], ... \"Vodka\": [20, 33, 18], ... }, ... columns=[ ... \"City\", ... \"State\", ... \"Name\", ... \"Mango\", ... \"Orange\", ... \"Watermelon\", ... \"Gin\", ... \"Vodka\", ... ], ... ) >>> df City State Name Mango Orange Watermelon Gin Vodka 0 Houston Texas Aria 4 10 40 16 20 1 Austin Texas Penelope 10 8 99 200 33 2 Hoover Alabama Niko 90 14 43 34 18 >>> df.pivot_longer( ... index=[\"City\", \"State\"], ... column_names=slice(\"Mango\", \"Vodka\"), ... names_to=(\"Fruit\", \"Drink\"), ... values_to=(\"Pounds\", \"Ounces\"), ... names_pattern=[r\"M|O|W\", r\"G|V\"], ... ) City State Fruit Pounds Drink Ounces 0 Houston Texas Mango 4 Gin 16.0 1 Austin Texas Mango 10 Gin 200.0 2 Hoover Alabama Mango 90 Gin 34.0 3 Houston Texas Orange 10 Vodka 20.0 4 Austin Texas Orange 8 Vodka 33.0 5 Hoover Alabama Orange 14 Vodka 18.0 6 Houston Texas Watermelon 40 None NaN 7 Austin Texas Watermelon 99 None NaN 8 Hoover Alabama Watermelon 43 None NaN Version Changed 0.24.0 Added dropna parameter. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required index Union[list, tuple, str, Pattern] Name(s) of columns to use as identifier variables. Should be either a single column name, or a list/tuple of column names. index should be a list of tuples if the columns are a MultiIndex. None column_names Union[list, tuple, str, Pattern] Name(s) of columns to unpivot. Should be either a single column name or a list/tuple of column names. column_names should be a list of tuples if the columns are a MultiIndex. None names_to Union[list, tuple, str] Name of new column as a string that will contain what were previously the column names in column_names . The default is variable if no value is provided. It can also be a list/tuple of strings that will serve as new column names, if name_sep or names_pattern is provided. If .value is in names_to , new column names will be extracted from part of the existing column names and overrides values_to . None values_to Optional[str] Name of new column as a string that will contain what were previously the values of the columns in column_names . values_to can also be a list/tuple and requires that names_pattern is also a list/tuple. 'value' column_level Union[int, str] If columns are a MultiIndex, then use this level to unpivot the DataFrame. Provided for compatibility with pandas' melt, and applies only if neither names_sep nor names_pattern is provided. None names_sep Union[str, Pattern] Determines how the column name is broken up, if names_to contains multiple values. It takes the same specification as pandas' str.split method, and can be a string or regular expression. names_sep does not work with MultiIndex columns. None names_pattern Union[list, tuple, str, Pattern] Determines how the column name is broken up. It can be a regular expression containing matching groups (it takes the same specification as pandas' str.extract method), or a list/tuple of regular expressions. If it is a single regex, the number of groups must match the length of names_to . For a list/tuple of regular expressions, names_to must also be a list/tuple and the lengths of both arguments must match. names_pattern does not work with MultiIndex columns. None names_transform Union[str, Callable, dict] Use this option to change the types of columns that have been transformed to rows. This does not applies to the values' columns. Accepts any argument that is acceptable by pd.astype . None dropna bool Determines whether or not to drop nulls from the values columns. Default is False . False sort_by_appearance Optional[bool] Default False . Boolean value that determines the final look of the DataFrame. If True , the unpivoted DataFrame will be stacked in order of first appearance. False ignore_index Optional[bool] Default True . If True , the original index is ignored. If False , the original index is retained and the index labels will be repeated as necessary. True Returns: Type Description DataFrame A pandas DataFrame that has been unpivoted from wide to long format. Source code in janitor/functions/pivot.py @pf.register_dataframe_method def pivot_longer( df: pd.DataFrame, index: Optional[Union[list, tuple, str, Pattern]] = None, column_names: Optional[Union[list, tuple, str, Pattern]] = None, names_to: Optional[Union[list, tuple, str]] = None, values_to: Optional[str] = \"value\", column_level: Optional[Union[int, str]] = None, names_sep: Optional[Union[str, Pattern]] = None, names_pattern: Optional[Union[list, tuple, str, Pattern]] = None, names_transform: Optional[Union[str, Callable, dict]] = None, dropna: bool = False, sort_by_appearance: Optional[bool] = False, ignore_index: Optional[bool] = True, ) -> pd.DataFrame: \"\"\" Unpivots a DataFrame from *wide* to *long* format. This method does not mutate the original DataFrame. It is modeled after the `pivot_longer` function in R's tidyr package, and also takes inspiration from R's data.table package. This function is useful to massage a DataFrame into a format where one or more columns are considered measured variables, and all other columns are considered as identifier variables. All measured variables are *unpivoted* (and typically duplicated) along the row axis. Column selection in `index` and `column_names` is possible using the [`select_columns`][janitor.functions.select.select_columns] syntax. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"Sepal.Length\": [5.1, 5.9], ... \"Sepal.Width\": [3.5, 3.0], ... \"Petal.Length\": [1.4, 5.1], ... \"Petal.Width\": [0.2, 1.8], ... \"Species\": [\"setosa\", \"virginica\"], ... } ... ) >>> df Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 5.1 3.5 1.4 0.2 setosa 1 5.9 3.0 5.1 1.8 virginica Replicate pandas' melt: >>> df.pivot_longer(index = 'Species') Species variable value 0 setosa Sepal.Length 5.1 1 virginica Sepal.Length 5.9 2 setosa Sepal.Width 3.5 3 virginica Sepal.Width 3.0 4 setosa Petal.Length 1.4 5 virginica Petal.Length 5.1 6 setosa Petal.Width 0.2 7 virginica Petal.Width 1.8 Split the column labels into parts: >>> df.pivot_longer( ... index = 'Species', ... names_to = ('part', 'dimension'), ... names_sep = '.', ... sort_by_appearance = True, ... ) Species part dimension value 0 setosa Sepal Length 5.1 1 setosa Sepal Width 3.5 2 setosa Petal Length 1.4 3 setosa Petal Width 0.2 4 virginica Sepal Length 5.9 5 virginica Sepal Width 3.0 6 virginica Petal Length 5.1 7 virginica Petal Width 1.8 Retain parts of the column names as headers: >>> df.pivot_longer( ... index = 'Species', ... names_to = ('part', '.value'), ... names_sep = '.', ... sort_by_appearance = True, ... ) Species part Length Width 0 setosa Sepal 5.1 3.5 1 setosa Petal 1.4 0.2 2 virginica Sepal 5.9 3.0 3 virginica Petal 5.1 1.8 Split the column labels based on regex: >>> df = pd.DataFrame({\"id\": [1], \"new_sp_m5564\": [2], \"newrel_f65\": [3]}) >>> df id new_sp_m5564 newrel_f65 0 1 2 3 >>> df.pivot_longer( ... index = 'id', ... names_to = ('diagnosis', 'gender', 'age'), ... names_pattern = r\"new_?(.+)_(.)(\\\\d+)\", ... ) id diagnosis gender age value 0 1 sp m 5564 2 1 1 rel f 65 3 Convert the dtypes of specific columns with `names_transform`: >>> result = (df ... .pivot_longer( ... index = 'id', ... names_to = ('diagnosis', 'gender', 'age'), ... names_pattern = r\"new_?(.+)_(.)(\\\\d+)\", ... names_transform = {'gender': 'category', 'age':'int'}) ... ) >>> result.dtypes id int64 gender category age int64 value int64 dtype: object Use multiple `.value` to reshape dataframe: >>> df = pd.DataFrame( ... [ ... { ... \"x_1_mean\": 10, ... \"x_2_mean\": 20, ... \"y_1_mean\": 30, ... \"y_2_mean\": 40, ... \"unit\": 50, ... } ... ] ... ) >>> df x_1_mean x_2_mean y_1_mean y_2_mean unit 0 10 20 30 40 50 >>> df.pivot_longer( ... index=\"unit\", ... names_to=(\".value\", \"time\", \".value\"), ... names_pattern=r\"(x|y)_([0-9])(_mean)\", ... ) unit time x_mean y_mean 0 50 1 10 30 1 50 2 20 40 Multiple values_to: >>> df = pd.DataFrame( ... { ... \"City\": [\"Houston\", \"Austin\", \"Hoover\"], ... \"State\": [\"Texas\", \"Texas\", \"Alabama\"], ... \"Name\": [\"Aria\", \"Penelope\", \"Niko\"], ... \"Mango\": [4, 10, 90], ... \"Orange\": [10, 8, 14], ... \"Watermelon\": [40, 99, 43], ... \"Gin\": [16, 200, 34], ... \"Vodka\": [20, 33, 18], ... }, ... columns=[ ... \"City\", ... \"State\", ... \"Name\", ... \"Mango\", ... \"Orange\", ... \"Watermelon\", ... \"Gin\", ... \"Vodka\", ... ], ... ) >>> df City State Name Mango Orange Watermelon Gin Vodka 0 Houston Texas Aria 4 10 40 16 20 1 Austin Texas Penelope 10 8 99 200 33 2 Hoover Alabama Niko 90 14 43 34 18 >>> df.pivot_longer( ... index=[\"City\", \"State\"], ... column_names=slice(\"Mango\", \"Vodka\"), ... names_to=(\"Fruit\", \"Drink\"), ... values_to=(\"Pounds\", \"Ounces\"), ... names_pattern=[r\"M|O|W\", r\"G|V\"], ... ) City State Fruit Pounds Drink Ounces 0 Houston Texas Mango 4 Gin 16.0 1 Austin Texas Mango 10 Gin 200.0 2 Hoover Alabama Mango 90 Gin 34.0 3 Houston Texas Orange 10 Vodka 20.0 4 Austin Texas Orange 8 Vodka 33.0 5 Hoover Alabama Orange 14 Vodka 18.0 6 Houston Texas Watermelon 40 None NaN 7 Austin Texas Watermelon 99 None NaN 8 Hoover Alabama Watermelon 43 None NaN !!! abstract \"Version Changed\" - 0.24.0 - Added `dropna` parameter. :param df: A pandas DataFrame. :param index: Name(s) of columns to use as identifier variables. Should be either a single column name, or a list/tuple of column names. `index` should be a list of tuples if the columns are a MultiIndex. :param column_names: Name(s) of columns to unpivot. Should be either a single column name or a list/tuple of column names. `column_names` should be a list of tuples if the columns are a MultiIndex. :param names_to: Name of new column as a string that will contain what were previously the column names in `column_names`. The default is `variable` if no value is provided. It can also be a list/tuple of strings that will serve as new column names, if `name_sep` or `names_pattern` is provided. If `.value` is in `names_to`, new column names will be extracted from part of the existing column names and overrides`values_to`. :param values_to: Name of new column as a string that will contain what were previously the values of the columns in `column_names`. values_to can also be a list/tuple and requires that names_pattern is also a list/tuple. :param column_level: If columns are a MultiIndex, then use this level to unpivot the DataFrame. Provided for compatibility with pandas' melt, and applies only if neither `names_sep` nor `names_pattern` is provided. :param names_sep: Determines how the column name is broken up, if `names_to` contains multiple values. It takes the same specification as pandas' `str.split` method, and can be a string or regular expression. `names_sep` does not work with MultiIndex columns. :param names_pattern: Determines how the column name is broken up. It can be a regular expression containing matching groups (it takes the same specification as pandas' `str.extract` method), or a list/tuple of regular expressions. If it is a single regex, the number of groups must match the length of `names_to`. For a list/tuple of regular expressions, `names_to` must also be a list/tuple and the lengths of both arguments must match. `names_pattern` does not work with MultiIndex columns. :param names_transform: Use this option to change the types of columns that have been transformed to rows. This does not applies to the values' columns. Accepts any argument that is acceptable by `pd.astype`. :param dropna: Determines whether or not to drop nulls from the values columns. Default is `False`. :param sort_by_appearance: Default `False`. Boolean value that determines the final look of the DataFrame. If `True`, the unpivoted DataFrame will be stacked in order of first appearance. :param ignore_index: Default `True`. If `True`, the original index is ignored. If `False`, the original index is retained and the index labels will be repeated as necessary. :returns: A pandas DataFrame that has been unpivoted from wide to long format. \"\"\" # noqa: E501 # this code builds on the wonderful work of @benjaminjack\u2019s PR # https://github.com/benjaminjack/pyjanitor/commit/e3df817903c20dd21634461c8a92aec137963ed0 df = df.copy() ( df, index, column_names, names_to, values_to, names_sep, names_pattern, names_transform, dropna, sort_by_appearance, ignore_index, ) = _data_checks_pivot_longer( df, index, column_names, names_to, values_to, column_level, names_sep, names_pattern, names_transform, dropna, sort_by_appearance, ignore_index, ) return _computations_pivot_longer( df, index, column_names, names_to, values_to, names_sep, names_pattern, names_transform, dropna, sort_by_appearance, ignore_index, ) pivot_wider(df, index=None, names_from=None, values_from=None, flatten_levels=True, names_sep='_', names_glue=None, reset_index=True, names_expand=False, index_expand=False) Reshapes data from long to wide form. The number of columns are increased, while decreasing the number of rows. It is the inverse of the pivot_longer method, and is a wrapper around pd.DataFrame.pivot method. This method does not mutate the original DataFrame. Column selection in index , names_from and values_from is possible using the select_columns syntax. A ValueError is raised if the combination of the index and names_from is not unique. By default, values from values_from are always at the top level if the columns are not flattened. If flattened, the values from values_from are usually at the start of each label in the columns. Example: >>> import pandas as pd >>> import janitor >>> df = [{'dep': 5.5, 'step': 1, 'a': 20, 'b': 30}, ... {'dep': 5.5, 'step': 2, 'a': 25, 'b': 37}, ... {'dep': 6.1, 'step': 1, 'a': 22, 'b': 19}, ... {'dep': 6.1, 'step': 2, 'a': 18, 'b': 29}] >>> df = pd.DataFrame(df) >>> df dep step a b 0 5.5 1 20 30 1 5.5 2 25 37 2 6.1 1 22 19 3 6.1 2 18 29 Pivot and flatten columns: >>> df.pivot_wider( ... index = \"dep\", ... names_from = \"step\", ... ) dep a_1 a_2 b_1 b_2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 Modify columns with names_sep : >>> df.pivot_wider( ... index = \"dep\", ... names_from = \"step\", ... names_sep = \"\", ... ) dep a1 a2 b1 b2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 Modify columns with names_glue : >>> df.pivot_wider( ... index = \"dep\", ... names_from = \"step\", ... names_glue = \"{_value}_step{step}\", ... ) dep a_step1 a_step2 b_step1 b_step2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 Version Changed 0.24.0 Added reset_index , names_expand and index_expand parameters. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required index Union[list, str] Name(s) of columns to use as identifier variables. It should be either a single column name, or a list of column names. If index is not provided, the DataFrame's index is used. None names_from Union[list, str] Name(s) of column(s) to use to make the new DataFrame's columns. Should be either a single column name, or a list of column names. None values_from Union[list, str] Name(s) of column(s) that will be used for populating the new DataFrame's values. If values_from is not specified, all remaining columns will be used. None flatten_levels Optional[bool] Default is True . If False , the DataFrame stays as a MultiIndex. True names_sep str If names_from or values_from contain multiple variables, this will be used to join the values into a single string to use as a column name. Default is _ . Applicable only if flatten_levels is True . '_' names_glue str A string to control the output of the flattened columns. It offers more flexibility in creating custom column names, and uses python's str.format_map under the hood. Simply create the string template, using the column labels in names_from , and special _value as a placeholder for values_from . Applicable only if flatten_levels is True . None reset_index bool Determines whether to restore index as a column/columns. Applicable only if index is provided, and flatten_levels is True . Default is True . True names_expand bool Expand columns to show all the categories. Applies only if names_from is a categorical column. Default is False . False index_expand bool Expand the index to show all the categories. Applies only if index is a categorical column. Default is False . False Returns: Type Description DataFrame A pandas DataFrame that has been unpivoted from long to wide form. Source code in janitor/functions/pivot.py @pf.register_dataframe_method def pivot_wider( df: pd.DataFrame, index: Optional[Union[list, str]] = None, names_from: Optional[Union[list, str]] = None, values_from: Optional[Union[list, str]] = None, flatten_levels: Optional[bool] = True, names_sep: str = \"_\", names_glue: str = None, reset_index: bool = True, names_expand: bool = False, index_expand: bool = False, ) -> pd.DataFrame: \"\"\" Reshapes data from *long* to *wide* form. The number of columns are increased, while decreasing the number of rows. It is the inverse of the [`pivot_longer`][janitor.functions.pivot.pivot_longer] method, and is a wrapper around `pd.DataFrame.pivot` method. This method does not mutate the original DataFrame. Column selection in `index`, `names_from` and `values_from` is possible using the [`select_columns`][janitor.functions.select.select_columns] syntax. A ValueError is raised if the combination of the `index` and `names_from` is not unique. By default, values from `values_from` are always at the top level if the columns are not flattened. If flattened, the values from `values_from` are usually at the start of each label in the columns. Example: >>> import pandas as pd >>> import janitor >>> df = [{'dep': 5.5, 'step': 1, 'a': 20, 'b': 30}, ... {'dep': 5.5, 'step': 2, 'a': 25, 'b': 37}, ... {'dep': 6.1, 'step': 1, 'a': 22, 'b': 19}, ... {'dep': 6.1, 'step': 2, 'a': 18, 'b': 29}] >>> df = pd.DataFrame(df) >>> df dep step a b 0 5.5 1 20 30 1 5.5 2 25 37 2 6.1 1 22 19 3 6.1 2 18 29 Pivot and flatten columns: >>> df.pivot_wider( ... index = \"dep\", ... names_from = \"step\", ... ) dep a_1 a_2 b_1 b_2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 Modify columns with `names_sep`: >>> df.pivot_wider( ... index = \"dep\", ... names_from = \"step\", ... names_sep = \"\", ... ) dep a1 a2 b1 b2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 Modify columns with `names_glue`: >>> df.pivot_wider( ... index = \"dep\", ... names_from = \"step\", ... names_glue = \"{_value}_step{step}\", ... ) dep a_step1 a_step2 b_step1 b_step2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 !!! abstract \"Version Changed\" - 0.24.0 - Added `reset_index`, `names_expand` and `index_expand` parameters. :param df: A pandas DataFrame. :param index: Name(s) of columns to use as identifier variables. It should be either a single column name, or a list of column names. If `index` is not provided, the DataFrame's index is used. :param names_from: Name(s) of column(s) to use to make the new DataFrame's columns. Should be either a single column name, or a list of column names. :param values_from: Name(s) of column(s) that will be used for populating the new DataFrame's values. If `values_from` is not specified, all remaining columns will be used. :param flatten_levels: Default is `True`. If `False`, the DataFrame stays as a MultiIndex. :param names_sep: If `names_from` or `values_from` contain multiple variables, this will be used to join the values into a single string to use as a column name. Default is `_`. Applicable only if `flatten_levels` is `True`. :param names_glue: A string to control the output of the flattened columns. It offers more flexibility in creating custom column names, and uses python's `str.format_map` under the hood. Simply create the string template, using the column labels in `names_from`, and special `_value` as a placeholder for `values_from`. Applicable only if `flatten_levels` is `True`. :param reset_index: Determines whether to restore `index` as a column/columns. Applicable only if `index` is provided, and `flatten_levels` is `True`. Default is `True`. :param names_expand: Expand columns to show all the categories. Applies only if `names_from` is a categorical column. Default is `False`. :param index_expand: Expand the index to show all the categories. Applies only if `index` is a categorical column. Default is `False`. :returns: A pandas DataFrame that has been unpivoted from long to wide form. \"\"\" # noqa: E501 df = df.copy() return _computations_pivot_wider( df, index, names_from, values_from, flatten_levels, names_sep, names_glue, reset_index, names_expand, index_expand, ) process_text process_text(df, column_name, string_function, **kwargs) Apply a Pandas string method to an existing column. This function aims to make string cleaning easy, while chaining, by simply passing the string method name, along with keyword arguments, if any, to the function. This modifies an existing column; it does not create a new column; new columns can be created via pyjanitor's transform_columns . A list of all the string methods in Pandas can be accessed here . Example: >>> import pandas as pd >>> import janitor >>> import re >>> df = pd.DataFrame({\"text\": [\"Ragnar\", \"sammywemmy\", \"ginger\"], ... \"code\": [1, 2, 3]}) >>> df text code 0 Ragnar 1 1 sammywemmy 2 2 ginger 3 >>> df.process_text(column_name=\"text\", string_function=\"lower\") text code 0 ragnar 1 1 sammywemmy 2 2 ginger 3 For string methods with parameters, simply pass the keyword arguments: >>> df.process_text( ... column_name=\"text\", ... string_function=\"extract\", ... pat=r\"(ag)\", ... expand=False, ... flags=re.IGNORECASE, ... ) text code 0 ag 1 1 NaN 2 2 NaN 3 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str String column to be operated on. required string_function str pandas string method to be applied. required kwargs Keyword arguments for parameters of the string_function . {} Returns: Type Description DataFrame A pandas DataFrame with modified column. Exceptions: Type Description KeyError If string_function is not a Pandas string method. ValueError If the text function returns a DataFrame, instead of a Series. Source code in janitor/functions/process_text.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def process_text( df: pd.DataFrame, column_name: str, string_function: str, **kwargs, ) -> pd.DataFrame: \"\"\" Apply a Pandas string method to an existing column. This function aims to make string cleaning easy, while chaining, by simply passing the string method name, along with keyword arguments, if any, to the function. This modifies an existing column; it does not create a new column; new columns can be created via pyjanitor's [`transform_columns`][janitor.functions.transform_columns.transform_columns]. A list of all the string methods in Pandas can be accessed [here](https://pandas.pydata.org/docs/user_guide/text.html#method-summary). Example: >>> import pandas as pd >>> import janitor >>> import re >>> df = pd.DataFrame({\"text\": [\"Ragnar\", \"sammywemmy\", \"ginger\"], ... \"code\": [1, 2, 3]}) >>> df text code 0 Ragnar 1 1 sammywemmy 2 2 ginger 3 >>> df.process_text(column_name=\"text\", string_function=\"lower\") text code 0 ragnar 1 1 sammywemmy 2 2 ginger 3 For string methods with parameters, simply pass the keyword arguments: >>> df.process_text( ... column_name=\"text\", ... string_function=\"extract\", ... pat=r\"(ag)\", ... expand=False, ... flags=re.IGNORECASE, ... ) text code 0 ag 1 1 NaN 2 2 NaN 3 :param df: A pandas DataFrame. :param column_name: String column to be operated on. :param string_function: pandas string method to be applied. :param kwargs: Keyword arguments for parameters of the `string_function`. :returns: A pandas DataFrame with modified column. :raises KeyError: If `string_function` is not a Pandas string method. :raises ValueError: If the text function returns a DataFrame, instead of a Series. \"\"\" # noqa: E501 check(\"column_name\", column_name, [str]) check(\"string_function\", string_function, [str]) check_column(df, [column_name]) pandas_string_methods = [ func.__name__ for _, func in inspect.getmembers(pd.Series.str, inspect.isfunction) if not func.__name__.startswith(\"_\") ] if string_function not in pandas_string_methods: raise KeyError(f\"{string_function} is not a Pandas string method.\") result = getattr(df[column_name].str, string_function)(**kwargs) if isinstance(result, pd.DataFrame): raise ValueError( \"The outcome of the processed text is a DataFrame, \" \"which is not supported in `process_text`.\" ) return df.assign(**{column_name: result}) remove_columns Implementation of remove_columns. remove_columns(df, column_names) Remove the set of columns specified in column_names . This method does not mutate the original DataFrame. Intended to be the method-chaining alternative to del df[col] . Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [2, 4, 6], \"b\": [1, 3, 5], \"c\": [7, 8, 9]}) >>> df a b c 0 2 1 7 1 4 3 8 2 6 5 9 >>> df.remove_columns(column_names=['a', 'c']) b 0 1 1 3 2 5 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names Union[str, Iterable[str], Hashable] The columns to remove. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/remove_columns.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def remove_columns( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable], ) -> pd.DataFrame: \"\"\"Remove the set of columns specified in `column_names`. This method does not mutate the original DataFrame. Intended to be the method-chaining alternative to `del df[col]`. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [2, 4, 6], \"b\": [1, 3, 5], \"c\": [7, 8, 9]}) >>> df a b c 0 2 1 7 1 4 3 8 2 6 5 9 >>> df.remove_columns(column_names=['a', 'c']) b 0 1 1 3 2 5 :param df: A pandas DataFrame. :param column_names: The columns to remove. :returns: A pandas DataFrame. \"\"\" return df.drop(columns=column_names) remove_empty Implementation of remove_empty. remove_empty(df) Drop all rows and columns that are completely null. This method also resets the index (by default) since it doesn't make sense to preserve the index of a completely empty row. This method mutates the original DataFrame. Implementation is inspired from StackOverflow . Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [1, np.nan, 2], ... \"b\": [3, np.nan, 4], ... \"c\": [np.nan, np.nan, np.nan], ... }) >>> df a b c 0 1.0 3.0 NaN 1 NaN NaN NaN 2 2.0 4.0 NaN >>> df.remove_empty() a b 0 1.0 3.0 1 2.0 4.0 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/remove_empty.py @pf.register_dataframe_method def remove_empty(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Drop all rows and columns that are completely null. This method also resets the index (by default) since it doesn't make sense to preserve the index of a completely empty row. This method mutates the original DataFrame. Implementation is inspired from [StackOverflow][so]. [so]: https://stackoverflow.com/questions/38884538/python-pandas-find-all-rows-where-all-values-are-nan Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [1, np.nan, 2], ... \"b\": [3, np.nan, 4], ... \"c\": [np.nan, np.nan, np.nan], ... }) >>> df a b c 0 1.0 3.0 NaN 1 NaN NaN NaN 2 2.0 4.0 NaN >>> df.remove_empty() a b 0 1.0 3.0 1 2.0 4.0 :param df: The pandas DataFrame object. :returns: A pandas DataFrame. \"\"\" # noqa: E501 nanrows = df.index[df.isna().all(axis=1)] df = df.drop(index=nanrows).reset_index(drop=True) nancols = df.columns[df.isna().all(axis=0)] df = df.drop(columns=nancols) return df rename_columns rename_column(df, old_column_name, new_column_name) Rename a column in place. This method does not mutate the original DataFrame. Example: Change the name of column 'a' to 'a_new'. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.rename_column(old_column_name='a', new_column_name='a_new') a_new b 0 0 a 1 1 b 2 2 c This is just syntactic sugar/a convenience function for renaming one column at a time. If you are convinced that there are multiple columns in need of changing, then use the pandas.DataFrame.rename method. Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required old_column_name str The old column name. required new_column_name str The new column name. required Returns: Type Description DataFrame A pandas DataFrame with renamed columns. Source code in janitor/functions/rename_columns.py @pf.register_dataframe_method @deprecated_alias(old=\"old_column_name\", new=\"new_column_name\") def rename_column( df: pd.DataFrame, old_column_name: str, new_column_name: str, ) -> pd.DataFrame: \"\"\"Rename a column in place. This method does not mutate the original DataFrame. Example: Change the name of column 'a' to 'a_new'. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.rename_column(old_column_name='a', new_column_name='a_new') a_new b 0 0 a 1 1 b 2 2 c This is just syntactic sugar/a convenience function for renaming one column at a time. If you are convinced that there are multiple columns in need of changing, then use the `pandas.DataFrame.rename` method. :param df: The pandas DataFrame object. :param old_column_name: The old column name. :param new_column_name: The new column name. :returns: A pandas DataFrame with renamed columns. \"\"\" # noqa: E501 check_column(df, [old_column_name]) return df.rename(columns={old_column_name: new_column_name}) rename_columns(df, new_column_names=None, function=None) Rename columns. This method does not mutate the original DataFrame. Example: Rename columns using a dictionary which maps old names to new names. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"xyz\")}) >>> df a b 0 0 x 1 1 y 2 2 z >>> df.rename_columns(new_column_names={\"a\": \"a_new\", \"b\": \"b_new\"}) a_new b_new 0 0 x 1 1 y 2 2 z Example: Rename columns using a generic callable. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"xyz\")}) >>> df.rename_columns(function=str.upper) A B 0 0 x 1 1 y 2 2 z One of the new_column_names or function are a required parameter. If both are provided, then new_column_names takes priority and function is never executed. Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required new_column_names Optional[Dict] A dictionary of old and new column names. None function Callable A function which should be applied to all the columns. None Returns: Type Description DataFrame A pandas DataFrame with renamed columns. Exceptions: Type Description ValueError if both new_column_names and function are None. Source code in janitor/functions/rename_columns.py @pf.register_dataframe_method def rename_columns( df: pd.DataFrame, new_column_names: Union[Dict, None] = None, function: Callable = None, ) -> pd.DataFrame: \"\"\"Rename columns. This method does not mutate the original DataFrame. Example: Rename columns using a dictionary which maps old names to new names. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"xyz\")}) >>> df a b 0 0 x 1 1 y 2 2 z >>> df.rename_columns(new_column_names={\"a\": \"a_new\", \"b\": \"b_new\"}) a_new b_new 0 0 x 1 1 y 2 2 z Example: Rename columns using a generic callable. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"xyz\")}) >>> df.rename_columns(function=str.upper) A B 0 0 x 1 1 y 2 2 z One of the `new_column_names` or `function` are a required parameter. If both are provided, then `new_column_names` takes priority and `function` is never executed. :param df: The pandas DataFrame object. :param new_column_names: A dictionary of old and new column names. :param function: A function which should be applied to all the columns. :returns: A pandas DataFrame with renamed columns. :raises ValueError: if both `new_column_names` and `function` are None. \"\"\" # noqa: E501 if new_column_names is None and function is None: raise ValueError( \"One of new_column_names or function must be provided\" ) if new_column_names is not None: check_column(df, new_column_names) return df.rename(columns=new_column_names) return df.rename(mapper=function, axis=\"columns\") reorder_columns reorder_columns(df, column_order) Reorder DataFrame columns by specifying desired order as list of col names. Columns not specified retain their order and follow after the columns specified in column_order . All columns specified within the column_order list must be present within df . This method does not mutate the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": [1, 1, 1], \"col2\": [2, 2, 2], \"col3\": [3, 3, 3]}) >>> df col1 col2 col3 0 1 2 3 1 1 2 3 2 1 2 3 >>> df.reorder_columns(['col3', 'col1']) col3 col1 col2 0 3 1 2 1 3 1 2 2 3 1 2 Notice that the column order of df is now col3 , col1 , col2 . Internally, this function uses DataFrame.reindex with copy=False to avoid unnecessary data duplication. Parameters: Name Type Description Default df DataFrame DataFrame to reorder required column_order Union[Iterable[str], pandas.core.indexes.base.Index, Hashable] A list of column names or Pandas Index specifying their order in the returned DataFrame . required Returns: Type Description DataFrame A pandas DataFrame with reordered columns. Exceptions: Type Description IndexError if a column within column_order is not found within the DataFrame. Source code in janitor/functions/reorder_columns.py @pf.register_dataframe_method def reorder_columns( df: pd.DataFrame, column_order: Union[Iterable[str], pd.Index, Hashable] ) -> pd.DataFrame: \"\"\"Reorder DataFrame columns by specifying desired order as list of col names. Columns not specified retain their order and follow after the columns specified in `column_order`. All columns specified within the `column_order` list must be present within `df`. This method does not mutate the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": [1, 1, 1], \"col2\": [2, 2, 2], \"col3\": [3, 3, 3]}) >>> df col1 col2 col3 0 1 2 3 1 1 2 3 2 1 2 3 >>> df.reorder_columns(['col3', 'col1']) col3 col1 col2 0 3 1 2 1 3 1 2 2 3 1 2 Notice that the column order of `df` is now `col3`, `col1`, `col2`. Internally, this function uses `DataFrame.reindex` with `copy=False` to avoid unnecessary data duplication. :param df: `DataFrame` to reorder :param column_order: A list of column names or Pandas `Index` specifying their order in the returned `DataFrame`. :returns: A pandas DataFrame with reordered columns. :raises IndexError: if a column within `column_order` is not found within the DataFrame. \"\"\" # noqa: E501 check(\"column_order\", column_order, [list, tuple, pd.Index]) if any(col not in df.columns for col in column_order): raise IndexError( \"One or more columns in `column_order` were not found in the \" \"DataFrame.\" ) # if column_order is a Pandas index, needs conversion to list: column_order = list(column_order) return df.reindex( columns=( column_order + [col for col in df.columns if col not in column_order] ), copy=False, ) round_to_fraction Implementation of round_to_fraction round_to_fraction(df, column_name, denominator, digits=inf) Round all values in a column to a fraction. This method mutates the original DataFrame. Taken from the R package . Also, optionally round to a specified number of digits. Example: Round numeric column to the nearest 1/4 value. >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a1\": [1.263, 2.499, np.nan], ... \"a2\": [\"x\", \"y\", \"z\"], ... }) >>> df a1 a2 0 1.263 x 1 2.499 y 2 NaN z >>> df.round_to_fraction(\"a1\", denominator=4) a1 a2 0 1.25 x 1 2.50 y 2 NaN z Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Name of column to round to fraction. required denominator float The denominator of the fraction for rounding. Must be a positive number. required digits float The number of digits for rounding after rounding to the fraction. Default is np.inf (i.e. no subsequent rounding). inf Returns: Type Description DataFrame A pandas DataFrame with a column's values rounded. Exceptions: Type Description ValueError If denominator is not a positive number. Source code in janitor/functions/round_to_fraction.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\") def round_to_fraction( df: pd.DataFrame, column_name: Hashable, denominator: float, digits: float = np.inf, ) -> pd.DataFrame: \"\"\"Round all values in a column to a fraction. This method mutates the original DataFrame. Taken from [the R package](https://github.com/sfirke/janitor/issues/235). Also, optionally round to a specified number of digits. Example: Round numeric column to the nearest 1/4 value. >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a1\": [1.263, 2.499, np.nan], ... \"a2\": [\"x\", \"y\", \"z\"], ... }) >>> df a1 a2 0 1.263 x 1 2.499 y 2 NaN z >>> df.round_to_fraction(\"a1\", denominator=4) a1 a2 0 1.25 x 1 2.50 y 2 NaN z :param df: A pandas DataFrame. :param column_name: Name of column to round to fraction. :param denominator: The denominator of the fraction for rounding. Must be a positive number. :param digits: The number of digits for rounding after rounding to the fraction. Default is np.inf (i.e. no subsequent rounding). :returns: A pandas DataFrame with a column's values rounded. :raises ValueError: If `denominator` is not a positive number. \"\"\" check_column(df, column_name) check(\"denominator\", denominator, [float, int]) check(\"digits\", digits, [float, int]) if denominator <= 0: raise ValueError(\"denominator is expected to be a positive number.\") df[column_name] = round(df[column_name] * denominator, 0) / denominator if not np.isinf(digits): df[column_name] = round(df[column_name], digits) return df row_to_names Implementation of the row_to_names function. row_to_names(df, row_number=0, remove_row=False, remove_rows_above=False, reset_index=False) Elevates a row to be the column names of a DataFrame. This method mutates the original DataFrame. Contains options to remove the elevated row from the DataFrame along with removing the rows above the selected row. Example: Replace column names with the first row and reset the index. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [\"nums\", 6, 9], ... \"b\": [\"chars\", \"x\", \"y\"], ... }) >>> df a b 0 nums chars 1 6 x 2 9 y >>> df.row_to_names(0, remove_row=True, reset_index=True) nums chars 0 6 x 1 9 y Example: Remove rows above the elevated row and the elevated row itself. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [\"bla1\", \"nums\", 6, 9], ... \"b\": [\"bla2\", \"chars\", \"x\", \"y\"], ... }) >>> df a b 0 bla1 bla2 1 nums chars 2 6 x 3 9 y >>> df.row_to_names(1, remove_row=True, remove_rows_above=True, reset_index=True) nums chars 0 6 x 1 9 y Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required row_number int Position of the row containing the variable names. Note that indexing starts from 0. Defaults to 0 (first row). 0 remove_row bool Whether the row should be removed from the DataFrame. Defaults to False. False remove_rows_above bool Whether the rows above the selected row should be removed from the DataFrame. Defaults to False. False reset_index bool Whether the index should be reset on the returning DataFrame. Defaults to False. False Returns: Type Description DataFrame A pandas DataFrame with set column names. Source code in janitor/functions/row_to_names.py @pf.register_dataframe_method def row_to_names( df: pd.DataFrame, row_number: int = 0, remove_row: bool = False, remove_rows_above: bool = False, reset_index: bool = False, ) -> pd.DataFrame: \"\"\"Elevates a row to be the column names of a DataFrame. This method mutates the original DataFrame. Contains options to remove the elevated row from the DataFrame along with removing the rows above the selected row. Example: Replace column names with the first row and reset the index. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [\"nums\", 6, 9], ... \"b\": [\"chars\", \"x\", \"y\"], ... }) >>> df a b 0 nums chars 1 6 x 2 9 y >>> df.row_to_names(0, remove_row=True, reset_index=True) nums chars 0 6 x 1 9 y Example: Remove rows above the elevated row and the elevated row itself. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [\"bla1\", \"nums\", 6, 9], ... \"b\": [\"bla2\", \"chars\", \"x\", \"y\"], ... }) >>> df a b 0 bla1 bla2 1 nums chars 2 6 x 3 9 y >>> df.row_to_names(1, remove_row=True, remove_rows_above=True, reset_index=True) nums chars 0 6 x 1 9 y :param df: A pandas DataFrame. :param row_number: Position of the row containing the variable names. Note that indexing starts from 0. Defaults to 0 (first row). :param remove_row: Whether the row should be removed from the DataFrame. Defaults to False. :param remove_rows_above: Whether the rows above the selected row should be removed from the DataFrame. Defaults to False. :param reset_index: Whether the index should be reset on the returning DataFrame. Defaults to False. :returns: A pandas DataFrame with set column names. \"\"\" # noqa: E501 check(\"row_number\", row_number, [int]) warnings.warn( \"The function row_to_names will, in the official 1.0 release, \" \"change its behaviour to reset the dataframe's index by default. \" \"You can prepare for this change right now by explicitly setting \" \"`reset_index=True` when calling on `row_to_names`.\" ) df.columns = df.iloc[row_number, :] df.columns.name = None if remove_row: df = df.drop(df.index[row_number]) if remove_rows_above: df = df.drop(df.index[range(row_number)]) if reset_index: df = df.reset_index(drop=[\"index\"]) return df select select(df, *, rows=None, columns=None) Method-chainable selection of rows and columns. It accepts a string, shell-like glob strings (*string*) , regex, slice, array-like object, or a list of the previous options. Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary. This method does not mutate the original DataFrame. Selection can be inverted with the DropLabel class. New in version 0.24.0 Note The preferred option when selecting columns or rows in a Pandas DataFrame is with .loc or .iloc methods, as they are generally performant. select is primarily for convenience. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]], ... index=['cobra', 'viper', 'sidewinder'], ... columns=['max_speed', 'shield']) >>> df max_speed shield cobra 1 2 viper 4 5 sidewinder 7 8 >>> df.select(rows='cobra', columns='shield') shield cobra 2 Labels can be dropped with the DropLabel class: >>> df.select(rows=DropLabel('cobra')) max_speed shield viper 4 5 sidewinder 7 8 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required rows Valid inputs include: an exact label to look for, a shell-style glob string (e.g. *_thing_* ), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. None columns Valid inputs include: an exact label to look for, a shell-style glob string (e.g. *_thing_* ), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. None Returns: Type Description DataFrame A pandas DataFrame with the specified rows and/or columns selected. Source code in janitor/functions/select.py @pf.register_dataframe_method def select(df: pd.DataFrame, *, rows=None, columns=None) -> pd.DataFrame: \"\"\" Method-chainable selection of rows and columns. It accepts a string, shell-like glob strings `(*string*)`, regex, slice, array-like object, or a list of the previous options. Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary. This method does not mutate the original DataFrame. Selection can be inverted with the `DropLabel` class. !!! info \"New in version 0.24.0\" !!!note The preferred option when selecting columns or rows in a Pandas DataFrame is with `.loc` or `.iloc` methods, as they are generally performant. `select` is primarily for convenience. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]], ... index=['cobra', 'viper', 'sidewinder'], ... columns=['max_speed', 'shield']) >>> df max_speed shield cobra 1 2 viper 4 5 sidewinder 7 8 >>> df.select(rows='cobra', columns='shield') shield cobra 2 Labels can be dropped with the `DropLabel` class: >>> df.select(rows=DropLabel('cobra')) max_speed shield viper 4 5 sidewinder 7 8 :param df: A pandas DataFrame. :param rows: Valid inputs include: an exact label to look for, a shell-style glob string (e.g. `*_thing_*`), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. :param columns: Valid inputs include: an exact label to look for, a shell-style glob string (e.g. `*_thing_*`), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. :returns: A pandas DataFrame with the specified rows and/or columns selected. \"\"\" # noqa: E501 return _select(df, args=None, rows=rows, columns=columns, axis=\"both\") select_columns(df, *args, *, invert=False) Method-chainable selection of columns. It accepts a string, shell-like glob strings (*string*) , regex, slice, array-like object, or a list of the previous options. Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary. This method does not mutate the original DataFrame. Optional ability to invert selection of columns available as well. Note The preferred option when selecting columns or rows in a Pandas DataFrame is with .loc or .iloc methods, as they are generally performant. select_columns is primarily for convenience. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": [1, 2], \"foo\": [3, 4], \"col2\": [5, 6]}) >>> df col1 foo col2 0 1 3 5 1 2 4 6 >>> df.select_columns(\"col*\") col1 col2 0 1 5 1 2 6 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required args Valid inputs include: an exact column name to look for, a shell-style glob string (e.g. *_thing_* ), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. () invert bool Whether or not to invert the selection. This will result in the selection of the complement of the columns provided. False Returns: Type Description DataFrame A pandas DataFrame with the specified columns selected. Source code in janitor/functions/select.py @pf.register_dataframe_method @deprecated_alias(search_cols=\"search_column_names\") def select_columns( df: pd.DataFrame, *args, invert: bool = False, ) -> pd.DataFrame: \"\"\" Method-chainable selection of columns. It accepts a string, shell-like glob strings `(*string*)`, regex, slice, array-like object, or a list of the previous options. Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary. This method does not mutate the original DataFrame. Optional ability to invert selection of columns available as well. !!!note The preferred option when selecting columns or rows in a Pandas DataFrame is with `.loc` or `.iloc` methods, as they are generally performant. `select_columns` is primarily for convenience. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": [1, 2], \"foo\": [3, 4], \"col2\": [5, 6]}) >>> df col1 foo col2 0 1 3 5 1 2 4 6 >>> df.select_columns(\"col*\") col1 col2 0 1 5 1 2 6 :param df: A pandas DataFrame. :param args: Valid inputs include: an exact column name to look for, a shell-style glob string (e.g. `*_thing_*`), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. :param invert: Whether or not to invert the selection. This will result in the selection of the complement of the columns provided. :returns: A pandas DataFrame with the specified columns selected. \"\"\" # noqa: E501 return _select(df, args=args, invert=invert, axis=\"columns\") select_rows(df, *args, *, invert=False) Method-chainable selection of rows. It accepts a string, shell-like glob strings (*string*) , regex, slice, array-like object, or a list of the previous options. Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary. This method does not mutate the original DataFrame. Optional ability to invert selection of rows available as well. New in version 0.24.0 Note The preferred option when selecting columns or rows in a Pandas DataFrame is with .loc or .iloc methods, as they are generally performant. select_rows is primarily for convenience. Example: >>> import pandas as pd >>> import janitor >>> df = {\"col1\": [1, 2], \"foo\": [3, 4], \"col2\": [5, 6]} >>> df = pd.DataFrame.from_dict(df, orient='index') >>> df 0 1 col1 1 2 foo 3 4 col2 5 6 >>> df.select_rows(\"col*\") 0 1 col1 1 2 col2 5 6 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required args Valid inputs include: an exact index name to look for, a shell-style glob string (e.g. *_thing_* ), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. () invert bool Whether or not to invert the selection. This will result in the selection of the complement of the rows provided. False Returns: Type Description DataFrame A pandas DataFrame with the specified rows selected. Source code in janitor/functions/select.py @pf.register_dataframe_method def select_rows( df: pd.DataFrame, *args, invert: bool = False, ) -> pd.DataFrame: \"\"\" Method-chainable selection of rows. It accepts a string, shell-like glob strings `(*string*)`, regex, slice, array-like object, or a list of the previous options. Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary. This method does not mutate the original DataFrame. Optional ability to invert selection of rows available as well. !!! info \"New in version 0.24.0\" !!!note The preferred option when selecting columns or rows in a Pandas DataFrame is with `.loc` or `.iloc` methods, as they are generally performant. `select_rows` is primarily for convenience. Example: >>> import pandas as pd >>> import janitor >>> df = {\"col1\": [1, 2], \"foo\": [3, 4], \"col2\": [5, 6]} >>> df = pd.DataFrame.from_dict(df, orient='index') >>> df 0 1 col1 1 2 foo 3 4 col2 5 6 >>> df.select_rows(\"col*\") 0 1 col1 1 2 col2 5 6 :param df: A pandas DataFrame. :param args: Valid inputs include: an exact index name to look for, a shell-style glob string (e.g. `*_thing_*`), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. :param invert: Whether or not to invert the selection. This will result in the selection of the complement of the rows provided. :returns: A pandas DataFrame with the specified rows selected. \"\"\" # noqa: E501 return _select(df, args=args, invert=invert, axis=\"index\") shuffle Implementation of shuffle functions. shuffle(df, random_state=None, reset_index=True) Shuffle the rows of the DataFrame. This method does not mutate the original DataFrame. Super-sugary syntax! Underneath the hood, we use df.sample(frac=1) , with the option to set the random state. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"col1\": range(5), ... \"col2\": list(\"abcde\"), ... }) >>> df col1 col2 0 0 a 1 1 b 2 2 c 3 3 d 4 4 e >>> df.shuffle(random_state=42) col1 col2 0 1 b 1 4 e 2 2 c 3 0 a 4 3 d Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required random_state If provided, set a seed for the random number generator. None reset_index bool If True, reset the dataframe index to the default RangeIndex. True Returns: Type Description DataFrame A shuffled pandas DataFrame. Source code in janitor/functions/shuffle.py @pf.register_dataframe_method def shuffle( df: pd.DataFrame, random_state=None, reset_index: bool = True ) -> pd.DataFrame: \"\"\"Shuffle the rows of the DataFrame. This method does not mutate the original DataFrame. Super-sugary syntax! Underneath the hood, we use `df.sample(frac=1)`, with the option to set the random state. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"col1\": range(5), ... \"col2\": list(\"abcde\"), ... }) >>> df col1 col2 0 0 a 1 1 b 2 2 c 3 3 d 4 4 e >>> df.shuffle(random_state=42) col1 col2 0 1 b 1 4 e 2 2 c 3 0 a 4 3 d :param df: A pandas DataFrame. :param random_state: If provided, set a seed for the random number generator. :param reset_index: If True, reset the dataframe index to the default RangeIndex. :returns: A shuffled pandas DataFrame. \"\"\" result = df.sample(frac=1, random_state=random_state) if reset_index: result = result.reset_index(drop=True) return result sort_column_value_order Implementation of the sort_column_value_order function. sort_column_value_order(df, column, column_value_order, columns=None) This function adds precedence to certain values in a specified column, then sorts based on that column and any other specified columns. Example: >>> import pandas as pd >>> import janitor >>> import numpy as np >>> company_sales = { ... \"SalesMonth\": [\"Jan\", \"Feb\", \"Feb\", \"Mar\", \"April\"], ... \"Company1\": [150.0, 200.0, 200.0, 300.0, 400.0], ... \"Company2\": [180.0, 250.0, 250.0, np.nan, 500.0], ... \"Company3\": [400.0, 500.0, 500.0, 600.0, 675.0], ... } >>> df = pd.DataFrame.from_dict(company_sales) >>> df SalesMonth Company1 Company2 Company3 0 Jan 150.0 180.0 400.0 1 Feb 200.0 250.0 500.0 2 Feb 200.0 250.0 500.0 3 Mar 300.0 NaN 600.0 4 April 400.0 500.0 675.0 >>> df.sort_column_value_order( ... \"SalesMonth\", ... {\"April\": 1, \"Mar\": 2, \"Feb\": 3, \"Jan\": 4} ... ) SalesMonth Company1 Company2 Company3 4 April 400.0 500.0 675.0 3 Mar 300.0 NaN 600.0 1 Feb 200.0 250.0 500.0 2 Feb 200.0 250.0 500.0 0 Jan 150.0 180.0 400.0 Parameters: Name Type Description Default df DataFrame This is our DataFrame that we are manipulating required column str This is a column name as a string we are using to specify which column to sort by required column_value_order dict This is a dictionary of values that will represent precedence of the values in the specified column required columns This is a list of additional columns that we can sort by None Returns: Type Description DataFrame A sorted pandas DataFrame. Exceptions: Type Description ValueError raises error if chosen Column Name is not in Dataframe, or if column_value_order dictionary is empty. Source code in janitor/functions/sort_column_value_order.py @pf.register_dataframe_method def sort_column_value_order( df: pd.DataFrame, column: str, column_value_order: dict, columns=None ) -> pd.DataFrame: \"\"\" This function adds precedence to certain values in a specified column, then sorts based on that column and any other specified columns. Example: >>> import pandas as pd >>> import janitor >>> import numpy as np >>> company_sales = { ... \"SalesMonth\": [\"Jan\", \"Feb\", \"Feb\", \"Mar\", \"April\"], ... \"Company1\": [150.0, 200.0, 200.0, 300.0, 400.0], ... \"Company2\": [180.0, 250.0, 250.0, np.nan, 500.0], ... \"Company3\": [400.0, 500.0, 500.0, 600.0, 675.0], ... } >>> df = pd.DataFrame.from_dict(company_sales) >>> df SalesMonth Company1 Company2 Company3 0 Jan 150.0 180.0 400.0 1 Feb 200.0 250.0 500.0 2 Feb 200.0 250.0 500.0 3 Mar 300.0 NaN 600.0 4 April 400.0 500.0 675.0 >>> df.sort_column_value_order( ... \"SalesMonth\", ... {\"April\": 1, \"Mar\": 2, \"Feb\": 3, \"Jan\": 4} ... ) SalesMonth Company1 Company2 Company3 4 April 400.0 500.0 675.0 3 Mar 300.0 NaN 600.0 1 Feb 200.0 250.0 500.0 2 Feb 200.0 250.0 500.0 0 Jan 150.0 180.0 400.0 :param df: This is our DataFrame that we are manipulating :param column: This is a column name as a string we are using to specify which column to sort by :param column_value_order: This is a dictionary of values that will represent precedence of the values in the specified column :param columns: This is a list of additional columns that we can sort by :raises ValueError: raises error if chosen Column Name is not in Dataframe, or if column_value_order dictionary is empty. :return: A sorted pandas DataFrame. \"\"\" # Validation checks check_column(df, column, present=True) check(\"column_value_order\", column_value_order, [dict]) if not column_value_order: raise ValueError(\"column_value_order dictionary cannot be empty\") df = df.assign(cond_order=df[column].replace(column_value_order)) sort_by = [\"cond_order\"] if columns is not None: sort_by = [\"cond_order\"] + columns df = df.sort_values(sort_by).remove_columns(\"cond_order\") return df sort_naturally Implementation of the sort_naturally function. sort_naturally(df, column_name, **natsorted_kwargs) Sort a DataFrame by a column using natural sorting. Natural sorting is distinct from the default lexiographical sorting provided by pandas . For example, given the following list of items: [\"A1\", \"A11\", \"A3\", \"A2\", \"A10\"] lexicographical sorting would give us: [\"A1\", \"A10\", \"A11\", \"A2\", \"A3\"] By contrast, \"natural\" sorting would give us: [\"A1\", \"A2\", \"A3\", \"A10\", \"A11\"] This function thus provides natural sorting on a single column of a dataframe. To accomplish this, we do a natural sort on the unique values that are present in the dataframe. Then, we reconstitute the entire dataframe in the naturally sorted order. Natural sorting is provided by the Python package natsort All keyword arguments to natsort should be provided after the column name to sort by is provided. They are passed through to the natsorted function. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"Well\": [\"A21\", \"A3\", \"A21\", \"B2\", \"B51\", \"B12\"], ... \"Value\": [1, 2, 13, 3, 4, 7], ... } ... ) >>> df Well Value 0 A21 1 1 A3 2 2 A21 13 3 B2 3 4 B51 4 5 B12 7 >>> df.sort_naturally(\"Well\") Well Value 1 A3 2 0 A21 1 2 A21 13 3 B2 3 5 B12 7 4 B51 4 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str The column on which natural sorting should take place. required natsorted_kwargs Keyword arguments to be passed to natsort's natsorted function. {} Returns: Type Description DataFrame A sorted pandas DataFrame. Source code in janitor/functions/sort_naturally.py @pf.register_dataframe_method def sort_naturally( df: pd.DataFrame, column_name: str, **natsorted_kwargs ) -> pd.DataFrame: \"\"\"Sort a DataFrame by a column using *natural* sorting. Natural sorting is distinct from the default lexiographical sorting provided by `pandas`. For example, given the following list of items: [\"A1\", \"A11\", \"A3\", \"A2\", \"A10\"] lexicographical sorting would give us: [\"A1\", \"A10\", \"A11\", \"A2\", \"A3\"] By contrast, \"natural\" sorting would give us: [\"A1\", \"A2\", \"A3\", \"A10\", \"A11\"] This function thus provides *natural* sorting on a single column of a dataframe. To accomplish this, we do a natural sort on the unique values that are present in the dataframe. Then, we reconstitute the entire dataframe in the naturally sorted order. Natural sorting is provided by the Python package [natsort](https://natsort.readthedocs.io/en/master/index.html) All keyword arguments to `natsort` should be provided after the column name to sort by is provided. They are passed through to the `natsorted` function. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"Well\": [\"A21\", \"A3\", \"A21\", \"B2\", \"B51\", \"B12\"], ... \"Value\": [1, 2, 13, 3, 4, 7], ... } ... ) >>> df Well Value 0 A21 1 1 A3 2 2 A21 13 3 B2 3 4 B51 4 5 B12 7 >>> df.sort_naturally(\"Well\") Well Value 1 A3 2 0 A21 1 2 A21 13 3 B2 3 5 B12 7 4 B51 4 :param df: A pandas DataFrame. :param column_name: The column on which natural sorting should take place. :param natsorted_kwargs: Keyword arguments to be passed to natsort's `natsorted` function. :returns: A sorted pandas DataFrame. \"\"\" new_order = index_natsorted(df[column_name], **natsorted_kwargs) return df.iloc[new_order, :] take_first Implementation of take_first function. take_first(df, subset, by, ascending=True) Take the first row within each group specified by subset . Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [\"x\", \"x\", \"y\", \"y\"], \"b\": [0, 1, 2, 3]}) >>> df a b 0 x 0 1 x 1 2 y 2 3 y 3 >>> df.take_first(subset=\"a\", by=\"b\") a b 0 x 0 2 y 2 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required subset Union[Hashable, Iterable[Hashable]] Column(s) defining the group. required by Hashable Column to sort by. required ascending bool Whether or not to sort in ascending order, bool . True Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/take_first.py @pf.register_dataframe_method def take_first( df: pd.DataFrame, subset: Union[Hashable, Iterable[Hashable]], by: Hashable, ascending: bool = True, ) -> pd.DataFrame: \"\"\" Take the first row within each group specified by `subset`. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [\"x\", \"x\", \"y\", \"y\"], \"b\": [0, 1, 2, 3]}) >>> df a b 0 x 0 1 x 1 2 y 2 3 y 3 >>> df.take_first(subset=\"a\", by=\"b\") a b 0 x 0 2 y 2 :param df: A pandas DataFrame. :param subset: Column(s) defining the group. :param by: Column to sort by. :param ascending: Whether or not to sort in ascending order, `bool`. :returns: A pandas DataFrame. \"\"\" result = df.sort_values(by=by, ascending=ascending).drop_duplicates( subset=subset, keep=\"first\" ) return result then then(df, func) Add an arbitrary function to run in the pyjanitor method chain. This method does not mutate the original DataFrame. Example: A trivial example using a lambda func . >>> import pandas as pd >>> import janitor >>> (pd.DataFrame({\"a\": [1, 2, 3], \"b\": [7, 8, 9]}) ... .then(lambda df: df * 2)) a b 0 2 14 1 4 16 2 6 18 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required func Callable A function you would like to run in the method chain. It should take one parameter and return one parameter, each being the DataFrame object. After that, do whatever you want in the middle. Go crazy. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/then.py @pf.register_dataframe_method def then(df: pd.DataFrame, func: Callable) -> pd.DataFrame: \"\"\"Add an arbitrary function to run in the `pyjanitor` method chain. This method does not mutate the original DataFrame. Example: A trivial example using a lambda `func`. >>> import pandas as pd >>> import janitor >>> (pd.DataFrame({\"a\": [1, 2, 3], \"b\": [7, 8, 9]}) ... .then(lambda df: df * 2)) a b 0 2 14 1 4 16 2 6 18 :param df: A pandas DataFrame. :param func: A function you would like to run in the method chain. It should take one parameter and return one parameter, each being the DataFrame object. After that, do whatever you want in the middle. Go crazy. :returns: A pandas DataFrame. \"\"\" df = func(df) return df to_datetime to_datetime(df, column_name, **kwargs) Convert column to a datetime type, in-place. Intended to be the method-chaining equivalent of: df[column_name] = pd.to_datetime(df[column_name], **kwargs) This method mutates the original DataFrame. Example: Converting a string column to datetime type with custom format. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'date': ['20200101', '20200202', '20200303']}) >>> df date 0 20200101 1 20200202 2 20200303 >>> df.to_datetime('date', format='%Y%m%d') date 0 2020-01-01 1 2020-02-02 2 2020-03-03 Read the pandas documentation for to_datetime for more information. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Column name. required kwargs Provide any kwargs that pd.to_datetime can take. {} Returns: Type Description DataFrame A pandas DataFrame with updated datetime data. Source code in janitor/functions/to_datetime.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def to_datetime( df: pd.DataFrame, column_name: Hashable, **kwargs ) -> pd.DataFrame: \"\"\"Convert column to a datetime type, in-place. Intended to be the method-chaining equivalent of: df[column_name] = pd.to_datetime(df[column_name], **kwargs) This method mutates the original DataFrame. Example: Converting a string column to datetime type with custom format. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'date': ['20200101', '20200202', '20200303']}) >>> df date 0 20200101 1 20200202 2 20200303 >>> df.to_datetime('date', format='%Y%m%d') date 0 2020-01-01 1 2020-02-02 2 2020-03-03 Read the pandas documentation for [`to_datetime`][pd_docs] for more information. [pd_docs]: https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html :param df: A pandas DataFrame. :param column_name: Column name. :param kwargs: Provide any kwargs that `pd.to_datetime` can take. :returns: A pandas DataFrame with updated datetime data. \"\"\" # noqa: E501 df[column_name] = pd.to_datetime(df[column_name], **kwargs) return df toset Implementation of the toset function. toset(series) Return a set of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period) Example: >>> import pandas as pd >>> import janitor >>> s = pd.Series([1, 2, 3, 5, 5], index=[\"a\", \"b\", \"c\", \"d\", \"e\"]) >>> s a 1 b 2 c 3 d 5 e 5 dtype: int64 >>> s.toset() {1, 2, 3, 5} Parameters: Name Type Description Default series Series A pandas series. required Returns: Type Description Set A set of values. Source code in janitor/functions/toset.py @pf.register_series_method def toset(series: pd.Series) -> Set: \"\"\"Return a set of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period) Example: >>> import pandas as pd >>> import janitor >>> s = pd.Series([1, 2, 3, 5, 5], index=[\"a\", \"b\", \"c\", \"d\", \"e\"]) >>> s a 1 b 2 c 3 d 5 e 5 dtype: int64 >>> s.toset() {1, 2, 3, 5} :param series: A pandas series. :returns: A set of values. \"\"\" return set(series.tolist()) transform_columns transform_column(df, column_name, function, dest_column_name=None, elementwise=True) Transform the given column using the provided function. Meant to be the method-chaining equivalent of: df[dest_column_name] = df[column_name].apply(function) Functions can be applied in one of two ways: Element-wise (default; elementwise=True ). Then, the individual column elements will be passed in as the first argument of function . Column-wise ( elementwise=False ). Then, function is expected to take in a pandas Series and return a sequence that is of identical length to the original. If dest_column_name is provided, then the transformation result is stored in that column. Otherwise, the transformed result is stored under the name of the original column. This method does not mutate the original DataFrame. Example: Transform a column in-place with an element-wise function. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [2, 3, 4], ... \"b\": [\"area\", \"pyjanitor\", \"grapefruit\"], ... }) >>> df a b 0 2 area 1 3 pyjanitor 2 4 grapefruit >>> df.transform_column( ... column_name=\"a\", ... function=lambda x: x**2 - 1, ... ) a b 0 3 area 1 8 pyjanitor 2 15 grapefruit Example: Transform a column in-place with an column-wise function. >>> df.transform_column( ... column_name=\"b\", ... function=lambda srs: srs.str[:5], ... elementwise=False, ... ) a b 0 2 area 1 3 pyjan 2 4 grape Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column to transform. required function Callable A function to apply on the column. required dest_column_name Optional[str] The column name to store the transformation result in. Defaults to None, which will result in the original column name being overwritten. If a name is provided here, then a new column with the transformed values will be created. None elementwise bool Whether to apply the function elementwise or not. If elementwise is True, then the function's first argument should be the data type of each datum in the column of data, and should return a transformed datum. If elementwise is False, then the function's should expect a pandas Series passed into it, and return a pandas Series. True Returns: Type Description DataFrame A pandas DataFrame with a transformed column. Source code in janitor/functions/transform_columns.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\", dest_col_name=\"dest_column_name\") def transform_column( df: pd.DataFrame, column_name: Hashable, function: Callable, dest_column_name: Optional[str] = None, elementwise: bool = True, ) -> pd.DataFrame: \"\"\"Transform the given column using the provided function. Meant to be the method-chaining equivalent of: ```python df[dest_column_name] = df[column_name].apply(function) ``` Functions can be applied in one of two ways: - **Element-wise** (default; `elementwise=True`). Then, the individual column elements will be passed in as the first argument of `function`. - **Column-wise** (`elementwise=False`). Then, `function` is expected to take in a pandas Series and return a sequence that is of identical length to the original. If `dest_column_name` is provided, then the transformation result is stored in that column. Otherwise, the transformed result is stored under the name of the original column. This method does not mutate the original DataFrame. Example: Transform a column in-place with an element-wise function. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [2, 3, 4], ... \"b\": [\"area\", \"pyjanitor\", \"grapefruit\"], ... }) >>> df a b 0 2 area 1 3 pyjanitor 2 4 grapefruit >>> df.transform_column( ... column_name=\"a\", ... function=lambda x: x**2 - 1, ... ) a b 0 3 area 1 8 pyjanitor 2 15 grapefruit Example: Transform a column in-place with an column-wise function. >>> df.transform_column( ... column_name=\"b\", ... function=lambda srs: srs.str[:5], ... elementwise=False, ... ) a b 0 2 area 1 3 pyjan 2 4 grape :param df: A pandas DataFrame. :param column_name: The column to transform. :param function: A function to apply on the column. :param dest_column_name: The column name to store the transformation result in. Defaults to None, which will result in the original column name being overwritten. If a name is provided here, then a new column with the transformed values will be created. :param elementwise: Whether to apply the function elementwise or not. If `elementwise` is True, then the function's first argument should be the data type of each datum in the column of data, and should return a transformed datum. If `elementwise` is False, then the function's should expect a pandas Series passed into it, and return a pandas Series. :returns: A pandas DataFrame with a transformed column. \"\"\" check_column(df, column_name) if dest_column_name is None: dest_column_name = column_name elif dest_column_name != column_name: # If `dest_column_name` is provided and equals `column_name`, then we # assume that the user's intent is to perform an in-place # transformation (Same behaviour as when `dest_column_name` = None). # Otherwise we throw an error if `dest_column_name` already exists in # df. check_column(df, dest_column_name, present=False) result = _get_transform_column_result( df[column_name], function, elementwise, ) return df.assign(**{dest_column_name: result}) transform_columns(df, column_names, function, suffix=None, elementwise=True, new_column_names=None) Transform multiple columns through the same transformation. This method does not mutate the original DataFrame. Super syntactic sugar! Essentially wraps transform_column and calls it repeatedly over all column names provided. User can optionally supply either a suffix to create a new set of columns with the specified suffix, or provide a dictionary mapping each original column name in column_names to its corresponding new column name. Note that all column names must be strings. Example: log10 transform a list of columns, replacing original columns. >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"col1\": [5, 10, 15], ... \"col2\": [3, 6, 9], ... \"col3\": [10, 100, 1_000], ... }) >>> df col1 col2 col3 0 5 3 10 1 10 6 100 2 15 9 1000 >>> df.transform_columns([\"col1\", \"col2\", \"col3\"], np.log10) col1 col2 col3 0 0.698970 0.477121 1.0 1 1.000000 0.778151 2.0 2 1.176091 0.954243 3.0 Example: Using the suffix parameter to create new columns. >>> df.transform_columns([\"col1\", \"col3\"], np.log10, suffix=\"_log\") col1 col2 col3 col1_log col3_log 0 5 3 10 0.698970 1.0 1 10 6 100 1.000000 2.0 2 15 9 1000 1.176091 3.0 Example: Using the new_column_names parameter to create new columns. >>> df.transform_columns( ... [\"col1\", \"col3\"], ... np.log10, ... new_column_names={\"col1\": \"transform1\"}, ... ) col1 col2 col3 transform1 0 5 3 1.0 0.698970 1 10 6 2.0 1.000000 2 15 9 3.0 1.176091 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names Union[List[str], Tuple[str]] An iterable of columns to transform. required function Callable A function to apply on each column. required suffix Optional[str] Suffix to use when creating new columns to hold the transformed values. None elementwise bool Passed on to transform_column ; whether or not to apply the transformation function elementwise (True) or columnwise (False). True new_column_names Optional[Dict[str, str]] An explicit mapping of old column names in column_names to new column names. If any column specified in column_names is not a key in this dictionary, the transformation will happen in-place for that column. None Returns: Type Description DataFrame A pandas DataFrame with transformed columns. Exceptions: Type Description ValueError If both suffix and new_column_names are specified. Source code in janitor/functions/transform_columns.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\", new_names=\"new_column_names\") def transform_columns( df: pd.DataFrame, column_names: Union[List[str], Tuple[str]], function: Callable, suffix: Optional[str] = None, elementwise: bool = True, new_column_names: Optional[Dict[str, str]] = None, ) -> pd.DataFrame: \"\"\"Transform multiple columns through the same transformation. This method does not mutate the original DataFrame. Super syntactic sugar! Essentially wraps [`transform_column`][janitor.functions.transform_columns.transform_column] and calls it repeatedly over all column names provided. User can optionally supply either a suffix to create a new set of columns with the specified suffix, or provide a dictionary mapping each original column name in `column_names` to its corresponding new column name. Note that all column names must be strings. Example: log10 transform a list of columns, replacing original columns. >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"col1\": [5, 10, 15], ... \"col2\": [3, 6, 9], ... \"col3\": [10, 100, 1_000], ... }) >>> df col1 col2 col3 0 5 3 10 1 10 6 100 2 15 9 1000 >>> df.transform_columns([\"col1\", \"col2\", \"col3\"], np.log10) col1 col2 col3 0 0.698970 0.477121 1.0 1 1.000000 0.778151 2.0 2 1.176091 0.954243 3.0 Example: Using the `suffix` parameter to create new columns. >>> df.transform_columns([\"col1\", \"col3\"], np.log10, suffix=\"_log\") col1 col2 col3 col1_log col3_log 0 5 3 10 0.698970 1.0 1 10 6 100 1.000000 2.0 2 15 9 1000 1.176091 3.0 Example: Using the `new_column_names` parameter to create new columns. >>> df.transform_columns( ... [\"col1\", \"col3\"], ... np.log10, ... new_column_names={\"col1\": \"transform1\"}, ... ) col1 col2 col3 transform1 0 5 3 1.0 0.698970 1 10 6 2.0 1.000000 2 15 9 3.0 1.176091 :param df: A pandas DataFrame. :param column_names: An iterable of columns to transform. :param function: A function to apply on each column. :param suffix: Suffix to use when creating new columns to hold the transformed values. :param elementwise: Passed on to `transform_column`; whether or not to apply the transformation function elementwise (True) or columnwise (False). :param new_column_names: An explicit mapping of old column names in `column_names` to new column names. If any column specified in `column_names` is not a key in this dictionary, the transformation will happen in-place for that column. :returns: A pandas DataFrame with transformed columns. :raises ValueError: If both `suffix` and `new_column_names` are specified. \"\"\" # noqa: E501 check(\"column_names\", column_names, [list, tuple]) check_column(df, column_names) if suffix is not None and new_column_names is not None: raise ValueError( \"Only one of `suffix` or `new_column_names` should be specified.\" ) if suffix: check(\"suffix\", suffix, [str]) dest_column_names = {col: col + suffix for col in column_names} elif new_column_names: check(\"new_column_names\", new_column_names, [dict]) dest_column_names = { col: new_column_names.get(col, col) for col in column_names } else: dest_column_names = dict(zip(column_names, column_names)) results = {} for old_col, new_col in dest_column_names.items(): if old_col != new_col: check_column(df, new_col, present=False) results[new_col] = _get_transform_column_result( df[old_col], function, elementwise=elementwise, ) return df.assign(**results) truncate_datetime Implementation of the truncate_datetime family of functions. truncate_datetime_dataframe(df, datepart) Truncate times down to a user-specified precision of year, month, day, hour, minute, or second. This method does not mutate the original DataFrame. Examples: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"xxxx\", \"yyyy\", \"zzzz\"], ... \"dt\": pd.date_range(\"2020-03-11\", periods=3, freq=\"15H\"), ... }) >>> df foo dt 0 xxxx 2020-03-11 00:00:00 1 yyyy 2020-03-11 15:00:00 2 zzzz 2020-03-12 06:00:00 >>> df.truncate_datetime_dataframe(\"day\") foo dt 0 xxxx 2020-03-11 1 yyyy 2020-03-11 2 zzzz 2020-03-12 Parameters: Name Type Description Default df DataFrame The pandas DataFrame on which to truncate datetime. required datepart str Truncation precision, YEAR, MONTH, DAY, HOUR, MINUTE, SECOND. (String is automagically capitalized) required Returns: Type Description DataFrame A pandas DataFrame with all valid datetimes truncated down to the specified precision. Exceptions: Type Description ValueError If an invalid datepart precision is passed in. Source code in janitor/functions/truncate_datetime.py @pf.register_dataframe_method def truncate_datetime_dataframe( df: pd.DataFrame, datepart: str, ) -> pd.DataFrame: \"\"\"Truncate times down to a user-specified precision of year, month, day, hour, minute, or second. This method does not mutate the original DataFrame. Examples: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"xxxx\", \"yyyy\", \"zzzz\"], ... \"dt\": pd.date_range(\"2020-03-11\", periods=3, freq=\"15H\"), ... }) >>> df foo dt 0 xxxx 2020-03-11 00:00:00 1 yyyy 2020-03-11 15:00:00 2 zzzz 2020-03-12 06:00:00 >>> df.truncate_datetime_dataframe(\"day\") foo dt 0 xxxx 2020-03-11 1 yyyy 2020-03-11 2 zzzz 2020-03-12 :param df: The pandas DataFrame on which to truncate datetime. :param datepart: Truncation precision, YEAR, MONTH, DAY, HOUR, MINUTE, SECOND. (String is automagically capitalized) :raises ValueError: If an invalid `datepart` precision is passed in. :returns: A pandas DataFrame with all valid datetimes truncated down to the specified precision. \"\"\" ACCEPTABLE_DATEPARTS = (\"YEAR\", \"MONTH\", \"DAY\", \"HOUR\", \"MINUTE\", \"SECOND\") datepart = datepart.upper() if datepart not in ACCEPTABLE_DATEPARTS: raise ValueError( \"Received an invalid `datepart` precision. \" f\"Please enter any one of {ACCEPTABLE_DATEPARTS}.\" ) dt_cols = [ column for column, coltype in df.dtypes.items() if is_datetime64_any_dtype(coltype) ] if not dt_cols: # avoid copying df if no-op is expected return df df = df.copy() # NOTE: use **kwargs of `applymap` instead of lambda when we upgrade to # pandas >= 1.3.0 df[dt_cols] = df[dt_cols].applymap( lambda x: _truncate_datetime(x, datepart=datepart), ) return df update_where Function for updating values based on other column values update_where(df, conditions, target_column_name, target_val) Add multiple conditions to update a column in the dataframe. This method does not mutate the original DataFrame. Example usage: >>> data = { ... \"a\": [1, 2, 3, 4], ... \"b\": [5, 6, 7, 8], ... \"c\": [0, 0, 0, 0], ... } >>> df = pd.DataFrame(data) >>> df a b c 0 1 5 0 1 2 6 0 2 3 7 0 3 4 8 0 >>> df.update_where( ... conditions = (df.a > 2) & (df.b < 8), ... target_column_name = 'c', ... target_val = 10 ... ) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 >>> df.update_where( # supports pandas *query* style string expressions ... conditions = \"a > 2 and b < 8\", ... target_column_name = 'c', ... target_val = 10 ... ) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required conditions Any Conditions used to update a target column and target value. required target_column_name Hashable Column to be updated. If column does not exist in DataFrame, a new column will be created; note that entries that do not get set in the new column will be null. required target_val Any Value to be updated required Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError if conditions does not return a boolean array-like data structure. .. # noqa: DAR402 Source code in janitor/functions/update_where.py @pf.register_dataframe_method @deprecated_alias(target_col=\"target_column_name\") def update_where( df: pd.DataFrame, conditions: Any, target_column_name: Hashable, target_val: Any, ) -> pd.DataFrame: \"\"\" Add multiple conditions to update a column in the dataframe. This method does not mutate the original DataFrame. Example usage: >>> data = { ... \"a\": [1, 2, 3, 4], ... \"b\": [5, 6, 7, 8], ... \"c\": [0, 0, 0, 0], ... } >>> df = pd.DataFrame(data) >>> df a b c 0 1 5 0 1 2 6 0 2 3 7 0 3 4 8 0 >>> df.update_where( ... conditions = (df.a > 2) & (df.b < 8), ... target_column_name = 'c', ... target_val = 10 ... ) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 >>> df.update_where( # supports pandas *query* style string expressions ... conditions = \"a > 2 and b < 8\", ... target_column_name = 'c', ... target_val = 10 ... ) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 :param df: The pandas DataFrame object. :param conditions: Conditions used to update a target column and target value. :param target_column_name: Column to be updated. If column does not exist in DataFrame, a new column will be created; note that entries that do not get set in the new column will be null. :param target_val: Value to be updated :returns: A pandas DataFrame. :raises ValueError: if `conditions` does not return a boolean array-like data structure. .. # noqa: DAR402 \"\"\" df = df.copy() # use query mode if a string expression is passed if isinstance(conditions, str): conditions = df.eval(conditions) if not is_bool_dtype(conditions): raise ValueError( \"\"\" Kindly ensure that `conditions` passed evaluates to a Boolean dtype. \"\"\" ) df.loc[conditions, target_column_name] = target_val return df utils Utility functions for all of the functions submodule. DropLabel dataclass Helper class for removing labels within the select syntax. label can be any of the types supported in the select , select_rows and select_columns functions. An array of integers not matching the labels is returned. New in version 0.24.0 Parameters: Name Type Description Default label Label(s) to be dropped from the index. required Returns: Type Description A dataclass. Source code in janitor/functions/utils.py @dataclass class DropLabel: \"\"\" Helper class for removing labels within the `select` syntax. `label` can be any of the types supported in the `select`, `select_rows` and `select_columns` functions. An array of integers not matching the labels is returned. !!! info \"New in version 0.24.0\" :param label: Label(s) to be dropped from the index. :returns: A dataclass. \"\"\" label: Any patterns(regex_pattern) This function converts a string into a compiled regular expression; it can be used to select columns in the index or columns_names arguments of pivot_longer function. Warning : This function is deprecated. Kindly use `re.compile` instead. Parameters: Name Type Description Default regex_pattern Union[str, Pattern] string to be converted to compiled regular expression. required Returns: Type Description Pattern A compile regular expression from provided regex_pattern . Source code in janitor/functions/utils.py def patterns(regex_pattern: Union[str, Pattern]) -> Pattern: \"\"\" This function converts a string into a compiled regular expression; it can be used to select columns in the index or columns_names arguments of `pivot_longer` function. **Warning**: This function is deprecated. Kindly use `re.compile` instead. :param regex_pattern: string to be converted to compiled regular expression. :returns: A compile regular expression from provided `regex_pattern`. \"\"\" warnings.warn( \"This function is deprecated. Kindly use `re.compile` instead.\", DeprecationWarning, stacklevel=2, ) check(\"regular expression\", regex_pattern, [str, Pattern]) return re.compile(regex_pattern) unionize_dataframe_categories(*dataframes, *, column_names=None) Given a group of dataframes which contain some categorical columns, for each categorical column present, find all the possible categories across all the dataframes which have that column. Update each dataframes' corresponding column with a new categorical object that contains the original data but has labels for all the possible categories from all dataframes. This is useful when concatenating a list of dataframes which all have the same categorical columns into one dataframe. If, for a given categorical column, all input dataframes do not have at least one instance of all the possible categories, Pandas will change the output dtype of that column from category to object , losing out on dramatic speed gains you get from the former format. Usage example for concatenation of categorical column-containing dataframes: Instead of: concatenated_df = pd.concat([df1, df2, df3], ignore_index=True) which in your case has resulted in category -> object conversion, use: unionized_dataframes = unionize_dataframe_categories(df1, df2, df2) concatenated_df = pd.concat(unionized_dataframes, ignore_index=True) Parameters: Name Type Description Default dataframes The dataframes you wish to unionize the categorical objects for. () column_names Optional[Iterable[pandas.core.dtypes.dtypes.CategoricalDtype]] If supplied, only unionize this subset of columns. None Returns: Type Description List[pandas.core.frame.DataFrame] A list of the category-unioned dataframes in the same order they were provided. Exceptions: Type Description TypeError If any of the inputs are not pandas DataFrames. Source code in janitor/functions/utils.py def unionize_dataframe_categories( *dataframes, column_names: Optional[Iterable[pd.CategoricalDtype]] = None ) -> List[pd.DataFrame]: \"\"\" Given a group of dataframes which contain some categorical columns, for each categorical column present, find all the possible categories across all the dataframes which have that column. Update each dataframes' corresponding column with a new categorical object that contains the original data but has labels for all the possible categories from all dataframes. This is useful when concatenating a list of dataframes which all have the same categorical columns into one dataframe. If, for a given categorical column, all input dataframes do not have at least one instance of all the possible categories, Pandas will change the output dtype of that column from `category` to `object`, losing out on dramatic speed gains you get from the former format. Usage example for concatenation of categorical column-containing dataframes: Instead of: ```python concatenated_df = pd.concat([df1, df2, df3], ignore_index=True) ``` which in your case has resulted in `category` -> `object` conversion, use: ```python unionized_dataframes = unionize_dataframe_categories(df1, df2, df2) concatenated_df = pd.concat(unionized_dataframes, ignore_index=True) ``` :param dataframes: The dataframes you wish to unionize the categorical objects for. :param column_names: If supplied, only unionize this subset of columns. :returns: A list of the category-unioned dataframes in the same order they were provided. :raises TypeError: If any of the inputs are not pandas DataFrames. \"\"\" if any(not isinstance(df, pd.DataFrame) for df in dataframes): raise TypeError(\"Inputs must all be dataframes.\") if column_names is None: # Find all columns across all dataframes that are categorical column_names = set() for dataframe in dataframes: column_names = column_names.union( [ column_name for column_name in dataframe.columns if isinstance( dataframe[column_name].dtype, pd.CategoricalDtype ) ] ) else: column_names = [column_names] # For each categorical column, find all possible values across the DFs category_unions = { column_name: union_categoricals( [df[column_name] for df in dataframes if column_name in df.columns] ) for column_name in column_names } # Make a shallow copy of all DFs and modify the categorical columns # such that they can encode the union of all possible categories for each. refactored_dfs = [] for df in dataframes: df = df.copy(deep=False) for column_name, categorical in category_unions.items(): if column_name in df.columns: df[column_name] = pd.Categorical( df[column_name], categories=categorical.categories ) refactored_dfs.append(df) return refactored_dfs","title":"Functions"},{"location":"api/functions/#functions","text":"","title":"Functions"},{"location":"api/functions/#janitor.functions--general-functions","text":"pyjanitor's general-purpose data cleaning functions. NOTE: Instructions for future contributors: Place the source code of the functions in a file named after the function. Place utility functions in the same file. If you use a utility function from another source file, please refactor it out to janitor.functions.utils . Import the function into this file so that it shows up in the top-level API. Sort the imports in alphabetical order. Try to group related functions together (e.g. see convert_date.py ) Never import utils.","title":"General Functions"},{"location":"api/functions/#janitor.functions.add_columns","text":"","title":"add_columns"},{"location":"api/functions/#janitor.functions.add_columns.add_column","text":"Add a column to the dataframe. Intended to be the method-chaining alternative to: df[column_name] = value Example: Add a column of constant values to the dataframe. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_column(column_name=\"c\", value=1) a b c 0 0 a 1 1 1 b 1 2 2 c 1 Example: Add a column of different values to the dataframe. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_column(column_name=\"c\", value=list(\"efg\")) a b c 0 0 a e 1 1 b f 2 2 c g Example: Add a column using an iterator. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_column(column_name=\"c\", value=range(4, 7)) a b c 0 0 a 4 1 1 b 5 2 2 c 6 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str Name of the new column. Should be a string, in order for the column name to be compatible with the Feather binary format (this is a useful thing to have). required value Union[List[Any], Tuple[Any], Any] Either a single value, or a list/tuple of values. required fill_remaining bool If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. False Returns: Type Description DataFrame A pandas DataFrame with an added column. Exceptions: Type Description ValueError If attempting to add a column that already exists. ValueError If value has more elements that number of rows in the DataFrame. ValueError If attempting to add an iterable of values with a length not equal to the number of DataFrame rows. ValueError If value has length of 0 . Source code in janitor/functions/add_columns.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\") def add_column( df: pd.DataFrame, column_name: str, value: Union[List[Any], Tuple[Any], Any], fill_remaining: bool = False, ) -> pd.DataFrame: \"\"\"Add a column to the dataframe. Intended to be the method-chaining alternative to: ```python df[column_name] = value ``` Example: Add a column of constant values to the dataframe. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_column(column_name=\"c\", value=1) a b c 0 0 a 1 1 1 b 1 2 2 c 1 Example: Add a column of different values to the dataframe. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_column(column_name=\"c\", value=list(\"efg\")) a b c 0 0 a e 1 1 b f 2 2 c g Example: Add a column using an iterator. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_column(column_name=\"c\", value=range(4, 7)) a b c 0 0 a 4 1 1 b 5 2 2 c 6 :param df: A pandas DataFrame. :param column_name: Name of the new column. Should be a string, in order for the column name to be compatible with the Feather binary format (this is a useful thing to have). :param value: Either a single value, or a list/tuple of values. :param fill_remaining: If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. :returns: A pandas DataFrame with an added column. :raises ValueError: If attempting to add a column that already exists. :raises ValueError: If `value` has more elements that number of rows in the DataFrame. :raises ValueError: If attempting to add an iterable of values with a length not equal to the number of DataFrame rows. :raises ValueError: If `value` has length of `0`. \"\"\" check(\"column_name\", column_name, [str]) if column_name in df.columns: raise ValueError( f\"Attempted to add column that already exists: \" f\"{column_name}.\" ) nrows = len(df) if hasattr(value, \"__len__\") and not isinstance( value, (str, bytes, bytearray) ): len_value = len(value) # if `value` is a list, ndarray, etc. if len_value > nrows: raise ValueError( \"`value` has more elements than number of rows \" f\"in your `DataFrame`. vals: {len_value}, \" f\"df: {nrows}\" ) if len_value != nrows and not fill_remaining: raise ValueError( \"Attempted to add iterable of values with length\" \" not equal to number of DataFrame rows\" ) if not len_value: raise ValueError( \"`value` has to be an iterable of minimum length 1\" ) elif fill_remaining: # relevant if a scalar val was passed, yet fill_remaining == True len_value = 1 value = [value] df = df.copy() if fill_remaining: times_to_loop = int(np.ceil(nrows / len_value)) fill_values = list(value) * times_to_loop df[column_name] = fill_values[:nrows] else: df[column_name] = value return df","title":"add_column()"},{"location":"api/functions/#janitor.functions.add_columns.add_columns","text":"Add multiple columns to the dataframe. This method does not mutate the original DataFrame. Method to augment add_column with ability to add multiple columns in one go. This replaces the need for multiple add_column calls. Usage is through supplying kwargs where the key is the col name and the values correspond to the values of the new DataFrame column. Values passed can be scalar or iterable (list, ndarray, etc.) Example: Inserting two more columns into a dataframe. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_columns(x=4, y=list(\"def\")) a b x y 0 0 a 4 d 1 1 b 4 e 2 2 c 4 f Parameters: Name Type Description Default df DataFrame A pandas dataframe. required fill_remaining bool If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. (Passed to add_column ) False **kwargs Column, value pairs which are looped through in add_column calls. {} Returns: Type Description DataFrame A pandas DataFrame with added columns. Source code in janitor/functions/add_columns.py @pf.register_dataframe_method def add_columns( df: pd.DataFrame, fill_remaining: bool = False, **kwargs, ) -> pd.DataFrame: \"\"\"Add multiple columns to the dataframe. This method does not mutate the original DataFrame. Method to augment [`add_column`][janitor.functions.add_columns.add_column] with ability to add multiple columns in one go. This replaces the need for multiple [`add_column`][janitor.functions.add_columns.add_column] calls. Usage is through supplying kwargs where the key is the col name and the values correspond to the values of the new DataFrame column. Values passed can be scalar or iterable (list, ndarray, etc.) Example: Inserting two more columns into a dataframe. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.add_columns(x=4, y=list(\"def\")) a b x y 0 0 a 4 d 1 1 b 4 e 2 2 c 4 f :param df: A pandas dataframe. :param fill_remaining: If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. (Passed to `add_column`) :param **kwargs: Column, value pairs which are looped through in `add_column` calls. :returns: A pandas DataFrame with added columns. \"\"\" # Note: error checking can pretty much be handled in `add_column` for col_name, values in kwargs.items(): df = df.add_column(col_name, values, fill_remaining=fill_remaining) return df","title":"add_columns()"},{"location":"api/functions/#janitor.functions.also","text":"Implementation source for chainable function also .","title":"also"},{"location":"api/functions/#janitor.functions.also.also","text":"Run a function with side effects. This function allows you to run an arbitrary function in the pyjanitor method chain. Doing so will let you do things like save the dataframe to disk midway while continuing to modify the dataframe afterwards. Example: >>> import pandas as pd >>> import janitor >>> df = ( ... pd.DataFrame({\"a\": [1, 2, 3], \"b\": list(\"abc\")}) ... .query(\"a > 1\") ... .also(lambda df: print(f\"DataFrame shape is: {df.shape}\")) ... .rename_column(old_column_name=\"a\", new_column_name=\"a_new\") ... .also(lambda df: df.to_csv(\"midpoint.csv\")) ... .also( ... lambda df: print(f\"Columns: {df.columns}\") ... ) ... ) DataFrame shape is: (2, 2) Columns: Index(['a_new', 'b'], dtype='object') Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required func Callable A function you would like to run in the method chain. It should take one DataFrame object as a parameter and have no return. If there is a return, it will be ignored. required args Optional arguments for func . () kwargs Optional keyword arguments for func . {} Returns: Type Description DataFrame The input pandas DataFrame, unmodified. Source code in janitor/functions/also.py @pf.register_dataframe_method def also(df: pd.DataFrame, func: Callable, *args, **kwargs) -> pd.DataFrame: \"\"\"Run a function with side effects. This function allows you to run an arbitrary function in the `pyjanitor` method chain. Doing so will let you do things like save the dataframe to disk midway while continuing to modify the dataframe afterwards. Example: >>> import pandas as pd >>> import janitor >>> df = ( ... pd.DataFrame({\"a\": [1, 2, 3], \"b\": list(\"abc\")}) ... .query(\"a > 1\") ... .also(lambda df: print(f\"DataFrame shape is: {df.shape}\")) ... .rename_column(old_column_name=\"a\", new_column_name=\"a_new\") ... .also(lambda df: df.to_csv(\"midpoint.csv\")) ... .also( ... lambda df: print(f\"Columns: {df.columns}\") ... ) ... ) DataFrame shape is: (2, 2) Columns: Index(['a_new', 'b'], dtype='object') :param df: A pandas DataFrame. :param func: A function you would like to run in the method chain. It should take one DataFrame object as a parameter and have no return. If there is a return, it will be ignored. :param args: Optional arguments for `func`. :param kwargs: Optional keyword arguments for `func`. :returns: The input pandas DataFrame, unmodified. \"\"\" # noqa: E501 func(df.copy(), *args, **kwargs) return df","title":"also()"},{"location":"api/functions/#janitor.functions.bin_numeric","text":"","title":"bin_numeric"},{"location":"api/functions/#janitor.functions.bin_numeric.bin_numeric","text":"Generate a new column that labels bins for a specified numeric column. This method does not mutate the original DataFrame. A wrapper around the pandas cut() function to bin data of one column, generating a new column with the results. Example: Binning a numeric column with specific bin edges. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [3, 6, 9, 12, 15]}) >>> df.bin_numeric( ... from_column_name=\"a\", to_column_name=\"a_binned\", ... bins=[0, 5, 11, 15], ... ) a a_binned 0 3 (0, 5] 1 6 (5, 11] 2 9 (5, 11] 3 12 (11, 15] 4 15 (11, 15] Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required from_column_name str The column whose data you want binned. required to_column_name str The new column to be created with the binned data. required bins Union[int, Sequence[float], pandas.core.indexes.interval.IntervalIndex] The binning strategy to be utilized. Read the pd.cut documentation for more details. 5 **kwargs Additional kwargs to pass to pd.cut , except retbins . {} Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError If retbins is passed in as a kwarg. Source code in janitor/functions/bin_numeric.py @pf.register_dataframe_method @deprecated_alias( from_column=\"from_column_name\", to_column=\"to_column_name\", num_bins=\"bins\", ) def bin_numeric( df: pd.DataFrame, from_column_name: str, to_column_name: str, bins: Optional[Union[int, ScalarSequence, pd.IntervalIndex]] = 5, **kwargs, ) -> pd.DataFrame: \"\"\" Generate a new column that labels bins for a specified numeric column. This method does not mutate the original DataFrame. A wrapper around the pandas [`cut()`][pd_cut_docs] function to bin data of one column, generating a new column with the results. [pd_cut_docs]: https://pandas.pydata.org/docs/reference/api/pandas.cut.html Example: Binning a numeric column with specific bin edges. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [3, 6, 9, 12, 15]}) >>> df.bin_numeric( ... from_column_name=\"a\", to_column_name=\"a_binned\", ... bins=[0, 5, 11, 15], ... ) a a_binned 0 3 (0, 5] 1 6 (5, 11] 2 9 (5, 11] 3 12 (11, 15] 4 15 (11, 15] :param df: A pandas DataFrame. :param from_column_name: The column whose data you want binned. :param to_column_name: The new column to be created with the binned data. :param bins: The binning strategy to be utilized. Read the `pd.cut` documentation for more details. :param **kwargs: Additional kwargs to pass to `pd.cut`, except `retbins`. :return: A pandas DataFrame. :raises ValueError: If `retbins` is passed in as a kwarg. \"\"\" if \"retbins\" in kwargs: raise ValueError(\"`retbins` is not an acceptable keyword argument.\") check(\"from_column_name\", from_column_name, [str]) check(\"to_column_name\", to_column_name, [str]) check_column(df, from_column_name) df = df.assign( **{ to_column_name: pd.cut(df[from_column_name], bins=bins, **kwargs), } ) return df","title":"bin_numeric()"},{"location":"api/functions/#janitor.functions.case_when","text":"","title":"case_when"},{"location":"api/functions/#janitor.functions.case_when.case_when","text":"Create a column based on a condition or multiple conditions. Example usage: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"a\": [0, 0, 1, 2, \"hi\"], ... \"b\": [0, 3, 4, 5, \"bye\"], ... \"c\": [6, 7, 8, 9, \"wait\"], ... } ... ) >>> df a b c 0 0 0 6 1 0 3 7 2 1 4 8 3 2 5 9 4 hi bye wait >>> df.case_when( ... ((df.a == 0) & (df.b != 0)) | (df.c == \"wait\"), df.a, ... (df.b == 0) & (df.a == 0), \"x\", ... default = df.c, ... column_name = \"value\", ... ) a b c value 0 0 0 6 x 1 0 3 7 0 2 1 4 8 8 3 2 5 9 9 4 hi bye wait hi Similar to SQL and dplyr's case_when with inspiration from pydatatable if_else function. If your scenario requires direct replacement of values, pandas' replace method or map method should be better suited and more efficient; if the conditions check if a value is within a range of values, pandas' cut or qcut should be more efficient; np.where/np.select are also performant options. This function relies on pd.Series.mask method. When multiple conditions are satisfied, the first one is used. The variable *args parameters takes arguments of the form : condition0 , value0 , condition1 , value1 , ..., default . If condition0 evaluates to True , then assign value0 to column_name , if condition1 evaluates to True , then assign value1 to column_name , and so on. If none of the conditions evaluate to True , assign default to column_name . This function can be likened to SQL's case_when : CASE WHEN condition0 THEN value0 WHEN condition1 THEN value1 --- more conditions ELSE default END AS column_name compared to python's if-elif-else : if condition0: value0 elif condition1: value1 # more elifs else: default Version Changed 0.24.0 Added default parameter. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required args Variable argument of conditions and expected values. Takes the form condition0 , value0 , condition1 , value1 , ... . condition can be a 1-D boolean array, a callable, or a string. If condition is a callable, it should evaluate to a 1-D boolean array. The array should have the same length as the DataFrame. If it is a string, it is computed on the dataframe, via df.eval , and should return a 1-D boolean array. result can be a scalar, a 1-D array, or a callable. If result is a callable, it should evaluate to a 1-D array. For a 1-D array, it should have the same length as the DataFrame. () default Any scalar, 1-D array or callable. This is the element inserted in the output when all conditions evaluate to False. If callable, it should evaluate to a 1-D array. The 1-D array should be the same length as the DataFrame. None column_name str Name of column to assign results to. A new column is created, if it does not already exist in the DataFrame. required Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError if condition/value fails to evaluate. Source code in janitor/functions/case_when.py @pf.register_dataframe_method def case_when( df: pd.DataFrame, *args, default: Any = None, column_name: str ) -> pd.DataFrame: \"\"\" Create a column based on a condition or multiple conditions. Example usage: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"a\": [0, 0, 1, 2, \"hi\"], ... \"b\": [0, 3, 4, 5, \"bye\"], ... \"c\": [6, 7, 8, 9, \"wait\"], ... } ... ) >>> df a b c 0 0 0 6 1 0 3 7 2 1 4 8 3 2 5 9 4 hi bye wait >>> df.case_when( ... ((df.a == 0) & (df.b != 0)) | (df.c == \"wait\"), df.a, ... (df.b == 0) & (df.a == 0), \"x\", ... default = df.c, ... column_name = \"value\", ... ) a b c value 0 0 0 6 x 1 0 3 7 0 2 1 4 8 8 3 2 5 9 9 4 hi bye wait hi Similar to SQL and dplyr's case_when with inspiration from `pydatatable` if_else function. If your scenario requires direct replacement of values, pandas' `replace` method or `map` method should be better suited and more efficient; if the conditions check if a value is within a range of values, pandas' `cut` or `qcut` should be more efficient; `np.where/np.select` are also performant options. This function relies on `pd.Series.mask` method. When multiple conditions are satisfied, the first one is used. The variable `*args` parameters takes arguments of the form : `condition0`, `value0`, `condition1`, `value1`, ..., `default`. If `condition0` evaluates to `True`, then assign `value0` to `column_name`, if `condition1` evaluates to `True`, then assign `value1` to `column_name`, and so on. If none of the conditions evaluate to `True`, assign `default` to `column_name`. This function can be likened to SQL's `case_when`: ```sql CASE WHEN condition0 THEN value0 WHEN condition1 THEN value1 --- more conditions ELSE default END AS column_name ``` compared to python's `if-elif-else`: ```python if condition0: value0 elif condition1: value1 # more elifs else: default ``` !!! abstract \"Version Changed\" - 0.24.0 - Added `default` parameter. :param df: A pandas DataFrame. :param args: Variable argument of conditions and expected values. Takes the form `condition0`, `value0`, `condition1`, `value1`, ... . `condition` can be a 1-D boolean array, a callable, or a string. If `condition` is a callable, it should evaluate to a 1-D boolean array. The array should have the same length as the DataFrame. If it is a string, it is computed on the dataframe, via `df.eval`, and should return a 1-D boolean array. `result` can be a scalar, a 1-D array, or a callable. If `result` is a callable, it should evaluate to a 1-D array. For a 1-D array, it should have the same length as the DataFrame. :param default: scalar, 1-D array or callable. This is the element inserted in the output when all conditions evaluate to False. If callable, it should evaluate to a 1-D array. The 1-D array should be the same length as the DataFrame. :param column_name: Name of column to assign results to. A new column is created, if it does not already exist in the DataFrame. :raises ValueError: if condition/value fails to evaluate. :returns: A pandas DataFrame. \"\"\" # Preliminary checks on the case_when function. # The bare minimum checks are done; the remaining checks # are done within `pd.Series.mask`. check(\"column_name\", column_name, [str]) len_args = len(args) if len_args < 2: raise ValueError( \"At least two arguments are required for the `args` parameter\" ) if len_args % 2: if default is None: warnings.warn( \"The last argument in the variable arguments \" \"has been assigned as the default. \" \"Note however that this will be deprecated \" \"in a future release; use an even number \" \"of boolean conditions and values, \" \"and pass the default argument to the `default` \" \"parameter instead.\", DeprecationWarning, stacklevel=2, ) *args, default = args else: raise ValueError( \"The number of conditions and values do not match. \" f\"There are {len_args - len_args//2} conditions \" f\"and {len_args//2} values.\" ) booleans = [] replacements = [] for index, value in enumerate(args): if index % 2: if callable(value): value = apply_if_callable(value, df) replacements.append(value) else: if callable(value): value = apply_if_callable(value, df) elif isinstance(value, str): value = df.eval(value) booleans.append(value) if callable(default): default = apply_if_callable(default, df) if is_scalar(default): default = pd.Series([default]).repeat(len(df)) if not hasattr(default, \"shape\"): default = pd.Series([*default]) if isinstance(default, pd.Index): arr_ndim = default.nlevels else: arr_ndim = default.ndim if arr_ndim != 1: raise ValueError( \"The argument for the `default` parameter \" \"should either be a 1-D array, a scalar, \" \"or a callable that can evaluate to a 1-D array.\" ) if not isinstance(default, pd.Series): default = pd.Series(default) default.index = df.index # actual computation # ensures value assignment is on a first come basis booleans = booleans[::-1] replacements = replacements[::-1] for index, (condition, value) in enumerate(zip(booleans, replacements)): try: default = default.mask(condition, value) # error `feedoff` idea from SO # https://stackoverflow.com/a/46091127/7175713 except Exception as error: raise ValueError( f\"condition{index} and value{index} failed to evaluate. \" f\"Original error message: {error}\" ) from error return df.assign(**{column_name: default})","title":"case_when()"},{"location":"api/functions/#janitor.functions.change_type","text":"","title":"change_type"},{"location":"api/functions/#janitor.functions.change_type.change_type","text":"Change the type of a column. This method does not mutate the original DataFrame. Exceptions that are raised can be ignored. For example, if one has a mixed dtype column that has non-integer strings and integers, and you want to coerce everything to integers, you can optionally ignore the non-integer strings and replace them with NaN or keep the original value. Intended to be the method-chaining alternative to: df[col] = df[col].astype(dtype) Example: Change the type of a column. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": range(3), \"col2\": [\"m\", 5, True]}) >>> df col1 col2 0 0 m 1 1 5 2 2 True >>> df.change_type( ... \"col1\", dtype=str, ... ).change_type( ... \"col2\", dtype=float, ignore_exception=\"fillna\", ... ) col1 col2 0 0 NaN 1 1 5.0 2 2 1.0 Example: Change the type of multiple columns. Change the type of all columns, please use DataFrame.astype instead. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": range(3), \"col2\": [\"m\", 5, True]}) >>> df.change_type(['col1', 'col2'], str) col1 col2 0 0 m 1 1 5 2 2 True Parameters: Name Type Description Default df pd.DataFrame A pandas DataFrame. required column_name Hashable | list[Hashable] | pd.Index The column(s) in the dataframe. required dtype type The datatype to convert to. Should be one of the standard Python types, or a numpy datatype. required ignore_exception bool one of {False, \"fillna\", \"keep_values\"} . False Returns: Type Description pd.DataFrame A pandas DataFrame with changed column types. Exceptions: Type Description ValueError If unknown option provided for ignore_exception . Source code in janitor/functions/change_type.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def change_type( df: pd.DataFrame, column_name: Hashable | list[Hashable] | pd.Index, dtype: type, ignore_exception: bool = False, ) -> pd.DataFrame: \"\"\"Change the type of a column. This method does not mutate the original DataFrame. Exceptions that are raised can be ignored. For example, if one has a mixed dtype column that has non-integer strings and integers, and you want to coerce everything to integers, you can optionally ignore the non-integer strings and replace them with `NaN` or keep the original value. Intended to be the method-chaining alternative to: ```python df[col] = df[col].astype(dtype) ``` Example: Change the type of a column. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": range(3), \"col2\": [\"m\", 5, True]}) >>> df col1 col2 0 0 m 1 1 5 2 2 True >>> df.change_type( ... \"col1\", dtype=str, ... ).change_type( ... \"col2\", dtype=float, ignore_exception=\"fillna\", ... ) col1 col2 0 0 NaN 1 1 5.0 2 2 1.0 Example: Change the type of multiple columns. Change the type of all columns, please use `DataFrame.astype` instead. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": range(3), \"col2\": [\"m\", 5, True]}) >>> df.change_type(['col1', 'col2'], str) col1 col2 0 0 m 1 1 5 2 2 True :param df: A pandas DataFrame. :param column_name: The column(s) in the dataframe. :param dtype: The datatype to convert to. Should be one of the standard Python types, or a numpy datatype. :param ignore_exception: one of `{False, \"fillna\", \"keep_values\"}`. :returns: A pandas DataFrame with changed column types. :raises ValueError: If unknown option provided for `ignore_exception`. \"\"\" df = df.copy() # avoid mutating the original DataFrame if not ignore_exception: df[column_name] = df[column_name].astype(dtype) elif ignore_exception == \"keep_values\": df[column_name] = df[column_name].astype(dtype, errors=\"ignore\") elif ignore_exception == \"fillna\": if isinstance(column_name, Hashable): column_name = [column_name] df[column_name] = df[column_name].applymap(_convert, dtype=dtype) else: raise ValueError(\"Unknown option for ignore_exception\") return df","title":"change_type()"},{"location":"api/functions/#janitor.functions.clean_names","text":"Functions for cleaning columns names.","title":"clean_names"},{"location":"api/functions/#janitor.functions.clean_names.clean_names","text":"Clean column names. Takes all column names, converts them to lowercase, then replaces all spaces with underscores. By default, column names are converted to string types. This can be switched off by passing in enforce_string=False . This method does not mutate the original DataFrame. Example usage: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"Aloha\": range(3), ... \"Bell Chart\": range(3), ... \"Animals@#$%^\": range(3) ... } ... ) >>> df Aloha Bell Chart Animals@#$%^ 0 0 0 0 1 1 1 1 2 2 2 2 >>> df.clean_names() aloha bell_chart animals@#$%^ 0 0 0 0 1 1 1 1 2 2 2 2 >>> df.clean_names(remove_special=True) aloha bell_chart animals 0 0 0 0 1 1 1 1 2 2 2 2 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required strip_underscores Union[str, bool] (optional) Removes the outer underscores from all column names. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True. None case_type str (optional) Whether to make columns lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase. 'lower' remove_special bool (optional) Remove special characters from columns. Only letters, numbers and underscores are preserved. False strip_accents bool Whether or not to remove accents from columns names. True preserve_original_columns bool (optional) Preserve original names. This is later retrievable using df.original_columns . True enforce_string bool Whether or not to convert all column names to string type. Defaults to True, but can be turned off. Columns with >1 levels will not be converted by default. True truncate_limit int (optional) Truncates formatted column names to the specified length. Default None does not truncate. None Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/clean_names.py @pf.register_dataframe_method def clean_names( df: pd.DataFrame, strip_underscores: Optional[Union[str, bool]] = None, case_type: str = \"lower\", remove_special: bool = False, strip_accents: bool = True, preserve_original_columns: bool = True, enforce_string: bool = True, truncate_limit: int = None, ) -> pd.DataFrame: \"\"\" Clean column names. Takes all column names, converts them to lowercase, then replaces all spaces with underscores. By default, column names are converted to string types. This can be switched off by passing in `enforce_string=False`. This method does not mutate the original DataFrame. Example usage: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"Aloha\": range(3), ... \"Bell Chart\": range(3), ... \"Animals@#$%^\": range(3) ... } ... ) >>> df Aloha Bell Chart Animals@#$%^ 0 0 0 0 1 1 1 1 2 2 2 2 >>> df.clean_names() aloha bell_chart animals@#$%^ 0 0 0 0 1 1 1 1 2 2 2 2 >>> df.clean_names(remove_special=True) aloha bell_chart animals 0 0 0 0 1 1 1 1 2 2 2 2 :param df: The pandas DataFrame object. :param strip_underscores: (optional) Removes the outer underscores from all column names. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True. :param case_type: (optional) Whether to make columns lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase. :param remove_special: (optional) Remove special characters from columns. Only letters, numbers and underscores are preserved. :param strip_accents: Whether or not to remove accents from columns names. :param preserve_original_columns: (optional) Preserve original names. This is later retrievable using `df.original_columns`. :param enforce_string: Whether or not to convert all column names to string type. Defaults to True, but can be turned off. Columns with >1 levels will not be converted by default. :param truncate_limit: (optional) Truncates formatted column names to the specified length. Default None does not truncate. :returns: A pandas DataFrame. \"\"\" original_column_names = list(df.columns) if enforce_string: df = df.rename(columns=str) df = df.rename(columns=lambda x: _change_case(x, case_type)) df = df.rename(columns=_normalize_1) if remove_special: df = df.rename(columns=_remove_special) if strip_accents: df = df.rename(columns=_strip_accents) df = df.rename(columns=lambda x: re.sub(\"_+\", \"_\", x)) # noqa: PD005 df = _strip_underscores(df, strip_underscores) df = df.rename(columns=lambda x: x[:truncate_limit]) # Store the original column names, if enabled by user if preserve_original_columns: df.__dict__[\"original_columns\"] = original_column_names return df","title":"clean_names()"},{"location":"api/functions/#janitor.functions.coalesce","text":"Function for performing coalesce.","title":"coalesce"},{"location":"api/functions/#janitor.functions.coalesce.coalesce","text":"Coalesce two or more columns of data in order of column names provided. Given the variable arguments of column names, coalesce finds and returns the first non-missing value from these columns, for every row in the input dataframe. If all the column values are null for a particular row, then the default_value will be filled in. If target_column_name is not provided, then the first column is coalesced. This method does not mutate the original DataFrame. Example: Use coalesce with 3 columns, \"a\", \"b\" and \"c\". >>> import pandas as pd >>> import numpy as np >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [np.nan, 1, np.nan], ... \"b\": [2, 3, np.nan], ... \"c\": [4, np.nan, np.nan], ... }) >>> df.coalesce(\"a\", \"b\", \"c\") a b c 0 2.0 2.0 4.0 1 1.0 3.0 NaN 2 NaN NaN NaN Example: Provide a target_column_name. >>> df.coalesce(\"a\", \"b\", \"c\", target_column_name=\"new_col\") a b c new_col 0 NaN 2.0 4.0 2.0 1 1.0 3.0 NaN 1.0 2 NaN NaN NaN NaN Example: Provide a default value. >>> import pandas as pd >>> import numpy as np >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [1, np.nan, np.nan], ... \"b\": [2, 3, np.nan], ... }) >>> df.coalesce( ... \"a\", \"b\", ... target_column_name=\"new_col\", ... default_value=-1, ... ) a b new_col 0 1.0 2.0 1.0 1 NaN 3.0 3.0 2 NaN NaN -1.0 This is more syntactic diabetes! For R users, this should look familiar to dplyr 's coalesce function; for Python users, the interface should be more intuitive than the pandas.Series.combine_first method. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names A list of column names. () target_column_name Optional[str] The new column name after combining. If None , then the first column in column_names is updated, with the Null values replaced. None default_value Union[int, float, str] A scalar to replace any remaining nulls after coalescing. None Returns: Type Description DataFrame A pandas DataFrame with coalesced columns. Exceptions: Type Description ValueError if length of column_names is less than 2. Source code in janitor/functions/coalesce.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\", new_column_name=\"target_column_name\") def coalesce( df: pd.DataFrame, *column_names, target_column_name: Optional[str] = None, default_value: Optional[Union[int, float, str]] = None, ) -> pd.DataFrame: \"\"\"Coalesce two or more columns of data in order of column names provided. Given the variable arguments of column names, `coalesce` finds and returns the first non-missing value from these columns, for every row in the input dataframe. If all the column values are null for a particular row, then the `default_value` will be filled in. If `target_column_name` is not provided, then the first column is coalesced. This method does not mutate the original DataFrame. Example: Use `coalesce` with 3 columns, \"a\", \"b\" and \"c\". >>> import pandas as pd >>> import numpy as np >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [np.nan, 1, np.nan], ... \"b\": [2, 3, np.nan], ... \"c\": [4, np.nan, np.nan], ... }) >>> df.coalesce(\"a\", \"b\", \"c\") a b c 0 2.0 2.0 4.0 1 1.0 3.0 NaN 2 NaN NaN NaN Example: Provide a target_column_name. >>> df.coalesce(\"a\", \"b\", \"c\", target_column_name=\"new_col\") a b c new_col 0 NaN 2.0 4.0 2.0 1 1.0 3.0 NaN 1.0 2 NaN NaN NaN NaN Example: Provide a default value. >>> import pandas as pd >>> import numpy as np >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [1, np.nan, np.nan], ... \"b\": [2, 3, np.nan], ... }) >>> df.coalesce( ... \"a\", \"b\", ... target_column_name=\"new_col\", ... default_value=-1, ... ) a b new_col 0 1.0 2.0 1.0 1 NaN 3.0 3.0 2 NaN NaN -1.0 This is more syntactic diabetes! For R users, this should look familiar to `dplyr`'s `coalesce` function; for Python users, the interface should be more intuitive than the `pandas.Series.combine_first` method. :param df: A pandas DataFrame. :param column_names: A list of column names. :param target_column_name: The new column name after combining. If `None`, then the first column in `column_names` is updated, with the Null values replaced. :param default_value: A scalar to replace any remaining nulls after coalescing. :returns: A pandas DataFrame with coalesced columns. :raises ValueError: if length of `column_names` is less than 2. \"\"\" if not column_names: return df if len(column_names) < 2: raise ValueError( \"The number of columns to coalesce should be a minimum of 2.\" ) indices = _select_index([*column_names], df, axis=\"columns\") column_names = df.columns[indices] if target_column_name: check(\"target_column_name\", target_column_name, [str]) if default_value: check(\"default_value\", default_value, [int, float, str]) if target_column_name is None: target_column_name = column_names[0] outcome = df.loc(axis=1)[column_names].bfill(axis=\"columns\").iloc[:, 0] if outcome.hasnans and (default_value is not None): outcome = outcome.fillna(default_value) return df.assign(**{target_column_name: outcome})","title":"coalesce()"},{"location":"api/functions/#janitor.functions.collapse_levels","text":"Implementation of the collapse_levels function.","title":"collapse_levels"},{"location":"api/functions/#janitor.functions.collapse_levels.collapse_levels","text":"Flatten multi-level column dataframe to a single level. This method mutates the original DataFrame. Given a DataFrame containing multi-level columns, flatten to single-level by string-joining the column labels in each level. After a groupby / aggregate operation where .agg() is passed a list of multiple aggregation functions, a multi-level DataFrame is returned with the name of the function applied in the second level. It is sometimes convenient for later indexing to flatten out this multi-level configuration back into a single level. This function does this through a simple string-joining of all the names across different levels in a single column. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"class\": [\"bird\", \"bird\", \"bird\", \"mammal\", \"mammal\"], ... \"max_speed\": [389, 389, 24, 80, 21], ... \"type\": [\"falcon\", \"falcon\", \"parrot\", \"Lion\", \"Monkey\"], ... }) >>> df class max_speed type 0 bird 389 falcon 1 bird 389 falcon 2 bird 24 parrot 3 mammal 80 Lion 4 mammal 21 Monkey >>> grouped_df = df.groupby(\"class\").agg([\"mean\", \"median\"]) >>> grouped_df # doctest: +NORMALIZE_WHITESPACE max_speed mean median class bird 267.333333 389.0 mammal 50.500000 50.5 >>> grouped_df.collapse_levels(sep=\"_\") # doctest: +NORMALIZE_WHITESPACE max_speed_mean max_speed_median class bird 267.333333 389.0 mammal 50.500000 50.5 Before applying .collapse_levels , the .agg operation returns a multi-level column DataFrame whose columns are (level 1, level 2) : [(\"max_speed\", \"mean\"), (\"max_speed\", \"median\")] .collapse_levels then flattens the column MultiIndex into a single level index with names: [\"max_speed_mean\", \"max_speed_median\"] Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required sep str String separator used to join the column level names. '_' Returns: Type Description DataFrame A pandas DataFrame with single-level column index. Source code in janitor/functions/collapse_levels.py @pf.register_dataframe_method def collapse_levels(df: pd.DataFrame, sep: str = \"_\") -> pd.DataFrame: \"\"\"Flatten multi-level column dataframe to a single level. This method mutates the original DataFrame. Given a DataFrame containing multi-level columns, flatten to single-level by string-joining the column labels in each level. After a `groupby` / `aggregate` operation where `.agg()` is passed a list of multiple aggregation functions, a multi-level DataFrame is returned with the name of the function applied in the second level. It is sometimes convenient for later indexing to flatten out this multi-level configuration back into a single level. This function does this through a simple string-joining of all the names across different levels in a single column. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"class\": [\"bird\", \"bird\", \"bird\", \"mammal\", \"mammal\"], ... \"max_speed\": [389, 389, 24, 80, 21], ... \"type\": [\"falcon\", \"falcon\", \"parrot\", \"Lion\", \"Monkey\"], ... }) >>> df class max_speed type 0 bird 389 falcon 1 bird 389 falcon 2 bird 24 parrot 3 mammal 80 Lion 4 mammal 21 Monkey >>> grouped_df = df.groupby(\"class\").agg([\"mean\", \"median\"]) >>> grouped_df # doctest: +NORMALIZE_WHITESPACE max_speed mean median class bird 267.333333 389.0 mammal 50.500000 50.5 >>> grouped_df.collapse_levels(sep=\"_\") # doctest: +NORMALIZE_WHITESPACE max_speed_mean max_speed_median class bird 267.333333 389.0 mammal 50.500000 50.5 Before applying `.collapse_levels`, the `.agg` operation returns a multi-level column DataFrame whose columns are `(level 1, level 2)`: [(\"max_speed\", \"mean\"), (\"max_speed\", \"median\")] `.collapse_levels` then flattens the column MultiIndex into a single level index with names: [\"max_speed_mean\", \"max_speed_median\"] :param df: A pandas DataFrame. :param sep: String separator used to join the column level names. :returns: A pandas DataFrame with single-level column index. \"\"\" # noqa: E501 check(\"sep\", sep, [str]) # if already single-level, just return the DataFrame if not isinstance(df.columns, pd.MultiIndex): return df df.columns = [ sep.join(str(el) for el in tup if str(el) != \"\") for tup in df # noqa: PD011 ] return df","title":"collapse_levels()"},{"location":"api/functions/#janitor.functions.complete","text":"","title":"complete"},{"location":"api/functions/#janitor.functions.complete.complete","text":"It is modeled after tidyr's complete function, and is a wrapper around expand_grid , pd.merge and pd.fillna . In a way, it is the inverse of pd.dropna , as it exposes implicitly missing rows. Combinations of column names or a list/tuple of column names, or even a dictionary of column names and new values are possible. MultiIndex columns are not supported. Example: >>> import pandas as pd >>> import janitor >>> import numpy as np >>> df = pd.DataFrame( ... { ... \"Year\": [1999, 2000, 2004, 1999, 2004], ... \"Taxon\": [ ... \"Saccharina\", ... \"Saccharina\", ... \"Saccharina\", ... \"Agarum\", ... \"Agarum\", ... ], ... \"Abundance\": [4, 5, 2, 1, 8], ... } ... ) >>> df Year Taxon Abundance 0 1999 Saccharina 4 1 2000 Saccharina 5 2 2004 Saccharina 2 3 1999 Agarum 1 4 2004 Agarum 8 Expose missing pairings of Year and Taxon : >>> df.complete(\"Year\", \"Taxon\", sort=True) Year Taxon Abundance 0 1999 Agarum 1.0 1 1999 Saccharina 4.0 2 2000 Agarum NaN 3 2000 Saccharina 5.0 4 2004 Agarum 8.0 5 2004 Saccharina 2.0 Expose missing years from 1999 to 2004 : >>> df.complete( ... {\"Year\": range(df.Year.min(), df.Year.max() + 1)}, ... \"Taxon\", ... sort=True ... ) Year Taxon Abundance 0 1999 Agarum 1.0 1 1999 Saccharina 4.0 2 2000 Agarum NaN 3 2000 Saccharina 5.0 4 2001 Agarum NaN 5 2001 Saccharina NaN 6 2002 Agarum NaN 7 2002 Saccharina NaN 8 2003 Agarum NaN 9 2003 Saccharina NaN 10 2004 Agarum 8.0 11 2004 Saccharina 2.0 Fill missing values: >>> df = pd.DataFrame( ... dict( ... group=(1, 2, 1, 2), ... item_id=(1, 2, 2, 3), ... item_name=(\"a\", \"a\", \"b\", \"b\"), ... value1=(1, np.nan, 3, 4), ... value2=range(4, 8), ... ) ... ) >>> df group item_id item_name value1 value2 0 1 1 a 1.0 4 1 2 2 a NaN 5 2 1 2 b 3.0 6 3 2 3 b 4.0 7 >>> df.complete( ... \"group\", ... (\"item_id\", \"item_name\"), ... fill_value={\"value1\": 0, \"value2\": 99}, ... sort=True ... ) group item_id item_name value1 value2 0 1 1 a 1 4 1 1 2 a 0 99 2 1 2 b 3 6 3 1 3 b 0 99 4 2 1 a 0 99 5 2 2 a 0 5 6 2 2 b 0 99 7 2 3 b 4 7 Limit the fill to only implicit missing values by setting explicit to False : >>> df.complete( ... \"group\", ... (\"item_id\", \"item_name\"), ... fill_value={\"value1\": 0, \"value2\": 99}, ... explicit=False, ... sort=True ... ) group item_id item_name value1 value2 0 1 1 a 1.0 4.0 1 1 2 a 0.0 99.0 2 1 2 b 3.0 6.0 3 1 3 b 0.0 99.0 4 2 1 a 0.0 99.0 5 2 2 a NaN 5.0 6 2 2 b 0.0 99.0 7 2 3 b 4.0 7.0 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required *columns This refers to the columns to be completed. It could be column labels (string type), a list/tuple of column labels, or a dictionary that pairs column labels with new values. () sort bool Sort DataFrame based on *columns. Default is False . False by Union[list, str] label or list of labels to group by. The explicit missing rows are returned per group. None fill_value Union[Dict, Any] Scalar value to use instead of NaN for missing combinations. A dictionary, mapping columns names to a scalar value is also accepted. None explicit bool Determines if only implicitly missing values should be filled ( False ), or all nulls existing in the dataframe ( True ). Default is True . explicit is applicable only if fill_value is not None . True Returns: Type Description DataFrame A pandas DataFrame with explicit missing rows, if any. Source code in janitor/functions/complete.py @pf.register_dataframe_method def complete( df: pd.DataFrame, *columns, sort: bool = False, by: Optional[Union[list, str]] = None, fill_value: Optional[Union[Dict, Any]] = None, explicit: bool = True, ) -> pd.DataFrame: \"\"\" It is modeled after tidyr's `complete` function, and is a wrapper around [`expand_grid`][janitor.functions.expand_grid.expand_grid], `pd.merge` and `pd.fillna`. In a way, it is the inverse of `pd.dropna`, as it exposes implicitly missing rows. Combinations of column names or a list/tuple of column names, or even a dictionary of column names and new values are possible. MultiIndex columns are not supported. Example: >>> import pandas as pd >>> import janitor >>> import numpy as np >>> df = pd.DataFrame( ... { ... \"Year\": [1999, 2000, 2004, 1999, 2004], ... \"Taxon\": [ ... \"Saccharina\", ... \"Saccharina\", ... \"Saccharina\", ... \"Agarum\", ... \"Agarum\", ... ], ... \"Abundance\": [4, 5, 2, 1, 8], ... } ... ) >>> df Year Taxon Abundance 0 1999 Saccharina 4 1 2000 Saccharina 5 2 2004 Saccharina 2 3 1999 Agarum 1 4 2004 Agarum 8 Expose missing pairings of `Year` and `Taxon`: >>> df.complete(\"Year\", \"Taxon\", sort=True) Year Taxon Abundance 0 1999 Agarum 1.0 1 1999 Saccharina 4.0 2 2000 Agarum NaN 3 2000 Saccharina 5.0 4 2004 Agarum 8.0 5 2004 Saccharina 2.0 Expose missing years from 1999 to 2004 : >>> df.complete( ... {\"Year\": range(df.Year.min(), df.Year.max() + 1)}, ... \"Taxon\", ... sort=True ... ) Year Taxon Abundance 0 1999 Agarum 1.0 1 1999 Saccharina 4.0 2 2000 Agarum NaN 3 2000 Saccharina 5.0 4 2001 Agarum NaN 5 2001 Saccharina NaN 6 2002 Agarum NaN 7 2002 Saccharina NaN 8 2003 Agarum NaN 9 2003 Saccharina NaN 10 2004 Agarum 8.0 11 2004 Saccharina 2.0 Fill missing values: >>> df = pd.DataFrame( ... dict( ... group=(1, 2, 1, 2), ... item_id=(1, 2, 2, 3), ... item_name=(\"a\", \"a\", \"b\", \"b\"), ... value1=(1, np.nan, 3, 4), ... value2=range(4, 8), ... ) ... ) >>> df group item_id item_name value1 value2 0 1 1 a 1.0 4 1 2 2 a NaN 5 2 1 2 b 3.0 6 3 2 3 b 4.0 7 >>> df.complete( ... \"group\", ... (\"item_id\", \"item_name\"), ... fill_value={\"value1\": 0, \"value2\": 99}, ... sort=True ... ) group item_id item_name value1 value2 0 1 1 a 1 4 1 1 2 a 0 99 2 1 2 b 3 6 3 1 3 b 0 99 4 2 1 a 0 99 5 2 2 a 0 5 6 2 2 b 0 99 7 2 3 b 4 7 Limit the fill to only implicit missing values by setting explicit to `False`: >>> df.complete( ... \"group\", ... (\"item_id\", \"item_name\"), ... fill_value={\"value1\": 0, \"value2\": 99}, ... explicit=False, ... sort=True ... ) group item_id item_name value1 value2 0 1 1 a 1.0 4.0 1 1 2 a 0.0 99.0 2 1 2 b 3.0 6.0 3 1 3 b 0.0 99.0 4 2 1 a 0.0 99.0 5 2 2 a NaN 5.0 6 2 2 b 0.0 99.0 7 2 3 b 4.0 7.0 :param df: A pandas DataFrame. :param *columns: This refers to the columns to be completed. It could be column labels (string type), a list/tuple of column labels, or a dictionary that pairs column labels with new values. :param sort: Sort DataFrame based on *columns. Default is `False`. :param by: label or list of labels to group by. The explicit missing rows are returned per group. :param fill_value: Scalar value to use instead of NaN for missing combinations. A dictionary, mapping columns names to a scalar value is also accepted. :param explicit: Determines if only implicitly missing values should be filled (`False`), or all nulls existing in the dataframe (`True`). Default is `True`. `explicit` is applicable only if `fill_value` is not `None`. :returns: A pandas DataFrame with explicit missing rows, if any. \"\"\" if not columns: return df df = df.copy() return _computations_complete(df, columns, sort, by, fill_value, explicit)","title":"complete()"},{"location":"api/functions/#janitor.functions.concatenate_columns","text":"","title":"concatenate_columns"},{"location":"api/functions/#janitor.functions.concatenate_columns.concatenate_columns","text":"Concatenates the set of columns into a single column. Used to quickly generate an index based on a group of columns. This method mutates the original DataFrame. Example: Concatenate two columns row-wise. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [1, 3, 5], \"b\": list(\"xyz\")}) >>> df a b 0 1 x 1 3 y 2 5 z >>> df.concatenate_columns( ... column_names=[\"a\", \"b\"], new_column_name=\"m\", ... ) a b m 0 1 x 1-x 1 3 y 3-y 2 5 z 5-z Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names List[Hashable] A list of columns to concatenate together. required new_column_name Hashable The name of the new column. required sep str The separator between each column's data. '-' ignore_empty bool Ignore null values if exists. True Returns: Type Description DataFrame A pandas DataFrame with concatenated columns. Exceptions: Type Description JanitorError If at least two columns are not provided within column_names . Source code in janitor/functions/concatenate_columns.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def concatenate_columns( df: pd.DataFrame, column_names: List[Hashable], new_column_name: Hashable, sep: str = \"-\", ignore_empty: bool = True, ) -> pd.DataFrame: \"\"\"Concatenates the set of columns into a single column. Used to quickly generate an index based on a group of columns. This method mutates the original DataFrame. Example: Concatenate two columns row-wise. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [1, 3, 5], \"b\": list(\"xyz\")}) >>> df a b 0 1 x 1 3 y 2 5 z >>> df.concatenate_columns( ... column_names=[\"a\", \"b\"], new_column_name=\"m\", ... ) a b m 0 1 x 1-x 1 3 y 3-y 2 5 z 5-z :param df: A pandas DataFrame. :param column_names: A list of columns to concatenate together. :param new_column_name: The name of the new column. :param sep: The separator between each column's data. :param ignore_empty: Ignore null values if exists. :returns: A pandas DataFrame with concatenated columns. :raises JanitorError: If at least two columns are not provided within `column_names`. \"\"\" if len(column_names) < 2: raise JanitorError(\"At least two columns must be specified\") df[new_column_name] = ( df[column_names].astype(str).fillna(\"\").agg(sep.join, axis=1) ) if ignore_empty: def remove_empty_string(x): \"\"\"Ignore empty/null string values from the concatenated output.\"\"\" return sep.join(x for x in x.split(sep) if x) df[new_column_name] = df[new_column_name].transform( remove_empty_string ) return df","title":"concatenate_columns()"},{"location":"api/functions/#janitor.functions.conditional_join","text":"","title":"conditional_join"},{"location":"api/functions/#janitor.functions.conditional_join.conditional_join","text":"The conditional_join function operates similarly to pd.merge , but allows joins on inequality operators, or a combination of equi and non-equi joins. Joins solely on equality are not supported. If the join is solely on equality, pd.merge function covers that; if you are interested in nearest joins, or rolling joins, then pd.merge_asof covers that. There is also pandas' IntervalIndex, which is efficient for range joins, especially if the intervals do not overlap. Column selection in df_columns and right_columns is possible using the select_columns syntax. For strictly non-equi joins, involving either > , < , >= , <= operators, performance could be improved by setting use_numba to True . This assumes that numba is installed. To preserve row order, set sort_by_appearance to True . This function returns rows, if any, where values from df meet the condition(s) for values from right . The conditions are passed in as a variable argument of tuples, where the tuple is of the form (left_on, right_on, op) ; left_on is the column label from df , right_on is the column label from right , while op is the operator. For multiple conditions, the and( & ) operator is used to combine the results of the individual conditions. The operator can be any of == , != , <= , < , >= , > . The join is done only on the columns. MultiIndex columns are not supported. For non-equi joins, only numeric and date columns are supported. Only inner , left , and right joins are supported. If the columns from df and right have nothing in common, a single index column is returned; else, a MultiIndex column is returned. Example: >>> import pandas as pd >>> import janitor >>> df1 = pd.DataFrame({\"value_1\": [2, 5, 7, 1, 3, 4]}) >>> df2 = pd.DataFrame({\"value_2A\": [0, 3, 7, 12, 0, 2, 3, 1], ... \"value_2B\": [1, 5, 9, 15, 1, 4, 6, 3], ... }) >>> df1 value_1 0 2 1 5 2 7 3 1 4 3 5 4 >>> df2 value_2A value_2B 0 0 1 1 3 5 2 7 9 3 12 15 4 0 1 5 2 4 6 3 6 7 1 3 >>> df1.conditional_join( ... df2, ... (\"value_1\", \"value_2A\", \">\"), ... (\"value_1\", \"value_2B\", \"<\") ... ) value_1 value_2A value_2B 0 2 1 3 1 5 3 6 2 3 2 4 3 4 3 5 4 4 3 6 Version Changed 0.24.0 Added df_columns , right_columns , keep and use_numba parameters. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required right Union[pandas.core.frame.DataFrame, pandas.core.series.Series] Named Series or DataFrame to join to. required conditions Variable argument of tuple(s) of the form (left_on, right_on, op) , where left_on is the column label from df , right_on is the column label from right , while op is the operator. The operator can be any of == , != , <= , < , >= , > . For multiple conditions, the and( & ) operator is used to combine the results of the individual conditions. () how Literal['inner', 'left', 'right'] Indicates the type of join to be performed. It can be one of inner , left , right . Full outer join is not supported. Defaults to inner . 'inner' sort_by_appearance bool Default is False . This is useful for scenarios where the user wants the original order maintained. If True and how = left , the row order from the left dataframe is preserved; if True and how = right , the row order from the right dataframe is preserved. False df_columns Optional[Any] Columns to select from df . It can be a single column or a list of columns. It is also possible to rename the output columns via a dictionary. None right_columns Optional[Any] Columns to select from right . It can be a single column or a list of columns. It is also possible to rename the output columns via a dictionary. None keep Literal['first', 'last', 'all'] Choose whether to return the first match, last match or all matches. Default is all . 'all' use_numba bool Use numba, if installed, to accelerate the computation. Applicable only to strictly non-equi joins. Default is False . False Returns: Type Description DataFrame A pandas DataFrame of the two merged Pandas objects. Source code in janitor/functions/conditional_join.py @pf.register_dataframe_method def conditional_join( df: pd.DataFrame, right: Union[pd.DataFrame, pd.Series], *conditions, how: Literal[\"inner\", \"left\", \"right\"] = \"inner\", sort_by_appearance: bool = False, df_columns: Optional[Any] = None, right_columns: Optional[Any] = None, keep: Literal[\"first\", \"last\", \"all\"] = \"all\", use_numba: bool = False, ) -> pd.DataFrame: \"\"\" The conditional_join function operates similarly to `pd.merge`, but allows joins on inequality operators, or a combination of equi and non-equi joins. Joins solely on equality are not supported. If the join is solely on equality, `pd.merge` function covers that; if you are interested in nearest joins, or rolling joins, then `pd.merge_asof` covers that. There is also pandas' IntervalIndex, which is efficient for range joins, especially if the intervals do not overlap. Column selection in `df_columns` and `right_columns` is possible using the [`select_columns`][janitor.functions.select.select_columns] syntax. For strictly non-equi joins, involving either `>`, `<`, `>=`, `<=` operators, performance could be improved by setting `use_numba` to `True`. This assumes that `numba` is installed. To preserve row order, set `sort_by_appearance` to `True`. This function returns rows, if any, where values from `df` meet the condition(s) for values from `right`. The conditions are passed in as a variable argument of tuples, where the tuple is of the form `(left_on, right_on, op)`; `left_on` is the column label from `df`, `right_on` is the column label from `right`, while `op` is the operator. For multiple conditions, the and(`&`) operator is used to combine the results of the individual conditions. The operator can be any of `==`, `!=`, `<=`, `<`, `>=`, `>`. The join is done only on the columns. MultiIndex columns are not supported. For non-equi joins, only numeric and date columns are supported. Only `inner`, `left`, and `right` joins are supported. If the columns from `df` and `right` have nothing in common, a single index column is returned; else, a MultiIndex column is returned. Example: >>> import pandas as pd >>> import janitor >>> df1 = pd.DataFrame({\"value_1\": [2, 5, 7, 1, 3, 4]}) >>> df2 = pd.DataFrame({\"value_2A\": [0, 3, 7, 12, 0, 2, 3, 1], ... \"value_2B\": [1, 5, 9, 15, 1, 4, 6, 3], ... }) >>> df1 value_1 0 2 1 5 2 7 3 1 4 3 5 4 >>> df2 value_2A value_2B 0 0 1 1 3 5 2 7 9 3 12 15 4 0 1 5 2 4 6 3 6 7 1 3 >>> df1.conditional_join( ... df2, ... (\"value_1\", \"value_2A\", \">\"), ... (\"value_1\", \"value_2B\", \"<\") ... ) value_1 value_2A value_2B 0 2 1 3 1 5 3 6 2 3 2 4 3 4 3 5 4 4 3 6 !!! abstract \"Version Changed\" - 0.24.0 - Added `df_columns`, `right_columns`, `keep` and `use_numba` parameters. :param df: A pandas DataFrame. :param right: Named Series or DataFrame to join to. :param conditions: Variable argument of tuple(s) of the form `(left_on, right_on, op)`, where `left_on` is the column label from `df`, `right_on` is the column label from `right`, while `op` is the operator. The operator can be any of `==`, `!=`, `<=`, `<`, `>=`, `>`. For multiple conditions, the and(`&`) operator is used to combine the results of the individual conditions. :param how: Indicates the type of join to be performed. It can be one of `inner`, `left`, `right`. Full outer join is not supported. Defaults to `inner`. :param sort_by_appearance: Default is `False`. This is useful for scenarios where the user wants the original order maintained. If `True` and `how = left`, the row order from the left dataframe is preserved; if `True` and `how = right`, the row order from the right dataframe is preserved. :param df_columns: Columns to select from `df`. It can be a single column or a list of columns. It is also possible to rename the output columns via a dictionary. :param right_columns: Columns to select from `right`. It can be a single column or a list of columns. It is also possible to rename the output columns via a dictionary. :param keep: Choose whether to return the first match, last match or all matches. Default is `all`. :param use_numba: Use numba, if installed, to accelerate the computation. Applicable only to strictly non-equi joins. Default is `False`. :returns: A pandas DataFrame of the two merged Pandas objects. \"\"\" # noqa: E501 return _conditional_join_compute( df, right, conditions, how, sort_by_appearance, df_columns, right_columns, keep, use_numba, )","title":"conditional_join()"},{"location":"api/functions/#janitor.functions.convert_date","text":"","title":"convert_date"},{"location":"api/functions/#janitor.functions.convert_date.convert_excel_date","text":"Convert Excel's serial date format into Python datetime format. This method mutates the original DataFrame. Implementation is also from Stack Overflow Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"date\": [39690, 39690, 37118]}) >>> df date 0 39690 1 39690 2 37118 >>> df.convert_excel_date('date') date 0 2008-08-30 1 2008-08-30 2 2001-08-15 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable A column name. required Returns: Type Description DataFrame A pandas DataFrame with corrected dates. Exceptions: Type Description ValueError if there are non numeric values in the column. Source code in janitor/functions/convert_date.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def convert_excel_date( df: pd.DataFrame, column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert Excel's serial date format into Python datetime format. This method mutates the original DataFrame. Implementation is also from [Stack Overflow](https://stackoverflow.com/questions/38454403/convert-excel-style-date-with-pandas) Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"date\": [39690, 39690, 37118]}) >>> df date 0 39690 1 39690 2 37118 >>> df.convert_excel_date('date') date 0 2008-08-30 1 2008-08-30 2 2001-08-15 :param df: A pandas DataFrame. :param column_name: A column name. :returns: A pandas DataFrame with corrected dates. :raises ValueError: if there are non numeric values in the column. \"\"\" # noqa: E501 if not is_numeric_dtype(df[column_name]): raise ValueError( \"There are non-numeric values in the column. \\ All values must be numeric\" ) df[column_name] = pd.TimedeltaIndex( df[column_name], unit=\"d\" ) + dt.datetime( 1899, 12, 30 ) # noqa: W503 return df","title":"convert_excel_date()"},{"location":"api/functions/#janitor.functions.convert_date.convert_matlab_date","text":"Convert Matlab's serial date number into Python datetime format. Implementation is also from Stack Overflow This method mutates the original DataFrame. Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"date\": [737125.0, 737124.815863, 737124.4985, 737124]}) >>> df date 0 737125.000000 1 737124.815863 2 737124.498500 3 737124.000000 >>> df.convert_matlab_date('date') date 0 2018-03-06 00:00:00.000000 1 2018-03-05 19:34:50.563200 2 2018-03-05 11:57:50.399999 3 2018-03-05 00:00:00.000000 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable A column name. required Returns: Type Description DataFrame A pandas DataFrame with corrected dates. Source code in janitor/functions/convert_date.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def convert_matlab_date( df: pd.DataFrame, column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert Matlab's serial date number into Python datetime format. Implementation is also from [Stack Overflow](https://stackoverflow.com/questions/13965740/converting-matlabs-datenum-format-to-python) This method mutates the original DataFrame. Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"date\": [737125.0, 737124.815863, 737124.4985, 737124]}) >>> df date 0 737125.000000 1 737124.815863 2 737124.498500 3 737124.000000 >>> df.convert_matlab_date('date') date 0 2018-03-06 00:00:00.000000 1 2018-03-05 19:34:50.563200 2 2018-03-05 11:57:50.399999 3 2018-03-05 00:00:00.000000 :param df: A pandas DataFrame. :param column_name: A column name. :returns: A pandas DataFrame with corrected dates. \"\"\" # noqa: E501 days = pd.Series([dt.timedelta(v % 1) for v in df[column_name]]) df[column_name] = ( df[column_name].astype(int).apply(dt.datetime.fromordinal) + days - dt.timedelta(days=366) ) return df","title":"convert_matlab_date()"},{"location":"api/functions/#janitor.functions.convert_date.convert_unix_date","text":"Convert unix epoch time into Python datetime format. Note that this ignores local tz and convert all timestamps to naive datetime based on UTC! This method mutates the original DataFrame. Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"date\": [1651510462, 53394822, 1126233195]}) >>> df date 0 1651510462 1 53394822 2 1126233195 >>> df.convert_unix_date('date') date 0 2022-05-02 16:54:22 1 1971-09-10 23:53:42 2 2005-09-09 02:33:15 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable A column name. required Returns: Type Description DataFrame A pandas DataFrame with corrected dates. Source code in janitor/functions/convert_date.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def convert_unix_date(df: pd.DataFrame, column_name: Hashable) -> pd.DataFrame: \"\"\" Convert unix epoch time into Python datetime format. Note that this ignores local tz and convert all timestamps to naive datetime based on UTC! This method mutates the original DataFrame. Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"date\": [1651510462, 53394822, 1126233195]}) >>> df date 0 1651510462 1 53394822 2 1126233195 >>> df.convert_unix_date('date') date 0 2022-05-02 16:54:22 1 1971-09-10 23:53:42 2 2005-09-09 02:33:15 :param df: A pandas DataFrame. :param column_name: A column name. :returns: A pandas DataFrame with corrected dates. \"\"\" try: df[column_name] = pd.to_datetime(df[column_name], unit=\"s\") except OutOfBoundsDatetime: # Indicates time is in milliseconds. df[column_name] = pd.to_datetime(df[column_name], unit=\"ms\") return df","title":"convert_unix_date()"},{"location":"api/functions/#janitor.functions.count_cumulative_unique","text":"Implementation of count_cumulative_unique.","title":"count_cumulative_unique"},{"location":"api/functions/#janitor.functions.count_cumulative_unique.count_cumulative_unique","text":"Generates a running total of cumulative unique values in a given column. A new column will be created containing a running count of unique values in the specified column. If case_sensitive is True , then the case of any letters will matter (i.e., a != A ); otherwise, the case of any letters will not matter. This method does not mutate the original DataFrame. Examples: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"letters\": list(\"aabABb\"), ... \"numbers\": range(4, 10), ... }) >>> df letters numbers 0 a 4 1 a 5 2 b 6 3 A 7 4 B 8 5 b 9 >>> df.count_cumulative_unique( ... column_name=\"letters\", ... dest_column_name=\"letters_unique_count\", ... ) letters numbers letters_unique_count 0 a 4 1 1 a 5 1 2 b 6 2 3 A 7 3 4 B 8 4 5 b 9 4 Example: Cumulative counts, ignoring casing. >>> df.count_cumulative_unique( ... column_name=\"letters\", ... dest_column_name=\"letters_unique_count\", ... case_sensitive=False, ... ) letters numbers letters_unique_count 0 a 4 1 1 a 5 1 2 b 6 2 3 A 7 2 4 B 8 2 5 b 9 2 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Name of the column containing values from which a running count of unique values will be created. required dest_column_name str The name of the new column containing the cumulative count of unique values that will be created. required case_sensitive bool Whether or not uppercase and lowercase letters will be considered equal. Only valid with string-like columns. True Returns: Type Description DataFrame A pandas DataFrame with a new column containing a cumulative count of unique values from another column. Exceptions: Type Description TypeError If case_sensitive is False when counting a non-string column_name . Source code in janitor/functions/count_cumulative_unique.py @pf.register_dataframe_method def count_cumulative_unique( df: pd.DataFrame, column_name: Hashable, dest_column_name: str, case_sensitive: bool = True, ) -> pd.DataFrame: \"\"\"Generates a running total of cumulative unique values in a given column. A new column will be created containing a running count of unique values in the specified column. If `case_sensitive` is `True`, then the case of any letters will matter (i.e., `a != A`); otherwise, the case of any letters will not matter. This method does not mutate the original DataFrame. Examples: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"letters\": list(\"aabABb\"), ... \"numbers\": range(4, 10), ... }) >>> df letters numbers 0 a 4 1 a 5 2 b 6 3 A 7 4 B 8 5 b 9 >>> df.count_cumulative_unique( ... column_name=\"letters\", ... dest_column_name=\"letters_unique_count\", ... ) letters numbers letters_unique_count 0 a 4 1 1 a 5 1 2 b 6 2 3 A 7 3 4 B 8 4 5 b 9 4 Example: Cumulative counts, ignoring casing. >>> df.count_cumulative_unique( ... column_name=\"letters\", ... dest_column_name=\"letters_unique_count\", ... case_sensitive=False, ... ) letters numbers letters_unique_count 0 a 4 1 1 a 5 1 2 b 6 2 3 A 7 2 4 B 8 2 5 b 9 2 :param df: A pandas DataFrame. :param column_name: Name of the column containing values from which a running count of unique values will be created. :param dest_column_name: The name of the new column containing the cumulative count of unique values that will be created. :param case_sensitive: Whether or not uppercase and lowercase letters will be considered equal. Only valid with string-like columns. :returns: A pandas DataFrame with a new column containing a cumulative count of unique values from another column. :raises TypeError: If `case_sensitive` is False when counting a non-string `column_name`. \"\"\" check_column(df, column_name) check_column(df, dest_column_name, present=False) counter = df[column_name] if not case_sensitive: try: # Make it so that the the same uppercase and lowercase # letter are treated as one unique value counter = counter.str.lower() except (AttributeError, TypeError) as e: # AttributeError is raised by pandas when .str is used on # non-string types, e.g. int. # TypeError is raised by pandas when .str.lower is used on a # forbidden string type, e.g. bytes. raise TypeError( \"case_sensitive=False can only be used with a string-like \" f\"type. Column {column_name} is {counter.dtype} type.\" ) from e counter = ( counter.groupby(counter, sort=False).cumcount().to_numpy(copy=False) ) counter = np.cumsum(counter == 0) return df.assign(**{dest_column_name: counter})","title":"count_cumulative_unique()"},{"location":"api/functions/#janitor.functions.currency_column_to_numeric","text":"","title":"currency_column_to_numeric"},{"location":"api/functions/#janitor.functions.currency_column_to_numeric.currency_column_to_numeric","text":"Convert currency column to numeric. This method does not mutate the original DataFrame. This method allows one to take a column containing currency values, inadvertently imported as a string, and cast it as a float. This is usually the case when reading CSV files that were modified in Excel. Empty strings (i.e. '' ) are retained as NaN values. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a_col\": [\" 24.56\", \"-\", \"(12.12)\", \"1,000,000\"], ... \"d_col\": [\"\", \"foo\", \"1.23 dollars\", \"-1,000 yen\"], ... }) >>> df # doctest: +NORMALIZE_WHITESPACE a_col d_col 0 24.56 1 - foo 2 (12.12) 1.23 dollars 3 1,000,000 -1,000 yen The default cleaning style. >>> df.currency_column_to_numeric(\"d_col\") a_col d_col 0 24.56 NaN 1 - NaN 2 (12.12) 1.23 3 1,000,000 -1000.00 The accounting cleaning style. >>> df.currency_column_to_numeric(\"a_col\", cleaning_style=\"accounting\") # doctest: +NORMALIZE_WHITESPACE a_col d_col 0 24.56 1 0.00 foo 2 -12.12 1.23 dollars 3 1000000.00 -1,000 yen Valid cleaning styles are: None : Default cleaning is applied. Empty strings are always retained as NaN . Numbers, - , . are extracted and the resulting string is cast to a float. 'accounting' : Replaces numbers in parentheses with negatives, removes commas. Parameters: Name Type Description Default df DataFrame The pandas DataFrame. required column_name str The column containing currency values to modify. required cleaning_style Optional[str] What style of cleaning to perform. None cast_non_numeric Optional[dict] A dict of how to coerce certain strings to numeric type. For example, if there are values of 'REORDER' in the DataFrame, {'REORDER': 0} will cast all instances of 'REORDER' to 0. Only takes effect in the default cleaning style. None fill_all_non_numeric Union[float, int] Similar to cast_non_numeric , but fills all strings to the same value. For example, fill_all_non_numeric=1 , will make everything that doesn't coerce to a currency 1 . Only takes effect in the default cleaning style. None remove_non_numeric bool If set to True, rows of df that contain non-numeric values in the column_name column will be removed. Only takes effect in the default cleaning style. False Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError If cleaning_style is not one of the accepted styles. Source code in janitor/functions/currency_column_to_numeric.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\", type=\"cleaning_style\") def currency_column_to_numeric( df: pd.DataFrame, column_name: str, cleaning_style: Optional[str] = None, cast_non_numeric: Optional[dict] = None, fill_all_non_numeric: Optional[Union[float, int]] = None, remove_non_numeric: bool = False, ) -> pd.DataFrame: \"\"\"Convert currency column to numeric. This method does not mutate the original DataFrame. This method allows one to take a column containing currency values, inadvertently imported as a string, and cast it as a float. This is usually the case when reading CSV files that were modified in Excel. Empty strings (i.e. `''`) are retained as `NaN` values. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a_col\": [\" 24.56\", \"-\", \"(12.12)\", \"1,000,000\"], ... \"d_col\": [\"\", \"foo\", \"1.23 dollars\", \"-1,000 yen\"], ... }) >>> df # doctest: +NORMALIZE_WHITESPACE a_col d_col 0 24.56 1 - foo 2 (12.12) 1.23 dollars 3 1,000,000 -1,000 yen The default cleaning style. >>> df.currency_column_to_numeric(\"d_col\") a_col d_col 0 24.56 NaN 1 - NaN 2 (12.12) 1.23 3 1,000,000 -1000.00 The accounting cleaning style. >>> df.currency_column_to_numeric(\"a_col\", cleaning_style=\"accounting\") # doctest: +NORMALIZE_WHITESPACE a_col d_col 0 24.56 1 0.00 foo 2 -12.12 1.23 dollars 3 1000000.00 -1,000 yen Valid cleaning styles are: - `None`: Default cleaning is applied. Empty strings are always retained as `NaN`. Numbers, `-`, `.` are extracted and the resulting string is cast to a float. - `'accounting'`: Replaces numbers in parentheses with negatives, removes commas. :param df: The pandas DataFrame. :param column_name: The column containing currency values to modify. :param cleaning_style: What style of cleaning to perform. :param cast_non_numeric: A dict of how to coerce certain strings to numeric type. For example, if there are values of 'REORDER' in the DataFrame, `{'REORDER': 0}` will cast all instances of 'REORDER' to 0. Only takes effect in the default cleaning style. :param fill_all_non_numeric: Similar to `cast_non_numeric`, but fills all strings to the same value. For example, `fill_all_non_numeric=1`, will make everything that doesn't coerce to a currency `1`. Only takes effect in the default cleaning style. :param remove_non_numeric: If set to True, rows of `df` that contain non-numeric values in the `column_name` column will be removed. Only takes effect in the default cleaning style. :raises ValueError: If `cleaning_style` is not one of the accepted styles. :returns: A pandas DataFrame. \"\"\" # noqa: E501 check(\"column_name\", column_name, [str]) check_column(df, column_name) column_series = df[column_name] if cleaning_style == \"accounting\": df.loc[:, column_name] = df[column_name].apply( _clean_accounting_column ) return df if cleaning_style is not None: raise ValueError( \"`cleaning_style` is expected to be one of ('accounting', None). \" f\"Got {cleaning_style!r} instead.\" ) if cast_non_numeric: check(\"cast_non_numeric\", cast_non_numeric, [dict]) _make_cc_patrial = partial( _currency_column_to_numeric, cast_non_numeric=cast_non_numeric, ) column_series = column_series.apply(_make_cc_patrial) if remove_non_numeric: df = df.loc[column_series != \"\", :] # _replace_empty_string_with_none is applied here after the check on # remove_non_numeric since \"\" is our indicator that a string was coerced # in the original column column_series = _replace_empty_string_with_none(column_series) if fill_all_non_numeric is not None: check(\"fill_all_non_numeric\", fill_all_non_numeric, [int, float]) column_series = column_series.fillna(fill_all_non_numeric) column_series = _replace_original_empty_string_with_none(column_series) df = df.assign(**{column_name: pd.to_numeric(column_series)}) return df","title":"currency_column_to_numeric()"},{"location":"api/functions/#janitor.functions.deconcatenate_column","text":"Implementation of deconcatenating columns.","title":"deconcatenate_column"},{"location":"api/functions/#janitor.functions.deconcatenate_column.deconcatenate_column","text":"De-concatenates a single column into multiple columns. The column to de-concatenate can be either a collection (list, tuple, ...) which can be separated out with pd.Series.tolist() , or a string to slice based on sep . To determine this behaviour automatically, the first element in the column specified is inspected. If it is a string, then sep must be specified. Else, the function assumes that it is an iterable type (e.g. list or tuple ), and will attempt to deconcatenate by splitting the list. Given a column with string values, this is the inverse of the concatenate_columns function. Used to quickly split columns out of a single column. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"m\": [\"1-x\", \"2-y\", \"3-z\"]}) >>> df m 0 1-x 1 2-y 2 3-z >>> df.deconcatenate_column(\"m\", sep=\"-\", autoname=\"col\") m col1 col2 0 1-x 1 x 1 2-y 2 y 2 3-z 3 z The keyword argument preserve_position takes True or False boolean that controls whether the new_column_names will take the original position of the to-be-deconcatenated column_name : When preserve_position=False (default), df.columns change from [..., column_name, ...] to [..., column_name, ..., new_column_names] . In other words, the deconcatenated new columns are appended to the right of the original dataframe and the original column_name is NOT dropped. When preserve_position=True , df.column change from [..., column_name, ...] to [..., new_column_names, ...] . In other words, the deconcatenated new column will REPLACE the original column_name at its original position, and column_name itself is dropped. The keyword argument autoname accepts a base string and then automatically creates numbered column names based off the base string. For example, if col is passed in as the argument to autoname , and 4 columns are created, then the resulting columns will be named col1, col2, col3, col4 . Numbering is always 1-indexed, not 0-indexed, in order to make the column names human-friendly. This method does not mutate the original DataFrame. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column to split. required sep Optional[str] The separator delimiting the column's data. None new_column_names Union[List[str], Tuple[str]] A list of new column names post-splitting. None autoname str A base name for automatically naming the new columns. Takes precedence over new_column_names if both are provided. None preserve_position bool Boolean for whether or not to preserve original position of the column upon de-concatenation. False Returns: Type Description DataFrame A pandas DataFrame with a deconcatenated column. Exceptions: Type Description ValueError If column_name is not present in the DataFrame. ValueError If sep is not provided and the column values are of type str . ValueError If either new_column_names or autoname is not supplied. JanitorError If incorrect number of names is provided within new_column_names . Source code in janitor/functions/deconcatenate_column.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def deconcatenate_column( df: pd.DataFrame, column_name: Hashable, sep: Optional[str] = None, new_column_names: Optional[Union[List[str], Tuple[str]]] = None, autoname: str = None, preserve_position: bool = False, ) -> pd.DataFrame: \"\"\"De-concatenates a single column into multiple columns. The column to de-concatenate can be either a collection (list, tuple, ...) which can be separated out with `pd.Series.tolist()`, or a string to slice based on `sep`. To determine this behaviour automatically, the first element in the column specified is inspected. If it is a string, then `sep` must be specified. Else, the function assumes that it is an iterable type (e.g. `list` or `tuple`), and will attempt to deconcatenate by splitting the list. Given a column with string values, this is the inverse of the [`concatenate_columns`][janitor.functions.concatenate_columns.concatenate_columns] function. Used to quickly split columns out of a single column. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"m\": [\"1-x\", \"2-y\", \"3-z\"]}) >>> df m 0 1-x 1 2-y 2 3-z >>> df.deconcatenate_column(\"m\", sep=\"-\", autoname=\"col\") m col1 col2 0 1-x 1 x 1 2-y 2 y 2 3-z 3 z The keyword argument `preserve_position` takes `True` or `False` boolean that controls whether the `new_column_names` will take the original position of the to-be-deconcatenated `column_name`: - When `preserve_position=False` (default), `df.columns` change from `[..., column_name, ...]` to `[..., column_name, ..., new_column_names]`. In other words, the deconcatenated new columns are appended to the right of the original dataframe and the original `column_name` is NOT dropped. - When `preserve_position=True`, `df.column` change from `[..., column_name, ...]` to `[..., new_column_names, ...]`. In other words, the deconcatenated new column will REPLACE the original `column_name` at its original position, and `column_name` itself is dropped. The keyword argument `autoname` accepts a base string and then automatically creates numbered column names based off the base string. For example, if `col` is passed in as the argument to `autoname`, and 4 columns are created, then the resulting columns will be named `col1, col2, col3, col4`. Numbering is always 1-indexed, not 0-indexed, in order to make the column names human-friendly. This method does not mutate the original DataFrame. :param df: A pandas DataFrame. :param column_name: The column to split. :param sep: The separator delimiting the column's data. :param new_column_names: A list of new column names post-splitting. :param autoname: A base name for automatically naming the new columns. Takes precedence over `new_column_names` if both are provided. :param preserve_position: Boolean for whether or not to preserve original position of the column upon de-concatenation. :returns: A pandas DataFrame with a deconcatenated column. :raises ValueError: If `column_name` is not present in the DataFrame. :raises ValueError: If `sep` is not provided and the column values are of type `str`. :raises ValueError: If either `new_column_names` or `autoname` is not supplied. :raises JanitorError: If incorrect number of names is provided within `new_column_names`. \"\"\" # noqa: E501 if column_name not in df.columns: raise ValueError(f\"column name {column_name} not present in DataFrame\") if isinstance(df[column_name].iloc[0], str): if sep is None: raise ValueError( \"`sep` must be specified if the column values \" \"are of type `str`.\" ) df_deconcat = df[column_name].str.split(sep, expand=True) else: df_deconcat = pd.DataFrame( df[column_name].to_list(), columns=new_column_names, index=df.index ) if new_column_names is None and autoname is None: raise ValueError( \"One of `new_column_names` or `autoname` must be supplied.\" ) if autoname: new_column_names = [ f\"{autoname}{i}\" for i in range(1, df_deconcat.shape[1] + 1) ] if not len(new_column_names) == df_deconcat.shape[1]: raise JanitorError( f\"You need to provide {len(df_deconcat.shape[1])} names \" \"to `new_column_names`\" ) df_deconcat.columns = new_column_names df_new = pd.concat([df, df_deconcat], axis=1) if preserve_position: df_original = df.copy() cols = list(df_original.columns) index_original = cols.index(column_name) for i, col_new in enumerate(new_column_names): cols.insert(index_original + i, col_new) df_new = df_new.select_columns(cols).drop(columns=column_name) return df_new","title":"deconcatenate_column()"},{"location":"api/functions/#janitor.functions.drop_constant_columns","text":"Implementation of drop_constant_columns.","title":"drop_constant_columns"},{"location":"api/functions/#janitor.functions.drop_constant_columns.drop_constant_columns","text":"Finds and drops the constant columns from a Pandas DataFrame. Example: >>> import pandas as pd >>> import janitor >>> data_dict = { ... \"a\": [1, 1, 1], ... \"b\": [1, 2, 3], ... \"c\": [1, 1, 1], ... \"d\": [\"rabbit\", \"leopard\", \"lion\"], ... \"e\": [\"Cambridge\", \"Shanghai\", \"Basel\"] ... } >>> df = pd.DataFrame(data_dict) >>> df a b c d e 0 1 1 1 rabbit Cambridge 1 1 2 1 leopard Shanghai 2 1 3 1 lion Basel >>> df.drop_constant_columns() b d e 0 1 rabbit Cambridge 1 2 leopard Shanghai 2 3 lion Basel Parameters: Name Type Description Default df DataFrame Input Pandas DataFrame required Returns: Type Description DataFrame The Pandas DataFrame with the constant columns dropped. Source code in janitor/functions/drop_constant_columns.py @pf.register_dataframe_method def drop_constant_columns(df: pd.DataFrame) -> pd.DataFrame: \"\"\" Finds and drops the constant columns from a Pandas DataFrame. Example: >>> import pandas as pd >>> import janitor >>> data_dict = { ... \"a\": [1, 1, 1], ... \"b\": [1, 2, 3], ... \"c\": [1, 1, 1], ... \"d\": [\"rabbit\", \"leopard\", \"lion\"], ... \"e\": [\"Cambridge\", \"Shanghai\", \"Basel\"] ... } >>> df = pd.DataFrame(data_dict) >>> df a b c d e 0 1 1 1 rabbit Cambridge 1 1 2 1 leopard Shanghai 2 1 3 1 lion Basel >>> df.drop_constant_columns() b d e 0 1 rabbit Cambridge 1 2 leopard Shanghai 2 3 lion Basel :param df: Input Pandas DataFrame :returns: The Pandas DataFrame with the constant columns dropped. \"\"\" # Find the constant columns constant_columns = [] for col in df.columns: if len(df[col].unique()) == 1: constant_columns.append(col) # Drop constant columns from df and return it return df.drop(labels=constant_columns, axis=1)","title":"drop_constant_columns()"},{"location":"api/functions/#janitor.functions.drop_duplicate_columns","text":"Implementation for drop_duplicate_columns.","title":"drop_duplicate_columns"},{"location":"api/functions/#janitor.functions.drop_duplicate_columns.drop_duplicate_columns","text":"Remove a duplicated column specified by column_name . Specifying nth_index=0 will remove the first column, nth_index=1 will remove the second column, and so on and so forth. The corresponding tidyverse R's library is: select(-<column_name>_<nth_index + 1>) Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": range(2, 5), ... \"b\": range(3, 6), ... \"A\": range(4, 7), ... \"a*\": range(6, 9), ... }).clean_names(remove_special=True) >>> df a b a a 0 2 3 4 6 1 3 4 5 7 2 4 5 6 8 >>> df.drop_duplicate_columns(column_name=\"a\", nth_index=1) a b a 0 2 3 6 1 3 4 7 2 4 5 8 Parameters: Name Type Description Default df DataFrame A pandas DataFrame required column_name Hashable Name of duplicated columns. required nth_index int Among the duplicated columns, select the nth column to drop. 0 Returns: Type Description DataFrame A pandas DataFrame Source code in janitor/functions/drop_duplicate_columns.py @pf.register_dataframe_method def drop_duplicate_columns( df: pd.DataFrame, column_name: Hashable, nth_index: int = 0 ) -> pd.DataFrame: \"\"\"Remove a duplicated column specified by `column_name`. Specifying `nth_index=0` will remove the first column, `nth_index=1` will remove the second column, and so on and so forth. The corresponding tidyverse R's library is: `select(-<column_name>_<nth_index + 1>)` Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": range(2, 5), ... \"b\": range(3, 6), ... \"A\": range(4, 7), ... \"a*\": range(6, 9), ... }).clean_names(remove_special=True) >>> df a b a a 0 2 3 4 6 1 3 4 5 7 2 4 5 6 8 >>> df.drop_duplicate_columns(column_name=\"a\", nth_index=1) a b a 0 2 3 6 1 3 4 7 2 4 5 8 :param df: A pandas DataFrame :param column_name: Name of duplicated columns. :param nth_index: Among the duplicated columns, select the nth column to drop. :return: A pandas DataFrame \"\"\" col_indexes = [ col_idx for col_idx, col_name in enumerate(df.columns) if col_name == column_name ] # Select the column to remove based on nth_index. removed_col_idx = col_indexes[nth_index] # Filter out columns except for the one to be removed. filtered_cols = [ c_i for c_i, _ in enumerate(df.columns) if c_i != removed_col_idx ] return df.iloc[:, filtered_cols]","title":"drop_duplicate_columns()"},{"location":"api/functions/#janitor.functions.dropnotnull","text":"","title":"dropnotnull"},{"location":"api/functions/#janitor.functions.dropnotnull.dropnotnull","text":"Drop rows that do not have null values in the given column. This method does not mutate the original DataFrame. Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [1., np.NaN, 3.], \"b\": [None, \"y\", \"z\"]}) >>> df a b 0 1.0 None 1 NaN y 2 3.0 z >>> df.dropnotnull(\"a\") a b 1 NaN y >>> df.dropnotnull(\"b\") a b 0 1.0 None Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column name to drop rows from. required Returns: Type Description DataFrame A pandas DataFrame with dropped rows. Source code in janitor/functions/dropnotnull.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def dropnotnull(df: pd.DataFrame, column_name: Hashable) -> pd.DataFrame: \"\"\"Drop rows that do *not* have null values in the given column. This method does not mutate the original DataFrame. Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [1., np.NaN, 3.], \"b\": [None, \"y\", \"z\"]}) >>> df a b 0 1.0 None 1 NaN y 2 3.0 z >>> df.dropnotnull(\"a\") a b 1 NaN y >>> df.dropnotnull(\"b\") a b 0 1.0 None :param df: A pandas DataFrame. :param column_name: The column name to drop rows from. :returns: A pandas DataFrame with dropped rows. \"\"\" return df[pd.isna(df[column_name])]","title":"dropnotnull()"},{"location":"api/functions/#janitor.functions.encode_categorical","text":"","title":"encode_categorical"},{"location":"api/functions/#janitor.functions.encode_categorical.encode_categorical","text":"Encode the specified columns with Pandas' category dtype . It is syntactic sugar around pd.Categorical . This method does not mutate the original DataFrame. Simply pass a string, or a sequence of column names to column_names ; alternatively, you can pass kwargs, where the keys are the column names and the values can either be None, sort , appearance or a 1-D array-like object. None: column is cast to an unordered categorical. sort : column is cast to an ordered categorical, with the order defined by the sort-order of the categories. appearance : column is cast to an ordered categorical, with the order defined by the order of appearance in the original column. 1d-array-like object: column is cast to an ordered categorical, with the categories and order as specified in the input array. column_names and kwargs parameters cannot be used at the same time. Example: Using column_names >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df foo bar 0 b 4 1 b 5 2 a 6 3 c 7 4 b 8 >>> df.dtypes foo object bar int64 dtype: object >>> enc_df = df.encode_categorical(column_names=\"foo\") >>> enc_df.dtypes foo category bar int64 dtype: object >>> enc_df[\"foo\"].cat.categories Index(['a', 'b', 'c'], dtype='object') >>> enc_df[\"foo\"].cat.ordered False Example: Using kwargs to specify an ordered categorical. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df.dtypes foo object bar int64 dtype: object >>> enc_df = df.encode_categorical(foo=\"appearance\") >>> enc_df.dtypes foo category bar int64 dtype: object >>> enc_df[\"foo\"].cat.categories Index(['b', 'a', 'c'], dtype='object') >>> enc_df[\"foo\"].cat.ordered True Parameters: Name Type Description Default df DataFrame A pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] A column name or an iterable (list or tuple) of column names. None **kwargs A mapping from column name to either None , 'sort' or 'appearance' , or a 1-D array. This is useful in creating categorical columns that are ordered, or if the user needs to explicitly specify the categories. {} Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError If both column_names and kwargs are provided. Source code in janitor/functions/encode_categorical.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def encode_categorical( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable] = None, **kwargs, ) -> pd.DataFrame: \"\"\"Encode the specified columns with Pandas' [category dtype][cat]. [cat]: http://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html It is syntactic sugar around `pd.Categorical`. This method does not mutate the original DataFrame. Simply pass a string, or a sequence of column names to `column_names`; alternatively, you can pass kwargs, where the keys are the column names and the values can either be None, `sort`, `appearance` or a 1-D array-like object. - None: column is cast to an unordered categorical. - `sort`: column is cast to an ordered categorical, with the order defined by the sort-order of the categories. - `appearance`: column is cast to an ordered categorical, with the order defined by the order of appearance in the original column. - 1d-array-like object: column is cast to an ordered categorical, with the categories and order as specified in the input array. `column_names` and `kwargs` parameters cannot be used at the same time. Example: Using `column_names` >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df foo bar 0 b 4 1 b 5 2 a 6 3 c 7 4 b 8 >>> df.dtypes foo object bar int64 dtype: object >>> enc_df = df.encode_categorical(column_names=\"foo\") >>> enc_df.dtypes foo category bar int64 dtype: object >>> enc_df[\"foo\"].cat.categories Index(['a', 'b', 'c'], dtype='object') >>> enc_df[\"foo\"].cat.ordered False Example: Using `kwargs` to specify an ordered categorical. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df.dtypes foo object bar int64 dtype: object >>> enc_df = df.encode_categorical(foo=\"appearance\") >>> enc_df.dtypes foo category bar int64 dtype: object >>> enc_df[\"foo\"].cat.categories Index(['b', 'a', 'c'], dtype='object') >>> enc_df[\"foo\"].cat.ordered True :param df: A pandas DataFrame object. :param column_names: A column name or an iterable (list or tuple) of column names. :param **kwargs: A mapping from column name to either `None`, `'sort'` or `'appearance'`, or a 1-D array. This is useful in creating categorical columns that are ordered, or if the user needs to explicitly specify the categories. :returns: A pandas DataFrame. :raises ValueError: If both `column_names` and `kwargs` are provided. \"\"\" # noqa: E501 if all((column_names, kwargs)): raise ValueError( \"Only one of `column_names` or `kwargs` can be provided.\" ) # column_names deal with only category dtype (unordered) # kwargs takes care of scenarios where user wants an ordered category # or user supplies specific categories to create the categorical if column_names is not None: check(\"column_names\", column_names, [list, tuple, Hashable]) if isinstance(column_names, Hashable): column_names = [column_names] check_column(df, column_names) dtypes = {col: \"category\" for col in column_names} return df.astype(dtypes) return _computations_as_categorical(df, **kwargs)","title":"encode_categorical()"},{"location":"api/functions/#janitor.functions.expand_column","text":"Implementation for expand_column.","title":"expand_column"},{"location":"api/functions/#janitor.functions.expand_column.expand_column","text":"Expand a categorical column with multiple labels into dummy-coded columns. Super sugary syntax that wraps :py:meth: pandas.Series.str.get_dummies . This method does not mutate the original DataFrame. Functional usage syntax: >>> import pandas as pd >>> df = pd.DataFrame( ... { ... \"col1\": [\"A, B\", \"B, C, D\", \"E, F\", \"A, E, F\"], ... \"col2\": [1, 2, 3, 4], ... } ... ) >>> df = expand_column( ... df, ... column_name=\"col1\", ... sep=\", \" # note space in sep ... ) >>> df col1 col2 A B C D E F 0 A, B 1 1 1 0 0 0 0 1 B, C, D 2 0 1 1 1 0 0 2 E, F 3 0 0 0 0 1 1 3 A, E, F 4 1 0 0 0 1 1 Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = ( ... pd.DataFrame( ... { ... \"col1\": [\"A, B\", \"B, C, D\", \"E, F\", \"A, E, F\"], ... \"col2\": [1, 2, 3, 4], ... } ... ) ... .expand_column( ... column_name='col1', ... sep=', ' ... ) ... ) >>> df col1 col2 A B C D E F 0 A, B 1 1 1 0 0 0 0 1 B, C, D 2 0 1 1 1 0 0 2 E, F 3 0 0 0 0 1 1 3 A, E, F 4 1 0 0 0 1 1 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Which column to expand. required sep str The delimiter, same to :py:meth: ~pandas.Series.str.get_dummies 's sep , default as | . '|' concat bool Whether to return the expanded column concatenated to the original dataframe ( concat=True ), or to return it standalone ( concat=False ). True Returns: Type Description DataFrame A pandas DataFrame with an expanded column. Source code in janitor/functions/expand_column.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def expand_column( df: pd.DataFrame, column_name: Hashable, sep: str = \"|\", concat: bool = True, ) -> pd.DataFrame: \"\"\"Expand a categorical column with multiple labels into dummy-coded columns. Super sugary syntax that wraps :py:meth:`pandas.Series.str.get_dummies`. This method does not mutate the original DataFrame. Functional usage syntax: >>> import pandas as pd >>> df = pd.DataFrame( ... { ... \"col1\": [\"A, B\", \"B, C, D\", \"E, F\", \"A, E, F\"], ... \"col2\": [1, 2, 3, 4], ... } ... ) >>> df = expand_column( ... df, ... column_name=\"col1\", ... sep=\", \" # note space in sep ... ) >>> df col1 col2 A B C D E F 0 A, B 1 1 1 0 0 0 0 1 B, C, D 2 0 1 1 1 0 0 2 E, F 3 0 0 0 0 1 1 3 A, E, F 4 1 0 0 0 1 1 Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = ( ... pd.DataFrame( ... { ... \"col1\": [\"A, B\", \"B, C, D\", \"E, F\", \"A, E, F\"], ... \"col2\": [1, 2, 3, 4], ... } ... ) ... .expand_column( ... column_name='col1', ... sep=', ' ... ) ... ) >>> df col1 col2 A B C D E F 0 A, B 1 1 1 0 0 0 0 1 B, C, D 2 0 1 1 1 0 0 2 E, F 3 0 0 0 0 1 1 3 A, E, F 4 1 0 0 0 1 1 :param df: A pandas DataFrame. :param column_name: Which column to expand. :param sep: The delimiter, same to :py:meth:`~pandas.Series.str.get_dummies`'s `sep`, default as `|`. :param concat: Whether to return the expanded column concatenated to the original dataframe (`concat=True`), or to return it standalone (`concat=False`). :returns: A pandas DataFrame with an expanded column. \"\"\" # noqa: E501 expanded_df = df[column_name].str.get_dummies(sep=sep) if concat: df = df.join(expanded_df) return df return expanded_df","title":"expand_column()"},{"location":"api/functions/#janitor.functions.expand_grid","text":"","title":"expand_grid"},{"location":"api/functions/#janitor.functions.expand_grid.expand_grid","text":"Creates a DataFrame from a cartesian combination of all inputs. It is not restricted to DataFrame; it can work with any list-like structure that is 1 or 2 dimensional. If method-chaining to a DataFrame, a string argument to df_key parameter must be provided. Data types are preserved in this function, including pandas' extension array dtypes. The output will always be a DataFrame, usually with a MultiIndex column, with the keys of the others dictionary serving as the top level columns. If a DataFrame with MultiIndex columns is part of the arguments in others , the columns are flattened, before the final DataFrame is generated. If a pandas Series/DataFrame is passed, and has a labeled index, or a MultiIndex index, the index is discarded; the final DataFrame will have a RangeIndex. The MultiIndexed DataFrame can be flattened using pyjanitor's collapse_levels method; the user can also decide to drop any of the levels, via pandas' droplevel method. Example: >>> import pandas as pd >>> import janitor as jn >>> df = pd.DataFrame({\"x\": [1, 2], \"y\": [2, 1]}) >>> data = {\"z\": [1, 2, 3]} >>> df.expand_grid(df_key=\"df\", others=data) df z x y 0 0 1 2 1 1 1 2 2 2 1 2 3 3 2 1 1 4 2 1 2 5 2 1 3 Expand_grid works with non-pandas objects: >>> data = {\"x\": [1, 2, 3], \"y\": [1, 2]} >>> jn.expand_grid(others=data) x y 0 0 0 1 1 1 1 2 2 2 1 3 2 2 4 3 1 5 3 2 Parameters: Name Type Description Default df Optional[pandas.core.frame.DataFrame] A pandas DataFrame. None df_key Optional[str] name of key for the dataframe. It becomes part of the column names of the dataframe. None others Optional[Dict] A dictionary that contains the data to be combined with the dataframe. If no dataframe exists, all inputs in others will be combined to create a DataFrame. None Returns: Type Description DataFrame A pandas DataFrame of the cartesian product. Exceptions: Type Description KeyError if there is a DataFrame and df_key is not provided. Source code in janitor/functions/expand_grid.py @pf.register_dataframe_method def expand_grid( df: Optional[pd.DataFrame] = None, df_key: Optional[str] = None, *, others: Optional[Dict] = None, ) -> pd.DataFrame: \"\"\" Creates a DataFrame from a cartesian combination of all inputs. It is not restricted to DataFrame; it can work with any list-like structure that is 1 or 2 dimensional. If method-chaining to a DataFrame, a string argument to `df_key` parameter must be provided. Data types are preserved in this function, including pandas' extension array dtypes. The output will always be a DataFrame, usually with a MultiIndex column, with the keys of the `others` dictionary serving as the top level columns. If a DataFrame with MultiIndex columns is part of the arguments in `others`, the columns are flattened, before the final DataFrame is generated. If a pandas Series/DataFrame is passed, and has a labeled index, or a MultiIndex index, the index is discarded; the final DataFrame will have a RangeIndex. The MultiIndexed DataFrame can be flattened using pyjanitor's [`collapse_levels`][janitor.functions.collapse_levels.collapse_levels] method; the user can also decide to drop any of the levels, via pandas' `droplevel` method. Example: >>> import pandas as pd >>> import janitor as jn >>> df = pd.DataFrame({\"x\": [1, 2], \"y\": [2, 1]}) >>> data = {\"z\": [1, 2, 3]} >>> df.expand_grid(df_key=\"df\", others=data) df z x y 0 0 1 2 1 1 1 2 2 2 1 2 3 3 2 1 1 4 2 1 2 5 2 1 3 Expand_grid works with non-pandas objects: >>> data = {\"x\": [1, 2, 3], \"y\": [1, 2]} >>> jn.expand_grid(others=data) x y 0 0 0 1 1 1 1 2 2 2 1 3 2 2 4 3 1 5 3 2 :param df: A pandas DataFrame. :param df_key: name of key for the dataframe. It becomes part of the column names of the dataframe. :param others: A dictionary that contains the data to be combined with the dataframe. If no dataframe exists, all inputs in `others` will be combined to create a DataFrame. :returns: A pandas DataFrame of the cartesian product. :raises KeyError: if there is a DataFrame and `df_key` is not provided. \"\"\" if not others: if df is not None: return df return check(\"others\", others, [dict]) # if there is a DataFrame, for the method chaining, # it must have a key, to create a name value pair if df is not None: df = df.copy() if not df_key: raise KeyError( \"Using `expand_grid` as part of a \" \"DataFrame method chain requires that \" \"a string argument be provided for \" \"the `df_key` parameter. \" ) check(\"df_key\", df_key, [str]) others = {**{df_key: df}, **others} return _computations_expand_grid(others)","title":"expand_grid()"},{"location":"api/functions/#janitor.functions.factorize_columns","text":"Implementation of the factorize_columns function","title":"factorize_columns"},{"location":"api/functions/#janitor.functions.factorize_columns.factorize_columns","text":"Converts labels into numerical data. This method will create a new column with the string _enc appended after the original column's name. This can be overriden with the suffix parameter. Internally, this method uses pandas factorize method. It takes in an optional suffix and keyword arguments also. An empty string as suffix will override the existing column. This method does not mutate the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df foo bar 0 b 4 1 b 5 2 a 6 3 c 7 4 b 8 >>> df.factorize_columns(column_names=\"foo\") foo bar foo_enc 0 b 4 0 1 b 5 0 2 a 6 1 3 c 7 2 4 b 8 0 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] A column name or an iterable (list or tuple) of column names. required suffix str Suffix to be used for the new column. An empty string suffix means, it will override the existing column. '_enc' **kwargs Keyword arguments. It takes any of the keyword arguments, which the pandas factorize method takes like sort , na_sentinel , size_hint . {} Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/factorize_columns.py @pf.register_dataframe_method def factorize_columns( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable], suffix: str = \"_enc\", **kwargs, ) -> pd.DataFrame: \"\"\" Converts labels into numerical data. This method will create a new column with the string `_enc` appended after the original column's name. This can be overriden with the suffix parameter. Internally, this method uses pandas `factorize` method. It takes in an optional suffix and keyword arguments also. An empty string as suffix will override the existing column. This method does not mutate the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df foo bar 0 b 4 1 b 5 2 a 6 3 c 7 4 b 8 >>> df.factorize_columns(column_names=\"foo\") foo bar foo_enc 0 b 4 0 1 b 5 0 2 a 6 1 3 c 7 2 4 b 8 0 :param df: The pandas DataFrame object. :param column_names: A column name or an iterable (list or tuple) of column names. :param suffix: Suffix to be used for the new column. An empty string suffix means, it will override the existing column. :param **kwargs: Keyword arguments. It takes any of the keyword arguments, which the pandas factorize method takes like `sort`, `na_sentinel`, `size_hint`. :returns: A pandas DataFrame. \"\"\" df = _factorize(df.copy(), column_names, suffix, **kwargs) return df","title":"factorize_columns()"},{"location":"api/functions/#janitor.functions.fill","text":"","title":"fill"},{"location":"api/functions/#janitor.functions.fill.fill_direction","text":"Provide a method-chainable function for filling missing values in selected columns. It is a wrapper for pd.Series.ffill and pd.Series.bfill , and pairs the column name with one of up , down , updown , and downup . Example: >>> import pandas as pd >>> import janitor as jn >>> df = pd.DataFrame( ... { ... 'col1': [1, 2, 3, 4], ... 'col2': [None, 5, 6, 7], ... 'col3': [8, 9, 10, None], ... 'col4': [None, None, 11, None], ... 'col5': [None, 12, 13, None] ... } ... ) >>> df col1 col2 col3 col4 col5 0 1 NaN 8.0 NaN NaN 1 2 5.0 9.0 NaN 12.0 2 3 6.0 10.0 11.0 13.0 3 4 7.0 NaN NaN NaN >>> df.fill_direction( ... col2 = 'up', ... col3 = 'down', ... col4 = 'downup', ... col5 = 'updown' ... ) col1 col2 col3 col4 col5 0 1 5.0 8.0 11.0 12.0 1 2 5.0 9.0 11.0 12.0 2 3 6.0 10.0 11.0 13.0 3 4 7.0 10.0 11.0 13.0 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required kwargs Key - value pairs of columns and directions. Directions can be either down , up , updown (fill up then down) and downup (fill down then up). {} Returns: Type Description DataFrame A pandas DataFrame with modified column(s). Exceptions: Type Description ValueError if direction supplied is not one of down , up , updown , or downup . Source code in janitor/functions/fill.py @pf.register_dataframe_method def fill_direction(df: pd.DataFrame, **kwargs) -> pd.DataFrame: \"\"\" Provide a method-chainable function for filling missing values in selected columns. It is a wrapper for `pd.Series.ffill` and `pd.Series.bfill`, and pairs the column name with one of `up`, `down`, `updown`, and `downup`. Example: >>> import pandas as pd >>> import janitor as jn >>> df = pd.DataFrame( ... { ... 'col1': [1, 2, 3, 4], ... 'col2': [None, 5, 6, 7], ... 'col3': [8, 9, 10, None], ... 'col4': [None, None, 11, None], ... 'col5': [None, 12, 13, None] ... } ... ) >>> df col1 col2 col3 col4 col5 0 1 NaN 8.0 NaN NaN 1 2 5.0 9.0 NaN 12.0 2 3 6.0 10.0 11.0 13.0 3 4 7.0 NaN NaN NaN >>> df.fill_direction( ... col2 = 'up', ... col3 = 'down', ... col4 = 'downup', ... col5 = 'updown' ... ) col1 col2 col3 col4 col5 0 1 5.0 8.0 11.0 12.0 1 2 5.0 9.0 11.0 12.0 2 3 6.0 10.0 11.0 13.0 3 4 7.0 10.0 11.0 13.0 :param df: A pandas DataFrame. :param kwargs: Key - value pairs of columns and directions. Directions can be either `down`, `up`, `updown` (fill up then down) and `downup` (fill down then up). :returns: A pandas DataFrame with modified column(s). :raises ValueError: if direction supplied is not one of `down`, `up`, `updown`, or `downup`. \"\"\" if not kwargs: return df fill_types = {fill.name for fill in _FILLTYPE} for column_name, fill_type in kwargs.items(): check(\"column_name\", column_name, [str]) check(\"fill_type\", fill_type, [str]) if fill_type.upper() not in fill_types: raise ValueError( \"\"\" fill_type should be one of up, down, updown, or downup. \"\"\" ) check_column(df, kwargs) new_values = {} for column_name, fill_type in kwargs.items(): direction = _FILLTYPE[f\"{fill_type.upper()}\"].value if len(direction) == 1: direction = methodcaller(direction[0]) output = direction(df[column_name]) else: direction = [methodcaller(entry) for entry in direction] output = _chain_func(df[column_name], *direction) new_values[column_name] = output return df.assign(**new_values)","title":"fill_direction()"},{"location":"api/functions/#janitor.functions.fill.fill_empty","text":"Fill NaN values in specified columns with a given value. Super sugary syntax that wraps pandas.DataFrame.fillna . This method mutates the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... 'col1': [1, 2, 3], ... 'col2': [None, 4, None ], ... 'col3': [None, 5, 6] ... } ... ) >>> df col1 col2 col3 0 1 NaN NaN 1 2 4.0 5.0 2 3 NaN 6.0 >>> df.fill_empty(column_names = 'col2', value = 0) col1 col2 col3 0 1 0.0 NaN 1 2 4.0 5.0 2 3 0.0 6.0 >>> df.fill_empty(column_names = ['col2', 'col3'], value = 0) col1 col2 col3 0 1 0.0 0.0 1 2 4.0 5.0 2 3 0.0 6.0 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names Union[str, Iterable[str], Hashable] column_names: A column name or an iterable (list or tuple) of column names. If a single column name is passed in, then only that column will be filled; if a list or tuple is passed in, then those columns will all be filled with the same value. required value The value that replaces the NaN values. required Returns: Type Description DataFrame A pandas DataFrame with NaN values filled. Source code in janitor/functions/fill.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def fill_empty( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable], value ) -> pd.DataFrame: \"\"\" Fill `NaN` values in specified columns with a given value. Super sugary syntax that wraps `pandas.DataFrame.fillna`. This method mutates the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... 'col1': [1, 2, 3], ... 'col2': [None, 4, None ], ... 'col3': [None, 5, 6] ... } ... ) >>> df col1 col2 col3 0 1 NaN NaN 1 2 4.0 5.0 2 3 NaN 6.0 >>> df.fill_empty(column_names = 'col2', value = 0) col1 col2 col3 0 1 0.0 NaN 1 2 4.0 5.0 2 3 0.0 6.0 >>> df.fill_empty(column_names = ['col2', 'col3'], value = 0) col1 col2 col3 0 1 0.0 0.0 1 2 4.0 5.0 2 3 0.0 6.0 :param df: A pandas DataFrame. :param column_names: column_names: A column name or an iterable (list or tuple) of column names. If a single column name is passed in, then only that column will be filled; if a list or tuple is passed in, then those columns will all be filled with the same value. :param value: The value that replaces the `NaN` values. :returns: A pandas DataFrame with `NaN` values filled. \"\"\" check_column(df, column_names) return _fill_empty(df, column_names, value=value)","title":"fill_empty()"},{"location":"api/functions/#janitor.functions.filter","text":"","title":"filter"},{"location":"api/functions/#janitor.functions.filter.filter_column_isin","text":"Filter a dataframe for values in a column that exist in the given iterable. This method does not mutate the original DataFrame. Assumes exact matching; fuzzy matching not implemented. Example: Filter the dataframe to retain rows for which names are exactly James or John . >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"names\": [\"Jane\", \"Jeremy\", \"John\"], \"foo\": list(\"xyz\")}) >>> df names foo 0 Jane x 1 Jeremy y 2 John z >>> df.filter_column_isin(column_name=\"names\", iterable=[\"James\", \"John\"]) names foo 2 John z This is the method-chaining alternative to: df = df[df[\"names\"].isin([\"James\", \"John\"])] If complement=True , then we will only get rows for which the names are neither James nor John . Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column on which to filter. required iterable Iterable An iterable. Could be a list, tuple, another pandas Series. required complement bool Whether to return the complement of the selection or not. False Returns: Type Description DataFrame A filtered pandas DataFrame. Exceptions: Type Description ValueError If iterable does not have a length of 1 or greater. Source code in janitor/functions/filter.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def filter_column_isin( df: pd.DataFrame, column_name: Hashable, iterable: Iterable, complement: bool = False, ) -> pd.DataFrame: \"\"\"Filter a dataframe for values in a column that exist in the given iterable. This method does not mutate the original DataFrame. Assumes exact matching; fuzzy matching not implemented. Example: Filter the dataframe to retain rows for which `names` are exactly `James` or `John`. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"names\": [\"Jane\", \"Jeremy\", \"John\"], \"foo\": list(\"xyz\")}) >>> df names foo 0 Jane x 1 Jeremy y 2 John z >>> df.filter_column_isin(column_name=\"names\", iterable=[\"James\", \"John\"]) names foo 2 John z This is the method-chaining alternative to: ```python df = df[df[\"names\"].isin([\"James\", \"John\"])] ``` If `complement=True`, then we will only get rows for which the names are neither `James` nor `John`. :param df: A pandas DataFrame. :param column_name: The column on which to filter. :param iterable: An iterable. Could be a list, tuple, another pandas Series. :param complement: Whether to return the complement of the selection or not. :returns: A filtered pandas DataFrame. :raises ValueError: If `iterable` does not have a length of `1` or greater. \"\"\" # noqa: E501 if len(iterable) == 0: raise ValueError( \"`iterable` kwarg must be given an iterable of length 1 \" \"or greater.\" ) criteria = df[column_name].isin(iterable) if complement: return df[~criteria] return df[criteria]","title":"filter_column_isin()"},{"location":"api/functions/#janitor.functions.filter.filter_date","text":"Filter a date-based column based on certain criteria. This method does not mutate the original DataFrame. Dates may be finicky and this function builds on top of the magic from the pandas to_datetime function that is able to parse dates well. Additional options to parse the date type of your column may be found at the official pandas documentation . Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": range(5, 9), ... \"dt\": [\"2021-11-12\", \"2021-12-15\", \"2022-01-03\", \"2022-01-09\"], ... }) >>> df a dt 0 5 2021-11-12 1 6 2021-12-15 2 7 2022-01-03 3 8 2022-01-09 >>> df.filter_date(\"dt\", start_date=\"2021-12-01\", end_date=\"2022-01-05\") a dt 1 6 2021-12-15 2 7 2022-01-03 >>> df.filter_date(\"dt\", years=[2021], months=[12]) a dt 1 6 2021-12-15 Note This method will cast your column to a Timestamp! Note This only affects the format of the start_date and end_date parameters. If there's an issue with the format of the DataFrame being parsed, you would pass {'format': your_format} to column_date_options . Parameters: Name Type Description Default df DataFrame The dataframe to filter on. required column_name Hashable The column which to apply the fraction transformation. required start_date Optional[datetime.date] The beginning date to use to filter the DataFrame. None end_date Optional[datetime.date] The end date to use to filter the DataFrame. None years Optional[List] The years to use to filter the DataFrame. None months Optional[List] The months to use to filter the DataFrame. None days Optional[List] The days to use to filter the DataFrame. None column_date_options Optional[Dict] Special options to use when parsing the date column in the original DataFrame. The options may be found at the official Pandas documentation. None format Optional[str] If you're using a format for start_date or end_date that is not recognized natively by pandas' to_datetime function, you may supply the format yourself. Python date and time formats may be found here . None Returns: Type Description DataFrame A filtered pandas DataFrame. Source code in janitor/functions/filter.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\", start=\"start_date\", end=\"end_date\") def filter_date( df: pd.DataFrame, column_name: Hashable, start_date: Optional[dt.date] = None, end_date: Optional[dt.date] = None, years: Optional[List] = None, months: Optional[List] = None, days: Optional[List] = None, column_date_options: Optional[Dict] = None, format: Optional[str] = None, # skipcq: PYL-W0622 ) -> pd.DataFrame: \"\"\"Filter a date-based column based on certain criteria. This method does not mutate the original DataFrame. Dates may be finicky and this function builds on top of the *magic* from the pandas `to_datetime` function that is able to parse dates well. Additional options to parse the date type of your column may be found at the official pandas [documentation][datetime]. [datetime]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": range(5, 9), ... \"dt\": [\"2021-11-12\", \"2021-12-15\", \"2022-01-03\", \"2022-01-09\"], ... }) >>> df a dt 0 5 2021-11-12 1 6 2021-12-15 2 7 2022-01-03 3 8 2022-01-09 >>> df.filter_date(\"dt\", start_date=\"2021-12-01\", end_date=\"2022-01-05\") a dt 1 6 2021-12-15 2 7 2022-01-03 >>> df.filter_date(\"dt\", years=[2021], months=[12]) a dt 1 6 2021-12-15 !!!note This method will cast your column to a Timestamp! !!!note This only affects the format of the `start_date` and `end_date` parameters. If there's an issue with the format of the DataFrame being parsed, you would pass `{'format': your_format}` to `column_date_options`. :param df: The dataframe to filter on. :param column_name: The column which to apply the fraction transformation. :param start_date: The beginning date to use to filter the DataFrame. :param end_date: The end date to use to filter the DataFrame. :param years: The years to use to filter the DataFrame. :param months: The months to use to filter the DataFrame. :param days: The days to use to filter the DataFrame. :param column_date_options: Special options to use when parsing the date column in the original DataFrame. The options may be found at the official Pandas documentation. :param format: If you're using a format for `start_date` or `end_date` that is not recognized natively by pandas' `to_datetime` function, you may supply the format yourself. Python date and time formats may be found [here](http://strftime.org/). :returns: A filtered pandas DataFrame. \"\"\" # noqa: E501 def _date_filter_conditions(conditions): \"\"\"Taken from: https://stackoverflow.com/a/13616382.\"\"\" return reduce(np.logical_and, conditions) if column_date_options: df.loc[:, column_name] = pd.to_datetime( df.loc[:, column_name], **column_date_options ) else: df.loc[:, column_name] = pd.to_datetime(df.loc[:, column_name]) _filter_list = [] if start_date: start_date = pd.to_datetime(start_date, format=format) _filter_list.append(df.loc[:, column_name] >= start_date) if end_date: end_date = pd.to_datetime(end_date, format=format) _filter_list.append(df.loc[:, column_name] <= end_date) if years: _filter_list.append(df.loc[:, column_name].dt.year.isin(years)) if months: _filter_list.append(df.loc[:, column_name].dt.month.isin(months)) if days: _filter_list.append(df.loc[:, column_name].dt.day.isin(days)) if start_date and end_date and start_date > end_date: warnings.warn( f\"Your start date of {start_date} is after your end date of \" f\"{end_date}. Is this intended?\" ) return df.loc[_date_filter_conditions(_filter_list), :]","title":"filter_date()"},{"location":"api/functions/#janitor.functions.filter.filter_on","text":"Return a dataframe filtered on a particular criteria. This method does not mutate the original DataFrame. This is super-sugary syntax that wraps the pandas .query() API, enabling users to use strings to quickly specify filters for filtering their dataframe. The intent is that filter_on as a verb better matches the intent of a pandas user than the verb query . This is intended to be the method-chaining equivalent of the following: df = df[df[\"score\"] < 3] Example: Filter students who failed an exam (scored less than 50). >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"student_id\": [\"S1\", \"S2\", \"S3\"], ... \"score\": [40, 60, 85], ... }) >>> df student_id score 0 S1 40 1 S2 60 2 S3 85 >>> df.filter_on(\"score < 50\", complement=False) student_id score 0 S1 40 Credit to Brant Peterson for the name. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required criteria str A filtering criteria that returns an array or Series of booleans, on which pandas can filter on. required complement bool Whether to return the complement of the filter or not. If set to True, then the rows for which the criteria is False are retained instead. False Returns: Type Description DataFrame A filtered pandas DataFrame. Source code in janitor/functions/filter.py @pf.register_dataframe_method def filter_on( df: pd.DataFrame, criteria: str, complement: bool = False, ) -> pd.DataFrame: \"\"\"Return a dataframe filtered on a particular criteria. This method does not mutate the original DataFrame. This is super-sugary syntax that wraps the pandas `.query()` API, enabling users to use strings to quickly specify filters for filtering their dataframe. The intent is that `filter_on` as a verb better matches the intent of a pandas user than the verb `query`. This is intended to be the method-chaining equivalent of the following: ```python df = df[df[\"score\"] < 3] ``` Example: Filter students who failed an exam (scored less than 50). >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"student_id\": [\"S1\", \"S2\", \"S3\"], ... \"score\": [40, 60, 85], ... }) >>> df student_id score 0 S1 40 1 S2 60 2 S3 85 >>> df.filter_on(\"score < 50\", complement=False) student_id score 0 S1 40 Credit to Brant Peterson for the name. :param df: A pandas DataFrame. :param criteria: A filtering criteria that returns an array or Series of booleans, on which pandas can filter on. :param complement: Whether to return the complement of the filter or not. If set to True, then the rows for which the criteria is False are retained instead. :returns: A filtered pandas DataFrame. \"\"\" if complement: return df.query(f\"not ({criteria})\") return df.query(criteria)","title":"filter_on()"},{"location":"api/functions/#janitor.functions.filter.filter_string","text":"Filter a string-based column according to whether it contains a substring. This is super sugary syntax that builds on top of pandas.Series.str.contains . It is meant to be the method-chaining equivalent of the following: df = df[df[column_name].str.contains(search_string)]] This method does not mutate the original DataFrame. Example: Retain rows whose column values contain a particular substring. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": range(3, 6), \"b\": [\"bear\", \"peeL\", \"sail\"]}) >>> df a b 0 3 bear 1 4 peeL 2 5 sail >>> df.filter_string(column_name=\"b\", search_string=\"ee\") a b 1 4 peeL >>> df.filter_string(column_name=\"b\", search_string=\"L\", case=False) a b 1 4 peeL 2 5 sail Example: Filter names does not contain '.' (disable regex mode). >>> import pandas as pd >>> import janitor >>> df = pd.Series([\"JoseChen\", \"Brian.Salvi\"], name=\"Name\").to_frame() >>> df Name 0 JoseChen 1 Brian.Salvi >>> df.filter_string(column_name=\"Name\", search_string=\".\", regex=False, complement=True) Name 0 JoseChen Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column to filter. The column should contain strings. required search_string str A regex pattern or a (sub-)string to search. required complement bool Whether to return the complement of the filter or not. If set to True, then the rows for which the string search fails are retained instead. False case bool If True, case sensitive. True flags int Flags to pass through to the re module, e.g. re.IGNORECASE. 0 na Fill value for missing values. The default depends on dtype of the array. For object-dtype, numpy.nan is used. For StringDtype , pandas.NA is used. None regex bool If True, assumes search_string is a regular expression. If False, treats the search_string as a literal string. True Returns: Type Description DataFrame A filtered pandas DataFrame. Source code in janitor/functions/filter.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def filter_string( df: pd.DataFrame, column_name: Hashable, search_string: str, complement: bool = False, case: bool = True, flags: int = 0, na=None, regex: bool = True, ) -> pd.DataFrame: \"\"\"Filter a string-based column according to whether it contains a substring. This is super sugary syntax that builds on top of `pandas.Series.str.contains`. It is meant to be the method-chaining equivalent of the following: ```python df = df[df[column_name].str.contains(search_string)]] ``` This method does not mutate the original DataFrame. Example: Retain rows whose column values contain a particular substring. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": range(3, 6), \"b\": [\"bear\", \"peeL\", \"sail\"]}) >>> df a b 0 3 bear 1 4 peeL 2 5 sail >>> df.filter_string(column_name=\"b\", search_string=\"ee\") a b 1 4 peeL >>> df.filter_string(column_name=\"b\", search_string=\"L\", case=False) a b 1 4 peeL 2 5 sail Example: Filter names does not contain `'.'` (disable regex mode). >>> import pandas as pd >>> import janitor >>> df = pd.Series([\"JoseChen\", \"Brian.Salvi\"], name=\"Name\").to_frame() >>> df Name 0 JoseChen 1 Brian.Salvi >>> df.filter_string(column_name=\"Name\", search_string=\".\", regex=False, complement=True) Name 0 JoseChen :param df: A pandas DataFrame. :param column_name: The column to filter. The column should contain strings. :param search_string: A regex pattern or a (sub-)string to search. :param complement: Whether to return the complement of the filter or not. If set to True, then the rows for which the string search fails are retained instead. :param case: If True, case sensitive. :param flags: Flags to pass through to the re module, e.g. re.IGNORECASE. :param na: Fill value for missing values. The default depends on dtype of the array. For object-dtype, `numpy.nan` is used. For `StringDtype`, `pandas.NA` is used. :param regex: If True, assumes `search_string` is a regular expression. If False, treats the `search_string` as a literal string. :returns: A filtered pandas DataFrame. \"\"\" # noqa: E501 criteria = df[column_name].str.contains( pat=search_string, case=case, flags=flags, na=na, regex=regex, ) if complement: return df[~criteria] return df[criteria]","title":"filter_string()"},{"location":"api/functions/#janitor.functions.find_replace","text":"Implementation for find_replace.","title":"find_replace"},{"location":"api/functions/#janitor.functions.find_replace.find_replace","text":"Perform a find-and-replace action on provided columns. Depending on use case, users can choose either exact, full-value matching, or regular-expression-based fuzzy matching (hence allowing substring matching in the latter case). For strings, the matching is always case sensitive. For instance, given a DataFrame containing orders at a coffee shop: >>> df = pd.DataFrame({ ... \"customer\": [\"Mary\", \"Tom\", \"Lila\"], ... \"order\": [\"ice coffee\", \"lemonade\", \"regular coffee\"] ... }) >>> df customer order 0 Mary ice coffee 1 Tom lemonade 2 Lila regular coffee Our task is to replace values ice coffee and regular coffee of the order column into latte . Example 1 - exact matching (functional usage): >>> df = find_replace( ... df, ... match=\"exact\", ... order={\"ice coffee\": \"latte\", \"regular coffee\": \"latte\"}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte Example 1 - exact matching (method chaining): >>> df = df.find_replace( ... match=\"exact\", ... order={\"ice coffee\": \"latte\", \"regular coffee\": \"latte\"}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte Example 2 - Regular-expression-based matching (functional usage): >>> df = find_replace( ... df, ... match='regex', ... order={'coffee$': 'latte'}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte Example 2 - Regular-expression-based matching (method chaining usage): >>> df = df.find_replace( ... match='regex', ... order={'coffee$': 'latte'}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte To perform a find and replace on the entire DataFrame, pandas' df.replace() function provides the appropriate functionality. You can find more detail on the replace docs. This function only works with column names that have no spaces or punctuation in them. For example, a column name item_name would work with find_replace , because it is a contiguous string that can be parsed correctly, but item name would not be parsed correctly by the Python interpreter. If you have column names that might not be compatible, we recommend calling on clean_names() as the first method call. If, for whatever reason, that is not possible, then _find_replace is available as a function that you can do a pandas pipe call on. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required match str Whether or not to perform an exact match or not. Valid values are \"exact\" or \"regex\". 'exact' mappings keyword arguments corresponding to column names that have dictionaries passed in indicating what to find (keys) and what to replace with (values). {} Returns: Type Description DataFrame A pandas DataFrame with replaced values. Source code in janitor/functions/find_replace.py @pf.register_dataframe_method def find_replace( df: pd.DataFrame, match: str = \"exact\", **mappings ) -> pd.DataFrame: \"\"\" Perform a find-and-replace action on provided columns. Depending on use case, users can choose either exact, full-value matching, or regular-expression-based fuzzy matching (hence allowing substring matching in the latter case). For strings, the matching is always case sensitive. For instance, given a DataFrame containing orders at a coffee shop: >>> df = pd.DataFrame({ ... \"customer\": [\"Mary\", \"Tom\", \"Lila\"], ... \"order\": [\"ice coffee\", \"lemonade\", \"regular coffee\"] ... }) >>> df customer order 0 Mary ice coffee 1 Tom lemonade 2 Lila regular coffee Our task is to replace values `ice coffee` and `regular coffee` of the `order` column into `latte`. Example 1 - exact matching (functional usage): >>> df = find_replace( ... df, ... match=\"exact\", ... order={\"ice coffee\": \"latte\", \"regular coffee\": \"latte\"}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte Example 1 - exact matching (method chaining): >>> df = df.find_replace( ... match=\"exact\", ... order={\"ice coffee\": \"latte\", \"regular coffee\": \"latte\"}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte Example 2 - Regular-expression-based matching (functional usage): >>> df = find_replace( ... df, ... match='regex', ... order={'coffee$': 'latte'}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte Example 2 - Regular-expression-based matching (method chaining usage): >>> df = df.find_replace( ... match='regex', ... order={'coffee$': 'latte'}, ... ) >>> df customer order 0 Mary latte 1 Tom lemonade 2 Lila latte To perform a find and replace on the entire DataFrame, pandas' `df.replace()` function provides the appropriate functionality. You can find more detail on the [replace] docs. [replace]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html This function only works with column names that have no spaces or punctuation in them. For example, a column name `item_name` would work with `find_replace`, because it is a contiguous string that can be parsed correctly, but `item name` would not be parsed correctly by the Python interpreter. If you have column names that might not be compatible, we recommend calling on `clean_names()` as the first method call. If, for whatever reason, that is not possible, then `_find_replace` is available as a function that you can do a pandas [pipe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pipe.html) call on. :param df: A pandas DataFrame. :param match: Whether or not to perform an exact match or not. Valid values are \"exact\" or \"regex\". :param mappings: keyword arguments corresponding to column names that have dictionaries passed in indicating what to find (keys) and what to replace with (values). :returns: A pandas DataFrame with replaced values. \"\"\" # noqa: E501 for column_name, mapper in mappings.items(): df = _find_replace(df, column_name, mapper, match=match) return df","title":"find_replace()"},{"location":"api/functions/#janitor.functions.flag_nulls","text":"","title":"flag_nulls"},{"location":"api/functions/#janitor.functions.flag_nulls.flag_nulls","text":"Creates a new column to indicate whether you have null values in a given row. If the columns parameter is not set, looks across the entire DataFrame, otherwise will look only in the columns you set. This method does not mutate the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [\"w\", \"x\", None, \"z\"], \"b\": [5, None, 7, 8], ... }) >>> df.flag_nulls() a b null_flag 0 w 5.0 0 1 x NaN 1 2 None 7.0 1 3 z 8.0 0 >>> df.flag_nulls(columns=\"b\") a b null_flag 0 w 5.0 0 1 x NaN 1 2 None 7.0 0 3 z 8.0 0 Parameters: Name Type Description Default df DataFrame Input pandas DataFrame. required column_name Optional[Hashable] Name for the output column. 'null_flag' columns Union[str, Iterable[str], Hashable] List of columns to look at for finding null values. If you only want to look at one column, you can simply give its name. If set to None (default), all DataFrame columns are used. None Returns: Type Description DataFrame Input dataframe with the null flag column. Exceptions: Type Description ValueError if column_name is already present in the DataFrame. ValueError if any column within columns is not present in the DataFrame. Source code in janitor/functions/flag_nulls.py @pf.register_dataframe_method def flag_nulls( df: pd.DataFrame, column_name: Optional[Hashable] = \"null_flag\", columns: Optional[Union[str, Iterable[str], Hashable]] = None, ) -> pd.DataFrame: \"\"\"Creates a new column to indicate whether you have null values in a given row. If the columns parameter is not set, looks across the entire DataFrame, otherwise will look only in the columns you set. This method does not mutate the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [\"w\", \"x\", None, \"z\"], \"b\": [5, None, 7, 8], ... }) >>> df.flag_nulls() a b null_flag 0 w 5.0 0 1 x NaN 1 2 None 7.0 1 3 z 8.0 0 >>> df.flag_nulls(columns=\"b\") a b null_flag 0 w 5.0 0 1 x NaN 1 2 None 7.0 0 3 z 8.0 0 :param df: Input pandas DataFrame. :param column_name: Name for the output column. :param columns: List of columns to look at for finding null values. If you only want to look at one column, you can simply give its name. If set to None (default), all DataFrame columns are used. :returns: Input dataframe with the null flag column. :raises ValueError: if `column_name` is already present in the DataFrame. :raises ValueError: if any column within `columns` is not present in the DataFrame. <!-- # noqa: DAR402 --> \"\"\" # Sort out columns input if isinstance(columns, str): columns = [columns] elif columns is None: columns = df.columns elif not isinstance(columns, Iterable): # catches other hashable types columns = [columns] # Input sanitation checks check_column(df, columns) check_column(df, [column_name], present=False) # This algorithm works best for n_rows >> n_cols. See issue #501 null_array = np.zeros(len(df)) for col in columns: null_array = np.logical_or(null_array, pd.isna(df[col])) df = df.copy() df[column_name] = null_array.astype(int) return df","title":"flag_nulls()"},{"location":"api/functions/#janitor.functions.get_dupes","text":"Implementation of the get_dupes function","title":"get_dupes"},{"location":"api/functions/#janitor.functions.get_dupes.get_dupes","text":"Return all duplicate rows. This method does not mutate the original DataFrame. Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"], ... \"quantity\": [100, 100, 75, 200, 75], ... }) >>> df item quantity 0 shoe 100 1 shoe 100 2 bag 75 3 shoe 200 4 bag 75 >>> df.get_dupes() item quantity 0 shoe 100 1 shoe 100 2 bag 75 4 bag 75 Optional column_names usage: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"], ... \"quantity\": [100, 100, 75, 200, 75], ... }) >>> df item quantity 0 shoe 100 1 shoe 100 2 bag 75 3 shoe 200 4 bag 75 >>> df.get_dupes(column_names=[\"item\"]) item quantity 0 shoe 100 1 shoe 100 2 bag 75 3 shoe 200 4 bag 75 >>> df.get_dupes(column_names=[\"quantity\"]) item quantity 0 shoe 100 1 shoe 100 2 bag 75 4 bag 75 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] (optional) A column name or an iterable (list or tuple) of column names. Following pandas API, this only considers certain columns for identifying duplicates. Defaults to using all columns. None Returns: Type Description DataFrame The duplicate rows, as a pandas DataFrame. Source code in janitor/functions/get_dupes.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def get_dupes( df: pd.DataFrame, column_names: Optional[Union[str, Iterable[str], Hashable]] = None, ) -> pd.DataFrame: \"\"\" Return all duplicate rows. This method does not mutate the original DataFrame. Method chaining syntax: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"], ... \"quantity\": [100, 100, 75, 200, 75], ... }) >>> df item quantity 0 shoe 100 1 shoe 100 2 bag 75 3 shoe 200 4 bag 75 >>> df.get_dupes() item quantity 0 shoe 100 1 shoe 100 2 bag 75 4 bag 75 Optional `column_names` usage: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"], ... \"quantity\": [100, 100, 75, 200, 75], ... }) >>> df item quantity 0 shoe 100 1 shoe 100 2 bag 75 3 shoe 200 4 bag 75 >>> df.get_dupes(column_names=[\"item\"]) item quantity 0 shoe 100 1 shoe 100 2 bag 75 3 shoe 200 4 bag 75 >>> df.get_dupes(column_names=[\"quantity\"]) item quantity 0 shoe 100 1 shoe 100 2 bag 75 4 bag 75 :param df: The pandas DataFrame object. :param column_names: (optional) A column name or an iterable (list or tuple) of column names. Following pandas API, this only considers certain columns for identifying duplicates. Defaults to using all columns. :returns: The duplicate rows, as a pandas DataFrame. \"\"\" dupes = df.duplicated(subset=column_names, keep=False) return df[dupes == True] # noqa: E712","title":"get_dupes()"},{"location":"api/functions/#janitor.functions.groupby_agg","text":"","title":"groupby_agg"},{"location":"api/functions/#janitor.functions.groupby_agg.groupby_agg","text":"Shortcut for assigning a groupby-transform to a new column. This method does not mutate the original DataFrame. Intended to be the method-chaining equivalent of: df = df.assign(...=df.groupby(...)[...].transform(...)) Example: Basic usage. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"], ... \"quantity\": [100, 120, 75, 200, 25], ... }) >>> df.groupby_agg( ... by=\"item\", ... agg=\"mean\", ... agg_column_name=\"quantity\", ... new_column_name=\"avg_quantity\", ... ) item quantity avg_quantity 0 shoe 100 140.0 1 shoe 120 140.0 2 bag 75 50.0 3 shoe 200 140.0 4 bag 25 50.0 Example: Set dropna=False to compute the aggregation, treating the null values in the by column as an isolated \"group\". >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"x\": [\"a\", \"a\", None, \"b\"], \"y\": [9, 9, 9, 9], ... }) >>> df.groupby_agg( ... by=\"x\", ... agg=\"count\", ... agg_column_name=\"y\", ... new_column_name=\"y_count\", ... dropna=False, ... ) x y y_count 0 a 9 2 1 a 9 2 2 None 9 1 3 b 9 1 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required by Union[List, Callable, str] Column(s) to groupby on, will be passed into DataFrame.groupby . required new_column_name str Name of the aggregation output column. required agg_column_name str Name of the column to aggregate over. required agg Union[Callable, str] How to aggregate. required dropna bool Whether or not to include null values, if present in the by column(s). Default is True (null values in by are assigned NaN in the new column). True Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/groupby_agg.py @pf.register_dataframe_method @deprecated_alias(new_column=\"new_column_name\", agg_column=\"agg_column_name\") def groupby_agg( df: pd.DataFrame, by: Union[List, Callable, str], new_column_name: str, agg_column_name: str, agg: Union[Callable, str], dropna: bool = True, ) -> pd.DataFrame: \"\"\"Shortcut for assigning a groupby-transform to a new column. This method does not mutate the original DataFrame. Intended to be the method-chaining equivalent of: ```python df = df.assign(...=df.groupby(...)[...].transform(...)) ``` Example: Basic usage. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"], ... \"quantity\": [100, 120, 75, 200, 25], ... }) >>> df.groupby_agg( ... by=\"item\", ... agg=\"mean\", ... agg_column_name=\"quantity\", ... new_column_name=\"avg_quantity\", ... ) item quantity avg_quantity 0 shoe 100 140.0 1 shoe 120 140.0 2 bag 75 50.0 3 shoe 200 140.0 4 bag 25 50.0 Example: Set `dropna=False` to compute the aggregation, treating the null values in the `by` column as an isolated \"group\". >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"x\": [\"a\", \"a\", None, \"b\"], \"y\": [9, 9, 9, 9], ... }) >>> df.groupby_agg( ... by=\"x\", ... agg=\"count\", ... agg_column_name=\"y\", ... new_column_name=\"y_count\", ... dropna=False, ... ) x y y_count 0 a 9 2 1 a 9 2 2 None 9 1 3 b 9 1 :param df: A pandas DataFrame. :param by: Column(s) to groupby on, will be passed into `DataFrame.groupby`. :param new_column_name: Name of the aggregation output column. :param agg_column_name: Name of the column to aggregate over. :param agg: How to aggregate. :param dropna: Whether or not to include null values, if present in the `by` column(s). Default is True (null values in `by` are assigned NaN in the new column). :returns: A pandas DataFrame. \"\"\" # noqa: E501 return df.assign( **{ new_column_name: df.groupby(by, dropna=dropna)[ agg_column_name ].transform(agg), } )","title":"groupby_agg()"},{"location":"api/functions/#janitor.functions.groupby_topk","text":"Implementation of the groupby_topk function","title":"groupby_topk"},{"location":"api/functions/#janitor.functions.groupby_topk.groupby_topk","text":"Return top k rows from a groupby of a set of columns. Returns a DataFrame that has the top k values per column , grouped by by . Under the hood it uses nlargest/nsmallest , for numeric columns, which avoids sorting the entire dataframe, and is usually more performant. For non-numeric columns, pd.sort_values is used. No sorting is done to the by column(s); the order is maintained in the final output. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"age\": [20, 23, 22, 43, 21], ... \"id\": [1, 4, 6, 2, 5], ... \"result\": [\"pass\", \"pass\", \"fail\", \"pass\", \"fail\"], ... } ... ) >>> df age id result 0 20 1 pass 1 23 4 pass 2 22 6 fail 3 43 2 pass 4 21 5 fail Ascending top 3: >>> df.groupby_topk(by=\"result\", column=\"age\", k=3) age id result 0 20 1 pass 1 23 4 pass 2 43 2 pass 3 21 5 fail 4 22 6 fail Descending top 2: >>> df.groupby_topk( ... by=\"result\", column=\"age\", k=2, ascending=False, ignore_index=False ... ) age id result 3 43 2 pass 1 23 4 pass 2 22 6 fail 4 21 5 fail Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required by Union[list, Hashable] Column name(s) to group input DataFrame df by. required column Hashable Name of the column that determines k rows to return. required k int Number of top rows to return for each group. required dropna bool If True , and NA values exist in by , the NA values are not used in the groupby computation to get the relevant k rows. If False , and NA values exist in by , then the NA values are used in the groupby computation to get the relevant k rows. The default is True . True ascending bool Default is True . If True , the smallest top k rows, determined by column are returned; if False, the largest top k rows, determined by column` are returned. True ignore_index bool Default True . If True , the original index is ignored. If False , the original index for the top k rows is retained. True Returns: Type Description DataFrame A pandas DataFrame with top k rows per column , grouped by by . Exceptions: Type Description ValueError if k is less than 1. Source code in janitor/functions/groupby_topk.py @pf.register_dataframe_method @deprecated_alias(groupby_column_name=\"by\", sort_column_name=\"column\") def groupby_topk( df: pd.DataFrame, by: Union[list, Hashable], column: Hashable, k: int, dropna: bool = True, ascending: bool = True, ignore_index: bool = True, ) -> pd.DataFrame: \"\"\" Return top `k` rows from a groupby of a set of columns. Returns a DataFrame that has the top `k` values per `column`, grouped by `by`. Under the hood it uses `nlargest/nsmallest`, for numeric columns, which avoids sorting the entire dataframe, and is usually more performant. For non-numeric columns, `pd.sort_values` is used. No sorting is done to the `by` column(s); the order is maintained in the final output. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"age\": [20, 23, 22, 43, 21], ... \"id\": [1, 4, 6, 2, 5], ... \"result\": [\"pass\", \"pass\", \"fail\", \"pass\", \"fail\"], ... } ... ) >>> df age id result 0 20 1 pass 1 23 4 pass 2 22 6 fail 3 43 2 pass 4 21 5 fail Ascending top 3: >>> df.groupby_topk(by=\"result\", column=\"age\", k=3) age id result 0 20 1 pass 1 23 4 pass 2 43 2 pass 3 21 5 fail 4 22 6 fail Descending top 2: >>> df.groupby_topk( ... by=\"result\", column=\"age\", k=2, ascending=False, ignore_index=False ... ) age id result 3 43 2 pass 1 23 4 pass 2 22 6 fail 4 21 5 fail :param df: A pandas DataFrame. :param by: Column name(s) to group input DataFrame `df` by. :param column: Name of the column that determines `k` rows to return. :param k: Number of top rows to return for each group. :param dropna: If `True`, and `NA` values exist in `by`, the `NA` values are not used in the groupby computation to get the relevant `k` rows. If `False`, and `NA` values exist in `by`, then the `NA` values are used in the groupby computation to get the relevant `k` rows. The default is `True`. :param ascending: Default is `True`. If `True`, the smallest top `k` rows, determined by `column` are returned; if `False, the largest top `k` rows, determined by `column` are returned. :param ignore_index: Default `True`. If `True`, the original index is ignored. If `False`, the original index for the top `k` rows is retained. :returns: A pandas DataFrame with top `k` rows per `column`, grouped by `by`. :raises ValueError: if `k` is less than 1. \"\"\" # noqa: E501 if isinstance(by, Hashable): by = [by] check(\"by\", by, [Hashable, list]) check_column(df, [column]) check_column(df, by) if k < 1: raise ValueError( \"Numbers of rows per group \" \"to be returned must be greater than 0.\" ) indices = df.groupby(by=by, dropna=dropna, sort=False, observed=True) indices = indices[column] try: if ascending: indices = indices.nsmallest(n=k) else: indices = indices.nlargest(n=k) except TypeError: indices = indices.apply( lambda d: d.sort_values(ascending=ascending).head(k) ) indices = indices.index.get_level_values(-1) if ignore_index: return df.loc[indices].reset_index(drop=True) return df.loc[indices]","title":"groupby_topk()"},{"location":"api/functions/#janitor.functions.impute","text":"Implementation of impute function","title":"impute"},{"location":"api/functions/#janitor.functions.impute.impute","text":"Method-chainable imputation of values in a column. This method mutates the original DataFrame. Underneath the hood, this function calls the .fillna() method available to every pandas.Series object. Either one of value or statistic_column_name should be provided. If value is provided, then all null values in the selected column will take on the value provided. If statistic_column_name is provided, then all null values in the selected column will take on the summary statistic value of other non-null values. Currently supported statistics include: mean (also aliased by average ) median mode minimum (also aliased by min ) maximum (also aliased by max ) Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [1, 2, 3], ... \"sales\": np.nan, ... \"score\": [np.nan, 3, 2], ... }) >>> df a sales score 0 1 NaN NaN 1 2 NaN 3.0 2 3 NaN 2.0 Imputing null values with 0 (using the value parameter): >>> df.impute(column_name=\"sales\", value=0.0) a sales score 0 1 0.0 NaN 1 2 0.0 3.0 2 3 0.0 2.0 Imputing null values with median (using the statistic_column_name parameter): >>> df.impute(column_name=\"score\", statistic_column_name=\"median\") a sales score 0 1 0.0 2.5 1 2 0.0 3.0 2 3 0.0 2.0 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The name of the column on which to impute values. required value Optional[Any] The value used for imputation, passed into .fillna method of the underlying pandas Series. None statistic_column_name Optional[str] The column statistic to impute. None Returns: Type Description DataFrame An imputed pandas DataFrame. Exceptions: Type Description ValueError If both value and statistic_column_name are provided. KeyError If statistic_column_name is not one of mean , average , median , mode , minimum , min , maximum , or max . Source code in janitor/functions/impute.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") @deprecated_alias(statistic=\"statistic_column_name\") def impute( df: pd.DataFrame, column_name: Hashable, value: Optional[Any] = None, statistic_column_name: Optional[str] = None, ) -> pd.DataFrame: \"\"\" Method-chainable imputation of values in a column. This method mutates the original DataFrame. Underneath the hood, this function calls the `.fillna()` method available to every `pandas.Series` object. Either one of `value` or `statistic_column_name` should be provided. If `value` is provided, then all null values in the selected column will take on the value provided. If `statistic_column_name` is provided, then all null values in the selected column will take on the summary statistic value of other non-null values. Currently supported statistics include: - `mean` (also aliased by `average`) - `median` - `mode` - `minimum` (also aliased by `min`) - `maximum` (also aliased by `max`) Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [1, 2, 3], ... \"sales\": np.nan, ... \"score\": [np.nan, 3, 2], ... }) >>> df a sales score 0 1 NaN NaN 1 2 NaN 3.0 2 3 NaN 2.0 Imputing null values with 0 (using the `value` parameter): >>> df.impute(column_name=\"sales\", value=0.0) a sales score 0 1 0.0 NaN 1 2 0.0 3.0 2 3 0.0 2.0 Imputing null values with median (using the `statistic_column_name` parameter): >>> df.impute(column_name=\"score\", statistic_column_name=\"median\") a sales score 0 1 0.0 2.5 1 2 0.0 3.0 2 3 0.0 2.0 :param df: A pandas DataFrame. :param column_name: The name of the column on which to impute values. :param value: The value used for imputation, passed into `.fillna` method of the underlying pandas Series. :param statistic_column_name: The column statistic to impute. :returns: An imputed pandas DataFrame. :raises ValueError: If both `value` and `statistic_column_name` are provided. :raises KeyError: If `statistic_column_name` is not one of `mean`, `average`, `median`, `mode`, `minimum`, `min`, `maximum`, or `max`. \"\"\" # Firstly, we check that only one of `value` or `statistic` are provided. if value is not None and statistic_column_name is not None: raise ValueError( \"Only one of `value` or `statistic_column_name` should be \" \"provided.\" ) # If statistic is provided, then we compute the relevant summary statistic # from the other data. funcs = { \"mean\": np.mean, \"average\": np.mean, # aliased \"median\": np.median, \"mode\": ss.mode, \"minimum\": np.min, \"min\": np.min, # aliased \"maximum\": np.max, \"max\": np.max, # aliased } if statistic_column_name is not None: # Check that the statistic keyword argument is one of the approved. if statistic_column_name not in funcs: raise KeyError( f\"`statistic_column_name` must be one of {funcs.keys()}.\" ) value = funcs[statistic_column_name]( df[column_name].dropna().to_numpy() ) # special treatment for mode, because scipy stats mode returns a # moderesult object. if statistic_column_name == \"mode\": value = value.mode[0] # The code is architected this way - if `value` is not provided but # statistic is, we then overwrite the None value taken on by `value`, and # use it to set the imputation column. if value is not None: df[column_name] = df[column_name].fillna(value) return df","title":"impute()"},{"location":"api/functions/#janitor.functions.jitter","text":"Implementation of the jitter function.","title":"jitter"},{"location":"api/functions/#janitor.functions.jitter.jitter","text":"Adds Gaussian noise (jitter) to the values of a column. Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [3, 4, 5, np.nan]}) >>> df a 0 3.0 1 4.0 2 5.0 3 NaN >>> df.jitter(\"a\", dest_column_name=\"a_jit\", scale=1, random_state=42) a a_jit 0 3.0 3.496714 1 4.0 3.861736 2 5.0 5.647689 3 NaN NaN A new column will be created containing the values of the original column with Gaussian noise added. For each value in the column, a Gaussian distribution is created having a location (mean) equal to the value and a scale (standard deviation) equal to scale . A random value is then sampled from this distribution, which is the jittered value. If a tuple is supplied for clip , then any values of the new column less than clip[0] will be set to clip[0] , and any values greater than clip[1] will be set to clip[1] . Additionally, if a numeric value is supplied for random_state , this value will be used to set the random seed used for sampling. NaN values are ignored in this method. This method mutates the original DataFrame. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Name of the column containing values to add Gaussian jitter to. required dest_column_name str The name of the new column containing the jittered values that will be created. required scale number A positive value multiplied by the original column value to determine the scale (standard deviation) of the Gaussian distribution to sample from. (A value of zero results in no jittering.) required clip Optional[Iterable[numpy.number]] An iterable of two values (minimum and maximum) to clip the jittered values to, default to None. None random_state Optional[numpy.number] An integer or 1-d array value used to set the random seed, default to None. None Returns: Type Description DataFrame A pandas DataFrame with a new column containing Gaussian-jittered values from another column. Exceptions: Type Description TypeError If column_name is not numeric. ValueError If scale is not a numerical value greater than 0 . ValueError If clip is not an iterable of length 2 . ValueError If clip[0] is greater than clip[1] . Source code in janitor/functions/jitter.py @pf.register_dataframe_method def jitter( df: pd.DataFrame, column_name: Hashable, dest_column_name: str, scale: np.number, clip: Optional[Iterable[np.number]] = None, random_state: Optional[np.number] = None, ) -> pd.DataFrame: \"\"\" Adds Gaussian noise (jitter) to the values of a column. Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [3, 4, 5, np.nan]}) >>> df a 0 3.0 1 4.0 2 5.0 3 NaN >>> df.jitter(\"a\", dest_column_name=\"a_jit\", scale=1, random_state=42) a a_jit 0 3.0 3.496714 1 4.0 3.861736 2 5.0 5.647689 3 NaN NaN A new column will be created containing the values of the original column with Gaussian noise added. For each value in the column, a Gaussian distribution is created having a location (mean) equal to the value and a scale (standard deviation) equal to `scale`. A random value is then sampled from this distribution, which is the jittered value. If a tuple is supplied for `clip`, then any values of the new column less than `clip[0]` will be set to `clip[0]`, and any values greater than `clip[1]` will be set to `clip[1]`. Additionally, if a numeric value is supplied for `random_state`, this value will be used to set the random seed used for sampling. NaN values are ignored in this method. This method mutates the original DataFrame. :param df: A pandas DataFrame. :param column_name: Name of the column containing values to add Gaussian jitter to. :param dest_column_name: The name of the new column containing the jittered values that will be created. :param scale: A positive value multiplied by the original column value to determine the scale (standard deviation) of the Gaussian distribution to sample from. (A value of zero results in no jittering.) :param clip: An iterable of two values (minimum and maximum) to clip the jittered values to, default to None. :param random_state: An integer or 1-d array value used to set the random seed, default to None. :returns: A pandas DataFrame with a new column containing Gaussian-jittered values from another column. :raises TypeError: If `column_name` is not numeric. :raises ValueError: If `scale` is not a numerical value greater than `0`. :raises ValueError: If `clip` is not an iterable of length `2`. :raises ValueError: If `clip[0]` is greater than `clip[1]`. \"\"\" # Check types check(\"scale\", scale, [int, float]) # Check that `column_name` is a numeric column if not np.issubdtype(df[column_name].dtype, np.number): raise TypeError(f\"{column_name} must be a numeric column.\") if scale <= 0: raise ValueError(\"`scale` must be a numeric value greater than 0.\") values = df[column_name] if random_state is not None: np.random.seed(random_state) result = np.random.normal(loc=values, scale=scale) if clip: # Ensure `clip` has length 2 if len(clip) != 2: raise ValueError(\"`clip` must be an iterable of length 2.\") # Ensure the values in `clip` are ordered as min, max if clip[1] < clip[0]: raise ValueError( \"`clip[0]` must be less than or equal to `clip[1]`.\" ) result = np.clip(result, *clip) df[dest_column_name] = result return df","title":"jitter()"},{"location":"api/functions/#janitor.functions.join_apply","text":"Implementation of the join_apply function","title":"join_apply"},{"location":"api/functions/#janitor.functions.join_apply.join_apply","text":"Join the result of applying a function across dataframe rows. This method does not mutate the original DataFrame. This is a convenience function that allows us to apply arbitrary functions that take any combination of information from any of the columns. The only requirement is that the function signature takes in a row from the DataFrame. Example: Sum the result of two columns into a new column. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\":[1, 2, 3], \"b\": [2, 3, 4]}) >>> df a b 0 1 2 1 2 3 2 3 4 >>> df.join_apply( ... func=lambda x: 2 * x[\"a\"] + x[\"b\"], ... new_column_name=\"2a+b\", ... ) a b 2a+b 0 1 2 4 1 2 3 7 2 3 4 10 Example: Incorporating conditionals in func . >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [20, 30, 40]}) >>> df a b 0 1 20 1 2 30 2 3 40 >>> def take_a_if_even(x): ... if x[\"a\"] % 2 == 0: ... return x[\"a\"] ... else: ... return x[\"b\"] >>> df.join_apply(take_a_if_even, \"a_if_even\") a b a_if_even 0 1 20 20 1 2 30 2 2 3 40 40 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required func Callable A function that is applied elementwise across all rows of the DataFrame. required new_column_name str Name of the resulting column. required Returns: Type Description DataFrame A pandas DataFrame with new column appended. Source code in janitor/functions/join_apply.py @pf.register_dataframe_method def join_apply( df: pd.DataFrame, func: Callable, new_column_name: str, ) -> pd.DataFrame: \"\"\" Join the result of applying a function across dataframe rows. This method does not mutate the original DataFrame. This is a convenience function that allows us to apply arbitrary functions that take any combination of information from any of the columns. The only requirement is that the function signature takes in a row from the DataFrame. Example: Sum the result of two columns into a new column. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\":[1, 2, 3], \"b\": [2, 3, 4]}) >>> df a b 0 1 2 1 2 3 2 3 4 >>> df.join_apply( ... func=lambda x: 2 * x[\"a\"] + x[\"b\"], ... new_column_name=\"2a+b\", ... ) a b 2a+b 0 1 2 4 1 2 3 7 2 3 4 10 Example: Incorporating conditionals in `func`. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [20, 30, 40]}) >>> df a b 0 1 20 1 2 30 2 3 40 >>> def take_a_if_even(x): ... if x[\"a\"] % 2 == 0: ... return x[\"a\"] ... else: ... return x[\"b\"] >>> df.join_apply(take_a_if_even, \"a_if_even\") a b a_if_even 0 1 20 20 1 2 30 2 2 3 40 40 :param df: A pandas DataFrame. :param func: A function that is applied elementwise across all rows of the DataFrame. :param new_column_name: Name of the resulting column. :returns: A pandas DataFrame with new column appended. \"\"\" df = df.copy().join(df.apply(func, axis=1).rename(new_column_name)) return df","title":"join_apply()"},{"location":"api/functions/#janitor.functions.label_encode","text":"Implementation of label_encode function","title":"label_encode"},{"location":"api/functions/#janitor.functions.label_encode.label_encode","text":"Convert labels into numerical data. This method will create a new column with the string _enc appended after the original column's name. Consider this to be syntactic sugar. This function uses the factorize pandas function under the hood. This method behaves differently from encode_categorical . This method creates a new column of numeric data. encode_categorical replaces the dtype of the original column with a categorical dtype. This method mutates the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df foo bar 0 b 4 1 b 5 2 a 6 3 c 7 4 b 8 >>> df.label_encode(column_names=\"foo\") foo bar foo_enc 0 b 4 0 1 b 5 0 2 a 6 1 3 c 7 2 4 b 8 0 Note This function will be deprecated in a 1.x release. Please use factorize_columns instead. Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] A column name or an iterable (list or tuple) of column names. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/label_encode.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def label_encode( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable], ) -> pd.DataFrame: \"\"\" Convert labels into numerical data. This method will create a new column with the string `_enc` appended after the original column's name. Consider this to be syntactic sugar. This function uses the `factorize` pandas function under the hood. This method behaves differently from [`encode_categorical`][janitor.functions.encode_categorical.encode_categorical]. This method creates a new column of numeric data. [`encode_categorical`][janitor.functions.encode_categorical.encode_categorical] replaces the dtype of the original column with a *categorical* dtype. This method mutates the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"], ... \"bar\": range(4, 9), ... }) >>> df foo bar 0 b 4 1 b 5 2 a 6 3 c 7 4 b 8 >>> df.label_encode(column_names=\"foo\") foo bar foo_enc 0 b 4 0 1 b 5 0 2 a 6 1 3 c 7 2 4 b 8 0 !!!note This function will be deprecated in a 1.x release. Please use [`factorize_columns`][janitor.functions.factorize_columns.factorize_columns] instead. :param df: The pandas DataFrame object. :param column_names: A column name or an iterable (list or tuple) of column names. :returns: A pandas DataFrame. \"\"\" # noqa: E501 warnings.warn( \"`label_encode` will be deprecated in a 1.x release. \" \"Please use `factorize_columns` instead.\" ) df = _factorize(df, column_names, \"_enc\") return df","title":"label_encode()"},{"location":"api/functions/#janitor.functions.limit_column_characters","text":"Implementation of limit_column_characters.","title":"limit_column_characters"},{"location":"api/functions/#janitor.functions.limit_column_characters.limit_column_characters","text":"Truncate column sizes to a specific length. This method mutates the original DataFrame. Method chaining will truncate all columns to a given length and append a given separator character with the index of duplicate columns, except for the first distinct column name. Example: >>> import pandas as pd >>> import janitor >>> data_dict = { ... \"really_long_name\": [9, 8, 7], ... \"another_really_long_name\": [2, 4, 6], ... \"another_really_longer_name\": list(\"xyz\"), ... \"this_is_getting_out_of_hand\": list(\"pqr\"), ... } >>> df = pd.DataFrame(data_dict) >>> df # doctest: +SKIP really_long_name another_really_long_name another_really_longer_name this_is_getting_out_of_hand 0 9 2 x p 1 8 4 y q 2 7 6 z r >>> df.limit_column_characters(7) really_ another another_1 this_is 0 9 2 x p 1 8 4 y q 2 7 6 z r Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_length int Character length for which to truncate all columns. The column separator value and number for duplicate column name does not contribute. Therefore, if all columns are truncated to 10 characters, the first distinct column will be 10 characters and the remaining will be 12 characters (assuming a column separator of one character). required col_separator str The separator to use for counting distinct column values, for example, '_' or '.' . Supply an empty string (i.e. '' ) to remove the separator. '_' Returns: Type Description DataFrame A pandas DataFrame with truncated column lengths. Source code in janitor/functions/limit_column_characters.py @pf.register_dataframe_method def limit_column_characters( df: pd.DataFrame, column_length: int, col_separator: str = \"_\", ) -> pd.DataFrame: \"\"\"Truncate column sizes to a specific length. This method mutates the original DataFrame. Method chaining will truncate all columns to a given length and append a given separator character with the index of duplicate columns, except for the first distinct column name. Example: >>> import pandas as pd >>> import janitor >>> data_dict = { ... \"really_long_name\": [9, 8, 7], ... \"another_really_long_name\": [2, 4, 6], ... \"another_really_longer_name\": list(\"xyz\"), ... \"this_is_getting_out_of_hand\": list(\"pqr\"), ... } >>> df = pd.DataFrame(data_dict) >>> df # doctest: +SKIP really_long_name another_really_long_name another_really_longer_name this_is_getting_out_of_hand 0 9 2 x p 1 8 4 y q 2 7 6 z r >>> df.limit_column_characters(7) really_ another another_1 this_is 0 9 2 x p 1 8 4 y q 2 7 6 z r :param df: A pandas DataFrame. :param column_length: Character length for which to truncate all columns. The column separator value and number for duplicate column name does not contribute. Therefore, if all columns are truncated to 10 characters, the first distinct column will be 10 characters and the remaining will be 12 characters (assuming a column separator of one character). :param col_separator: The separator to use for counting distinct column values, for example, `'_'` or `'.'`. Supply an empty string (i.e. `''`) to remove the separator. :returns: A pandas DataFrame with truncated column lengths. \"\"\" # noqa: E501 check(\"column_length\", column_length, [int]) check(\"col_separator\", col_separator, [str]) col_names = df.columns col_names = [col_name[:column_length] for col_name in col_names] col_name_set = set(col_names) col_name_count = {} # If no columns are duplicates, we can skip the loops below. if len(col_name_set) == len(col_names): df.columns = col_names return df for col_name_to_check in col_name_set: count = 0 for idx, col_name in enumerate(col_names): if col_name_to_check == col_name: col_name_count[idx] = count count += 1 final_col_names = [] for idx, col_name in enumerate(col_names): if col_name_count[idx] > 0: col_name_to_append = ( col_name + col_separator + str(col_name_count[idx]) ) final_col_names.append(col_name_to_append) else: final_col_names.append(col_name) df.columns = final_col_names return df","title":"limit_column_characters()"},{"location":"api/functions/#janitor.functions.min_max_scale","text":"","title":"min_max_scale"},{"location":"api/functions/#janitor.functions.min_max_scale.min_max_scale","text":"Scales DataFrame to between a minimum and maximum value. One can optionally set a new target minimum and maximum value using the feature_range keyword argument. If column_name is specified, then only that column(s) of data is scaled. Otherwise, the entire dataframe is scaled. If jointly is True , the column_names provided entire dataframe will be regnozied as the one to jointly scale. Otherwise, each column of data will be scaled separately. Example: Basic usage. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'a':[1, 2], 'b':[0, 1]}) >>> df.min_max_scale() a b 0 0.0 0.0 1 1.0 1.0 >>> df.min_max_scale(jointly=True) a b 0 0.5 0.0 1 1.0 0.5 Example: Setting custom minimum and maximum. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'a':[1, 2], 'b':[0, 1]}) >>> df.min_max_scale(feature_range=(0, 100)) a b 0 0.0 0.0 1 100.0 100.0 >>> df.min_max_scale(feature_range=(0, 100), jointly=True) a b 0 50.0 0.0 1 100.0 50.0 Example: Apply min-max to the selected columns. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'a':[1, 2], 'b':[0, 1], 'c': [1, 0]}) >>> df.min_max_scale( ... feature_range=(0, 100), ... column_name=[\"a\", \"c\"], ... ) a b c 0 0.0 0 100.0 1 100.0 1 0.0 >>> df.min_max_scale( ... feature_range=(0, 100), ... column_name=[\"a\", \"c\"], ... jointly=True, ... ) a b c 0 50.0 0 50.0 1 100.0 1 0.0 >>> df.min_max_scale(feature_range=(0, 100), column_name='a') a b c 0 0.0 0 1 1 100.0 1 0 The aforementioned example might be applied to something like scaling the isoelectric points of amino acids. While technically they range from approx 3-10, we can also think of them on the pH scale which ranges from 1 to 14. Hence, 3 gets scaled not to 0 but approx. 0.15 instead, while 10 gets scaled to approx. 0.69 instead. Version Changed 0.24.0 Deleted old_min , old_max , new_min , and new_max options. Added feature_range , and jointly options. Parameters: Name Type Description Default df pd.DataFrame A pandas DataFrame. required feature_range tuple[int | float, int | float] (optional) Desired range of transformed data. (0, 1) column_name str | int | list[str | int] | pd.Index (optional) The column on which to perform scaling. None jointly bool (bool) Scale the entire data if Ture. False Returns: Type Description pd.DataFrame A pandas DataFrame with scaled data. Exceptions: Type Description ValueError if feature_range isn't tuple type. ValueError if the length of feature_range isn't equal to two. ValueError if the element of feature_range isn't number type. ValueError if feature_range[1] <= feature_range[0] . Source code in janitor/functions/min_max_scale.py @pf.register_dataframe_method @deprecated_kwargs( \"old_min\", \"old_max\", \"new_min\", \"new_max\", message=( \"The keyword argument {argument!r} of {func_name!r} is deprecated. \" \"Please use 'feature_range' instead.\" ), ) @deprecated_alias(col_name=\"column_name\") def min_max_scale( df: pd.DataFrame, feature_range: tuple[int | float, int | float] = (0, 1), column_name: str | int | list[str | int] | pd.Index = None, jointly: bool = False, ) -> pd.DataFrame: \"\"\" Scales DataFrame to between a minimum and maximum value. One can optionally set a new target **minimum** and **maximum** value using the `feature_range` keyword argument. If `column_name` is specified, then only that column(s) of data is scaled. Otherwise, the entire dataframe is scaled. If `jointly` is `True`, the `column_names` provided entire dataframe will be regnozied as the one to jointly scale. Otherwise, each column of data will be scaled separately. Example: Basic usage. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'a':[1, 2], 'b':[0, 1]}) >>> df.min_max_scale() a b 0 0.0 0.0 1 1.0 1.0 >>> df.min_max_scale(jointly=True) a b 0 0.5 0.0 1 1.0 0.5 Example: Setting custom minimum and maximum. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'a':[1, 2], 'b':[0, 1]}) >>> df.min_max_scale(feature_range=(0, 100)) a b 0 0.0 0.0 1 100.0 100.0 >>> df.min_max_scale(feature_range=(0, 100), jointly=True) a b 0 50.0 0.0 1 100.0 50.0 Example: Apply min-max to the selected columns. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'a':[1, 2], 'b':[0, 1], 'c': [1, 0]}) >>> df.min_max_scale( ... feature_range=(0, 100), ... column_name=[\"a\", \"c\"], ... ) a b c 0 0.0 0 100.0 1 100.0 1 0.0 >>> df.min_max_scale( ... feature_range=(0, 100), ... column_name=[\"a\", \"c\"], ... jointly=True, ... ) a b c 0 50.0 0 50.0 1 100.0 1 0.0 >>> df.min_max_scale(feature_range=(0, 100), column_name='a') a b c 0 0.0 0 1 1 100.0 1 0 The aforementioned example might be applied to something like scaling the isoelectric points of amino acids. While technically they range from approx 3-10, we can also think of them on the pH scale which ranges from 1 to 14. Hence, 3 gets scaled not to 0 but approx. 0.15 instead, while 10 gets scaled to approx. 0.69 instead. !!! summary \"Version Changed\" - 0.24.0 - Deleted `old_min`, `old_max`, `new_min`, and `new_max` options. - Added `feature_range`, and `jointly` options. :param df: A pandas DataFrame. :param feature_range: (optional) Desired range of transformed data. :param column_name: (optional) The column on which to perform scaling. :param jointly: (bool) Scale the entire data if Ture. :returns: A pandas DataFrame with scaled data. :raises ValueError: if `feature_range` isn't tuple type. :raises ValueError: if the length of `feature_range` isn't equal to two. :raises ValueError: if the element of `feature_range` isn't number type. :raises ValueError: if `feature_range[1]` <= `feature_range[0]`. \"\"\" if not ( isinstance(feature_range, (tuple, list)) and len(feature_range) == 2 and all((isinstance(i, (int, float))) for i in feature_range) and feature_range[1] > feature_range[0] ): raise ValueError( \"`feature_range` should be a range type contains number element, \" \"the first element must be greater than the second one\" ) if column_name is not None: df = df.copy() # Avoid to change the original DataFrame. old_feature_range = df[column_name].pipe(_min_max_value, jointly) df[column_name] = df[column_name].pipe( _apply_min_max, *old_feature_range, *feature_range, ) else: old_feature_range = df.pipe(_min_max_value, jointly) df = df.pipe( _apply_min_max, *old_feature_range, *feature_range, ) return df","title":"min_max_scale()"},{"location":"api/functions/#janitor.functions.move","text":"Implementation of move.","title":"move"},{"location":"api/functions/#janitor.functions.move.move","text":"Moves a column or row to a position adjacent to another column or row in the dataframe. This operation does not reset the index of the dataframe. User must explicitly do so. This function does not apply to multilevel dataframes, and the dataframe must have unique column names or indices. Example: Moving a row >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [2, 4, 6, 8], \"b\": list(\"wxyz\")}) >>> df a b 0 2 w 1 4 x 2 6 y 3 8 z >>> df.move(source=0, target=3, position=\"before\", axis=0) a b 1 4 x 2 6 y 0 2 w 3 8 z Example: Moving a column >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [2, 4, 6], \"b\": [1, 3, 5], \"c\": [7, 8, 9]}) >>> df a b c 0 2 1 7 1 4 3 8 2 6 5 9 >>> df.move(source=\"a\", target=\"c\", position=\"after\", axis=1) b c a 0 1 7 2 1 3 8 4 2 5 9 6 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required source Union[int, str] Column or row to move. required target Union[int, str] Column or row to move adjacent to. required position str Specifies whether the Series is moved to before or after the adjacent Series. Values can be either before or after ; defaults to before . 'before' axis int Axis along which the function is applied. 0 to move a row, 1 to move a column. 0 Returns: Type Description DataFrame The dataframe with the Series moved. Exceptions: Type Description ValueError If axis is not 0 or 1 . ValueError If position is not before or after . ValueError If source row or column is not in dataframe. ValueError If target row or column is not in dataframe. Source code in janitor/functions/move.py @pf.register_dataframe_method def move( df: pd.DataFrame, source: Union[int, str], target: Union[int, str], position: str = \"before\", axis: int = 0, ) -> pd.DataFrame: \"\"\" Moves a column or row to a position adjacent to another column or row in the dataframe. This operation does not reset the index of the dataframe. User must explicitly do so. This function does not apply to multilevel dataframes, and the dataframe must have unique column names or indices. Example: Moving a row >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [2, 4, 6, 8], \"b\": list(\"wxyz\")}) >>> df a b 0 2 w 1 4 x 2 6 y 3 8 z >>> df.move(source=0, target=3, position=\"before\", axis=0) a b 1 4 x 2 6 y 0 2 w 3 8 z Example: Moving a column >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [2, 4, 6], \"b\": [1, 3, 5], \"c\": [7, 8, 9]}) >>> df a b c 0 2 1 7 1 4 3 8 2 6 5 9 >>> df.move(source=\"a\", target=\"c\", position=\"after\", axis=1) b c a 0 1 7 2 1 3 8 4 2 5 9 6 :param df: The pandas DataFrame object. :param source: Column or row to move. :param target: Column or row to move adjacent to. :param position: Specifies whether the Series is moved to before or after the adjacent Series. Values can be either `before` or `after`; defaults to `before`. :param axis: Axis along which the function is applied. 0 to move a row, 1 to move a column. :returns: The dataframe with the Series moved. :raises ValueError: If `axis` is not `0` or `1`. :raises ValueError: If `position` is not `before` or `after`. :raises ValueError: If `source` row or column is not in dataframe. :raises ValueError: If `target` row or column is not in dataframe. \"\"\" if axis not in [0, 1]: raise ValueError(f\"Invalid axis '{axis}'. Can only be 0 or 1.\") if position not in [\"before\", \"after\"]: raise ValueError( f\"Invalid position '{position}'. Can only be 'before' or 'after'.\" ) df = df.copy() if axis == 0: names = list(df.index) if source not in names: raise ValueError(f\"Source row '{source}' not in dataframe.\") if target not in names: raise ValueError(f\"Target row '{target}' not in dataframe.\") names.remove(source) pos = names.index(target) if position == \"after\": pos += 1 names.insert(pos, source) df = df.loc[names, :] else: names = list(df.columns) if source not in names: raise ValueError(f\"Source column '{source}' not in dataframe.\") if target not in names: raise ValueError(f\"Target column '{target}' not in dataframe.\") names.remove(source) pos = names.index(target) if position == \"after\": pos += 1 names.insert(pos, source) df = df.loc[:, names] return df","title":"move()"},{"location":"api/functions/#janitor.functions.pivot","text":"","title":"pivot"},{"location":"api/functions/#janitor.functions.pivot.pivot_longer","text":"Unpivots a DataFrame from wide to long format. This method does not mutate the original DataFrame. It is modeled after the pivot_longer function in R's tidyr package, and also takes inspiration from R's data.table package. This function is useful to massage a DataFrame into a format where one or more columns are considered measured variables, and all other columns are considered as identifier variables. All measured variables are unpivoted (and typically duplicated) along the row axis. Column selection in index and column_names is possible using the select_columns syntax. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"Sepal.Length\": [5.1, 5.9], ... \"Sepal.Width\": [3.5, 3.0], ... \"Petal.Length\": [1.4, 5.1], ... \"Petal.Width\": [0.2, 1.8], ... \"Species\": [\"setosa\", \"virginica\"], ... } ... ) >>> df Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 5.1 3.5 1.4 0.2 setosa 1 5.9 3.0 5.1 1.8 virginica Replicate pandas' melt: >>> df.pivot_longer(index = 'Species') Species variable value 0 setosa Sepal.Length 5.1 1 virginica Sepal.Length 5.9 2 setosa Sepal.Width 3.5 3 virginica Sepal.Width 3.0 4 setosa Petal.Length 1.4 5 virginica Petal.Length 5.1 6 setosa Petal.Width 0.2 7 virginica Petal.Width 1.8 Split the column labels into parts: >>> df.pivot_longer( ... index = 'Species', ... names_to = ('part', 'dimension'), ... names_sep = '.', ... sort_by_appearance = True, ... ) Species part dimension value 0 setosa Sepal Length 5.1 1 setosa Sepal Width 3.5 2 setosa Petal Length 1.4 3 setosa Petal Width 0.2 4 virginica Sepal Length 5.9 5 virginica Sepal Width 3.0 6 virginica Petal Length 5.1 7 virginica Petal Width 1.8 Retain parts of the column names as headers: >>> df.pivot_longer( ... index = 'Species', ... names_to = ('part', '.value'), ... names_sep = '.', ... sort_by_appearance = True, ... ) Species part Length Width 0 setosa Sepal 5.1 3.5 1 setosa Petal 1.4 0.2 2 virginica Sepal 5.9 3.0 3 virginica Petal 5.1 1.8 Split the column labels based on regex: >>> df = pd.DataFrame({\"id\": [1], \"new_sp_m5564\": [2], \"newrel_f65\": [3]}) >>> df id new_sp_m5564 newrel_f65 0 1 2 3 >>> df.pivot_longer( ... index = 'id', ... names_to = ('diagnosis', 'gender', 'age'), ... names_pattern = r\"new_?(.+)_(.)(\\d+)\", ... ) id diagnosis gender age value 0 1 sp m 5564 2 1 1 rel f 65 3 Convert the dtypes of specific columns with names_transform : >>> result = (df ... .pivot_longer( ... index = 'id', ... names_to = ('diagnosis', 'gender', 'age'), ... names_pattern = r\"new_?(.+)_(.)(\\d+)\", ... names_transform = {'gender': 'category', 'age':'int'}) ... ) >>> result.dtypes id int64 gender category age int64 value int64 dtype: object Use multiple .value to reshape dataframe: >>> df = pd.DataFrame( ... [ ... { ... \"x_1_mean\": 10, ... \"x_2_mean\": 20, ... \"y_1_mean\": 30, ... \"y_2_mean\": 40, ... \"unit\": 50, ... } ... ] ... ) >>> df x_1_mean x_2_mean y_1_mean y_2_mean unit 0 10 20 30 40 50 >>> df.pivot_longer( ... index=\"unit\", ... names_to=(\".value\", \"time\", \".value\"), ... names_pattern=r\"(x|y)_([0-9])(_mean)\", ... ) unit time x_mean y_mean 0 50 1 10 30 1 50 2 20 40 Multiple values_to: >>> df = pd.DataFrame( ... { ... \"City\": [\"Houston\", \"Austin\", \"Hoover\"], ... \"State\": [\"Texas\", \"Texas\", \"Alabama\"], ... \"Name\": [\"Aria\", \"Penelope\", \"Niko\"], ... \"Mango\": [4, 10, 90], ... \"Orange\": [10, 8, 14], ... \"Watermelon\": [40, 99, 43], ... \"Gin\": [16, 200, 34], ... \"Vodka\": [20, 33, 18], ... }, ... columns=[ ... \"City\", ... \"State\", ... \"Name\", ... \"Mango\", ... \"Orange\", ... \"Watermelon\", ... \"Gin\", ... \"Vodka\", ... ], ... ) >>> df City State Name Mango Orange Watermelon Gin Vodka 0 Houston Texas Aria 4 10 40 16 20 1 Austin Texas Penelope 10 8 99 200 33 2 Hoover Alabama Niko 90 14 43 34 18 >>> df.pivot_longer( ... index=[\"City\", \"State\"], ... column_names=slice(\"Mango\", \"Vodka\"), ... names_to=(\"Fruit\", \"Drink\"), ... values_to=(\"Pounds\", \"Ounces\"), ... names_pattern=[r\"M|O|W\", r\"G|V\"], ... ) City State Fruit Pounds Drink Ounces 0 Houston Texas Mango 4 Gin 16.0 1 Austin Texas Mango 10 Gin 200.0 2 Hoover Alabama Mango 90 Gin 34.0 3 Houston Texas Orange 10 Vodka 20.0 4 Austin Texas Orange 8 Vodka 33.0 5 Hoover Alabama Orange 14 Vodka 18.0 6 Houston Texas Watermelon 40 None NaN 7 Austin Texas Watermelon 99 None NaN 8 Hoover Alabama Watermelon 43 None NaN Version Changed 0.24.0 Added dropna parameter. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required index Union[list, tuple, str, Pattern] Name(s) of columns to use as identifier variables. Should be either a single column name, or a list/tuple of column names. index should be a list of tuples if the columns are a MultiIndex. None column_names Union[list, tuple, str, Pattern] Name(s) of columns to unpivot. Should be either a single column name or a list/tuple of column names. column_names should be a list of tuples if the columns are a MultiIndex. None names_to Union[list, tuple, str] Name of new column as a string that will contain what were previously the column names in column_names . The default is variable if no value is provided. It can also be a list/tuple of strings that will serve as new column names, if name_sep or names_pattern is provided. If .value is in names_to , new column names will be extracted from part of the existing column names and overrides values_to . None values_to Optional[str] Name of new column as a string that will contain what were previously the values of the columns in column_names . values_to can also be a list/tuple and requires that names_pattern is also a list/tuple. 'value' column_level Union[int, str] If columns are a MultiIndex, then use this level to unpivot the DataFrame. Provided for compatibility with pandas' melt, and applies only if neither names_sep nor names_pattern is provided. None names_sep Union[str, Pattern] Determines how the column name is broken up, if names_to contains multiple values. It takes the same specification as pandas' str.split method, and can be a string or regular expression. names_sep does not work with MultiIndex columns. None names_pattern Union[list, tuple, str, Pattern] Determines how the column name is broken up. It can be a regular expression containing matching groups (it takes the same specification as pandas' str.extract method), or a list/tuple of regular expressions. If it is a single regex, the number of groups must match the length of names_to . For a list/tuple of regular expressions, names_to must also be a list/tuple and the lengths of both arguments must match. names_pattern does not work with MultiIndex columns. None names_transform Union[str, Callable, dict] Use this option to change the types of columns that have been transformed to rows. This does not applies to the values' columns. Accepts any argument that is acceptable by pd.astype . None dropna bool Determines whether or not to drop nulls from the values columns. Default is False . False sort_by_appearance Optional[bool] Default False . Boolean value that determines the final look of the DataFrame. If True , the unpivoted DataFrame will be stacked in order of first appearance. False ignore_index Optional[bool] Default True . If True , the original index is ignored. If False , the original index is retained and the index labels will be repeated as necessary. True Returns: Type Description DataFrame A pandas DataFrame that has been unpivoted from wide to long format. Source code in janitor/functions/pivot.py @pf.register_dataframe_method def pivot_longer( df: pd.DataFrame, index: Optional[Union[list, tuple, str, Pattern]] = None, column_names: Optional[Union[list, tuple, str, Pattern]] = None, names_to: Optional[Union[list, tuple, str]] = None, values_to: Optional[str] = \"value\", column_level: Optional[Union[int, str]] = None, names_sep: Optional[Union[str, Pattern]] = None, names_pattern: Optional[Union[list, tuple, str, Pattern]] = None, names_transform: Optional[Union[str, Callable, dict]] = None, dropna: bool = False, sort_by_appearance: Optional[bool] = False, ignore_index: Optional[bool] = True, ) -> pd.DataFrame: \"\"\" Unpivots a DataFrame from *wide* to *long* format. This method does not mutate the original DataFrame. It is modeled after the `pivot_longer` function in R's tidyr package, and also takes inspiration from R's data.table package. This function is useful to massage a DataFrame into a format where one or more columns are considered measured variables, and all other columns are considered as identifier variables. All measured variables are *unpivoted* (and typically duplicated) along the row axis. Column selection in `index` and `column_names` is possible using the [`select_columns`][janitor.functions.select.select_columns] syntax. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"Sepal.Length\": [5.1, 5.9], ... \"Sepal.Width\": [3.5, 3.0], ... \"Petal.Length\": [1.4, 5.1], ... \"Petal.Width\": [0.2, 1.8], ... \"Species\": [\"setosa\", \"virginica\"], ... } ... ) >>> df Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 5.1 3.5 1.4 0.2 setosa 1 5.9 3.0 5.1 1.8 virginica Replicate pandas' melt: >>> df.pivot_longer(index = 'Species') Species variable value 0 setosa Sepal.Length 5.1 1 virginica Sepal.Length 5.9 2 setosa Sepal.Width 3.5 3 virginica Sepal.Width 3.0 4 setosa Petal.Length 1.4 5 virginica Petal.Length 5.1 6 setosa Petal.Width 0.2 7 virginica Petal.Width 1.8 Split the column labels into parts: >>> df.pivot_longer( ... index = 'Species', ... names_to = ('part', 'dimension'), ... names_sep = '.', ... sort_by_appearance = True, ... ) Species part dimension value 0 setosa Sepal Length 5.1 1 setosa Sepal Width 3.5 2 setosa Petal Length 1.4 3 setosa Petal Width 0.2 4 virginica Sepal Length 5.9 5 virginica Sepal Width 3.0 6 virginica Petal Length 5.1 7 virginica Petal Width 1.8 Retain parts of the column names as headers: >>> df.pivot_longer( ... index = 'Species', ... names_to = ('part', '.value'), ... names_sep = '.', ... sort_by_appearance = True, ... ) Species part Length Width 0 setosa Sepal 5.1 3.5 1 setosa Petal 1.4 0.2 2 virginica Sepal 5.9 3.0 3 virginica Petal 5.1 1.8 Split the column labels based on regex: >>> df = pd.DataFrame({\"id\": [1], \"new_sp_m5564\": [2], \"newrel_f65\": [3]}) >>> df id new_sp_m5564 newrel_f65 0 1 2 3 >>> df.pivot_longer( ... index = 'id', ... names_to = ('diagnosis', 'gender', 'age'), ... names_pattern = r\"new_?(.+)_(.)(\\\\d+)\", ... ) id diagnosis gender age value 0 1 sp m 5564 2 1 1 rel f 65 3 Convert the dtypes of specific columns with `names_transform`: >>> result = (df ... .pivot_longer( ... index = 'id', ... names_to = ('diagnosis', 'gender', 'age'), ... names_pattern = r\"new_?(.+)_(.)(\\\\d+)\", ... names_transform = {'gender': 'category', 'age':'int'}) ... ) >>> result.dtypes id int64 gender category age int64 value int64 dtype: object Use multiple `.value` to reshape dataframe: >>> df = pd.DataFrame( ... [ ... { ... \"x_1_mean\": 10, ... \"x_2_mean\": 20, ... \"y_1_mean\": 30, ... \"y_2_mean\": 40, ... \"unit\": 50, ... } ... ] ... ) >>> df x_1_mean x_2_mean y_1_mean y_2_mean unit 0 10 20 30 40 50 >>> df.pivot_longer( ... index=\"unit\", ... names_to=(\".value\", \"time\", \".value\"), ... names_pattern=r\"(x|y)_([0-9])(_mean)\", ... ) unit time x_mean y_mean 0 50 1 10 30 1 50 2 20 40 Multiple values_to: >>> df = pd.DataFrame( ... { ... \"City\": [\"Houston\", \"Austin\", \"Hoover\"], ... \"State\": [\"Texas\", \"Texas\", \"Alabama\"], ... \"Name\": [\"Aria\", \"Penelope\", \"Niko\"], ... \"Mango\": [4, 10, 90], ... \"Orange\": [10, 8, 14], ... \"Watermelon\": [40, 99, 43], ... \"Gin\": [16, 200, 34], ... \"Vodka\": [20, 33, 18], ... }, ... columns=[ ... \"City\", ... \"State\", ... \"Name\", ... \"Mango\", ... \"Orange\", ... \"Watermelon\", ... \"Gin\", ... \"Vodka\", ... ], ... ) >>> df City State Name Mango Orange Watermelon Gin Vodka 0 Houston Texas Aria 4 10 40 16 20 1 Austin Texas Penelope 10 8 99 200 33 2 Hoover Alabama Niko 90 14 43 34 18 >>> df.pivot_longer( ... index=[\"City\", \"State\"], ... column_names=slice(\"Mango\", \"Vodka\"), ... names_to=(\"Fruit\", \"Drink\"), ... values_to=(\"Pounds\", \"Ounces\"), ... names_pattern=[r\"M|O|W\", r\"G|V\"], ... ) City State Fruit Pounds Drink Ounces 0 Houston Texas Mango 4 Gin 16.0 1 Austin Texas Mango 10 Gin 200.0 2 Hoover Alabama Mango 90 Gin 34.0 3 Houston Texas Orange 10 Vodka 20.0 4 Austin Texas Orange 8 Vodka 33.0 5 Hoover Alabama Orange 14 Vodka 18.0 6 Houston Texas Watermelon 40 None NaN 7 Austin Texas Watermelon 99 None NaN 8 Hoover Alabama Watermelon 43 None NaN !!! abstract \"Version Changed\" - 0.24.0 - Added `dropna` parameter. :param df: A pandas DataFrame. :param index: Name(s) of columns to use as identifier variables. Should be either a single column name, or a list/tuple of column names. `index` should be a list of tuples if the columns are a MultiIndex. :param column_names: Name(s) of columns to unpivot. Should be either a single column name or a list/tuple of column names. `column_names` should be a list of tuples if the columns are a MultiIndex. :param names_to: Name of new column as a string that will contain what were previously the column names in `column_names`. The default is `variable` if no value is provided. It can also be a list/tuple of strings that will serve as new column names, if `name_sep` or `names_pattern` is provided. If `.value` is in `names_to`, new column names will be extracted from part of the existing column names and overrides`values_to`. :param values_to: Name of new column as a string that will contain what were previously the values of the columns in `column_names`. values_to can also be a list/tuple and requires that names_pattern is also a list/tuple. :param column_level: If columns are a MultiIndex, then use this level to unpivot the DataFrame. Provided for compatibility with pandas' melt, and applies only if neither `names_sep` nor `names_pattern` is provided. :param names_sep: Determines how the column name is broken up, if `names_to` contains multiple values. It takes the same specification as pandas' `str.split` method, and can be a string or regular expression. `names_sep` does not work with MultiIndex columns. :param names_pattern: Determines how the column name is broken up. It can be a regular expression containing matching groups (it takes the same specification as pandas' `str.extract` method), or a list/tuple of regular expressions. If it is a single regex, the number of groups must match the length of `names_to`. For a list/tuple of regular expressions, `names_to` must also be a list/tuple and the lengths of both arguments must match. `names_pattern` does not work with MultiIndex columns. :param names_transform: Use this option to change the types of columns that have been transformed to rows. This does not applies to the values' columns. Accepts any argument that is acceptable by `pd.astype`. :param dropna: Determines whether or not to drop nulls from the values columns. Default is `False`. :param sort_by_appearance: Default `False`. Boolean value that determines the final look of the DataFrame. If `True`, the unpivoted DataFrame will be stacked in order of first appearance. :param ignore_index: Default `True`. If `True`, the original index is ignored. If `False`, the original index is retained and the index labels will be repeated as necessary. :returns: A pandas DataFrame that has been unpivoted from wide to long format. \"\"\" # noqa: E501 # this code builds on the wonderful work of @benjaminjack\u2019s PR # https://github.com/benjaminjack/pyjanitor/commit/e3df817903c20dd21634461c8a92aec137963ed0 df = df.copy() ( df, index, column_names, names_to, values_to, names_sep, names_pattern, names_transform, dropna, sort_by_appearance, ignore_index, ) = _data_checks_pivot_longer( df, index, column_names, names_to, values_to, column_level, names_sep, names_pattern, names_transform, dropna, sort_by_appearance, ignore_index, ) return _computations_pivot_longer( df, index, column_names, names_to, values_to, names_sep, names_pattern, names_transform, dropna, sort_by_appearance, ignore_index, )","title":"pivot_longer()"},{"location":"api/functions/#janitor.functions.pivot.pivot_wider","text":"Reshapes data from long to wide form. The number of columns are increased, while decreasing the number of rows. It is the inverse of the pivot_longer method, and is a wrapper around pd.DataFrame.pivot method. This method does not mutate the original DataFrame. Column selection in index , names_from and values_from is possible using the select_columns syntax. A ValueError is raised if the combination of the index and names_from is not unique. By default, values from values_from are always at the top level if the columns are not flattened. If flattened, the values from values_from are usually at the start of each label in the columns. Example: >>> import pandas as pd >>> import janitor >>> df = [{'dep': 5.5, 'step': 1, 'a': 20, 'b': 30}, ... {'dep': 5.5, 'step': 2, 'a': 25, 'b': 37}, ... {'dep': 6.1, 'step': 1, 'a': 22, 'b': 19}, ... {'dep': 6.1, 'step': 2, 'a': 18, 'b': 29}] >>> df = pd.DataFrame(df) >>> df dep step a b 0 5.5 1 20 30 1 5.5 2 25 37 2 6.1 1 22 19 3 6.1 2 18 29 Pivot and flatten columns: >>> df.pivot_wider( ... index = \"dep\", ... names_from = \"step\", ... ) dep a_1 a_2 b_1 b_2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 Modify columns with names_sep : >>> df.pivot_wider( ... index = \"dep\", ... names_from = \"step\", ... names_sep = \"\", ... ) dep a1 a2 b1 b2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 Modify columns with names_glue : >>> df.pivot_wider( ... index = \"dep\", ... names_from = \"step\", ... names_glue = \"{_value}_step{step}\", ... ) dep a_step1 a_step2 b_step1 b_step2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 Version Changed 0.24.0 Added reset_index , names_expand and index_expand parameters. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required index Union[list, str] Name(s) of columns to use as identifier variables. It should be either a single column name, or a list of column names. If index is not provided, the DataFrame's index is used. None names_from Union[list, str] Name(s) of column(s) to use to make the new DataFrame's columns. Should be either a single column name, or a list of column names. None values_from Union[list, str] Name(s) of column(s) that will be used for populating the new DataFrame's values. If values_from is not specified, all remaining columns will be used. None flatten_levels Optional[bool] Default is True . If False , the DataFrame stays as a MultiIndex. True names_sep str If names_from or values_from contain multiple variables, this will be used to join the values into a single string to use as a column name. Default is _ . Applicable only if flatten_levels is True . '_' names_glue str A string to control the output of the flattened columns. It offers more flexibility in creating custom column names, and uses python's str.format_map under the hood. Simply create the string template, using the column labels in names_from , and special _value as a placeholder for values_from . Applicable only if flatten_levels is True . None reset_index bool Determines whether to restore index as a column/columns. Applicable only if index is provided, and flatten_levels is True . Default is True . True names_expand bool Expand columns to show all the categories. Applies only if names_from is a categorical column. Default is False . False index_expand bool Expand the index to show all the categories. Applies only if index is a categorical column. Default is False . False Returns: Type Description DataFrame A pandas DataFrame that has been unpivoted from long to wide form. Source code in janitor/functions/pivot.py @pf.register_dataframe_method def pivot_wider( df: pd.DataFrame, index: Optional[Union[list, str]] = None, names_from: Optional[Union[list, str]] = None, values_from: Optional[Union[list, str]] = None, flatten_levels: Optional[bool] = True, names_sep: str = \"_\", names_glue: str = None, reset_index: bool = True, names_expand: bool = False, index_expand: bool = False, ) -> pd.DataFrame: \"\"\" Reshapes data from *long* to *wide* form. The number of columns are increased, while decreasing the number of rows. It is the inverse of the [`pivot_longer`][janitor.functions.pivot.pivot_longer] method, and is a wrapper around `pd.DataFrame.pivot` method. This method does not mutate the original DataFrame. Column selection in `index`, `names_from` and `values_from` is possible using the [`select_columns`][janitor.functions.select.select_columns] syntax. A ValueError is raised if the combination of the `index` and `names_from` is not unique. By default, values from `values_from` are always at the top level if the columns are not flattened. If flattened, the values from `values_from` are usually at the start of each label in the columns. Example: >>> import pandas as pd >>> import janitor >>> df = [{'dep': 5.5, 'step': 1, 'a': 20, 'b': 30}, ... {'dep': 5.5, 'step': 2, 'a': 25, 'b': 37}, ... {'dep': 6.1, 'step': 1, 'a': 22, 'b': 19}, ... {'dep': 6.1, 'step': 2, 'a': 18, 'b': 29}] >>> df = pd.DataFrame(df) >>> df dep step a b 0 5.5 1 20 30 1 5.5 2 25 37 2 6.1 1 22 19 3 6.1 2 18 29 Pivot and flatten columns: >>> df.pivot_wider( ... index = \"dep\", ... names_from = \"step\", ... ) dep a_1 a_2 b_1 b_2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 Modify columns with `names_sep`: >>> df.pivot_wider( ... index = \"dep\", ... names_from = \"step\", ... names_sep = \"\", ... ) dep a1 a2 b1 b2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 Modify columns with `names_glue`: >>> df.pivot_wider( ... index = \"dep\", ... names_from = \"step\", ... names_glue = \"{_value}_step{step}\", ... ) dep a_step1 a_step2 b_step1 b_step2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 !!! abstract \"Version Changed\" - 0.24.0 - Added `reset_index`, `names_expand` and `index_expand` parameters. :param df: A pandas DataFrame. :param index: Name(s) of columns to use as identifier variables. It should be either a single column name, or a list of column names. If `index` is not provided, the DataFrame's index is used. :param names_from: Name(s) of column(s) to use to make the new DataFrame's columns. Should be either a single column name, or a list of column names. :param values_from: Name(s) of column(s) that will be used for populating the new DataFrame's values. If `values_from` is not specified, all remaining columns will be used. :param flatten_levels: Default is `True`. If `False`, the DataFrame stays as a MultiIndex. :param names_sep: If `names_from` or `values_from` contain multiple variables, this will be used to join the values into a single string to use as a column name. Default is `_`. Applicable only if `flatten_levels` is `True`. :param names_glue: A string to control the output of the flattened columns. It offers more flexibility in creating custom column names, and uses python's `str.format_map` under the hood. Simply create the string template, using the column labels in `names_from`, and special `_value` as a placeholder for `values_from`. Applicable only if `flatten_levels` is `True`. :param reset_index: Determines whether to restore `index` as a column/columns. Applicable only if `index` is provided, and `flatten_levels` is `True`. Default is `True`. :param names_expand: Expand columns to show all the categories. Applies only if `names_from` is a categorical column. Default is `False`. :param index_expand: Expand the index to show all the categories. Applies only if `index` is a categorical column. Default is `False`. :returns: A pandas DataFrame that has been unpivoted from long to wide form. \"\"\" # noqa: E501 df = df.copy() return _computations_pivot_wider( df, index, names_from, values_from, flatten_levels, names_sep, names_glue, reset_index, names_expand, index_expand, )","title":"pivot_wider()"},{"location":"api/functions/#janitor.functions.process_text","text":"","title":"process_text"},{"location":"api/functions/#janitor.functions.process_text.process_text","text":"Apply a Pandas string method to an existing column. This function aims to make string cleaning easy, while chaining, by simply passing the string method name, along with keyword arguments, if any, to the function. This modifies an existing column; it does not create a new column; new columns can be created via pyjanitor's transform_columns . A list of all the string methods in Pandas can be accessed here . Example: >>> import pandas as pd >>> import janitor >>> import re >>> df = pd.DataFrame({\"text\": [\"Ragnar\", \"sammywemmy\", \"ginger\"], ... \"code\": [1, 2, 3]}) >>> df text code 0 Ragnar 1 1 sammywemmy 2 2 ginger 3 >>> df.process_text(column_name=\"text\", string_function=\"lower\") text code 0 ragnar 1 1 sammywemmy 2 2 ginger 3 For string methods with parameters, simply pass the keyword arguments: >>> df.process_text( ... column_name=\"text\", ... string_function=\"extract\", ... pat=r\"(ag)\", ... expand=False, ... flags=re.IGNORECASE, ... ) text code 0 ag 1 1 NaN 2 2 NaN 3 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str String column to be operated on. required string_function str pandas string method to be applied. required kwargs Keyword arguments for parameters of the string_function . {} Returns: Type Description DataFrame A pandas DataFrame with modified column. Exceptions: Type Description KeyError If string_function is not a Pandas string method. ValueError If the text function returns a DataFrame, instead of a Series. Source code in janitor/functions/process_text.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def process_text( df: pd.DataFrame, column_name: str, string_function: str, **kwargs, ) -> pd.DataFrame: \"\"\" Apply a Pandas string method to an existing column. This function aims to make string cleaning easy, while chaining, by simply passing the string method name, along with keyword arguments, if any, to the function. This modifies an existing column; it does not create a new column; new columns can be created via pyjanitor's [`transform_columns`][janitor.functions.transform_columns.transform_columns]. A list of all the string methods in Pandas can be accessed [here](https://pandas.pydata.org/docs/user_guide/text.html#method-summary). Example: >>> import pandas as pd >>> import janitor >>> import re >>> df = pd.DataFrame({\"text\": [\"Ragnar\", \"sammywemmy\", \"ginger\"], ... \"code\": [1, 2, 3]}) >>> df text code 0 Ragnar 1 1 sammywemmy 2 2 ginger 3 >>> df.process_text(column_name=\"text\", string_function=\"lower\") text code 0 ragnar 1 1 sammywemmy 2 2 ginger 3 For string methods with parameters, simply pass the keyword arguments: >>> df.process_text( ... column_name=\"text\", ... string_function=\"extract\", ... pat=r\"(ag)\", ... expand=False, ... flags=re.IGNORECASE, ... ) text code 0 ag 1 1 NaN 2 2 NaN 3 :param df: A pandas DataFrame. :param column_name: String column to be operated on. :param string_function: pandas string method to be applied. :param kwargs: Keyword arguments for parameters of the `string_function`. :returns: A pandas DataFrame with modified column. :raises KeyError: If `string_function` is not a Pandas string method. :raises ValueError: If the text function returns a DataFrame, instead of a Series. \"\"\" # noqa: E501 check(\"column_name\", column_name, [str]) check(\"string_function\", string_function, [str]) check_column(df, [column_name]) pandas_string_methods = [ func.__name__ for _, func in inspect.getmembers(pd.Series.str, inspect.isfunction) if not func.__name__.startswith(\"_\") ] if string_function not in pandas_string_methods: raise KeyError(f\"{string_function} is not a Pandas string method.\") result = getattr(df[column_name].str, string_function)(**kwargs) if isinstance(result, pd.DataFrame): raise ValueError( \"The outcome of the processed text is a DataFrame, \" \"which is not supported in `process_text`.\" ) return df.assign(**{column_name: result})","title":"process_text()"},{"location":"api/functions/#janitor.functions.remove_columns","text":"Implementation of remove_columns.","title":"remove_columns"},{"location":"api/functions/#janitor.functions.remove_columns.remove_columns","text":"Remove the set of columns specified in column_names . This method does not mutate the original DataFrame. Intended to be the method-chaining alternative to del df[col] . Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [2, 4, 6], \"b\": [1, 3, 5], \"c\": [7, 8, 9]}) >>> df a b c 0 2 1 7 1 4 3 8 2 6 5 9 >>> df.remove_columns(column_names=['a', 'c']) b 0 1 1 3 2 5 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names Union[str, Iterable[str], Hashable] The columns to remove. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/remove_columns.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def remove_columns( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable], ) -> pd.DataFrame: \"\"\"Remove the set of columns specified in `column_names`. This method does not mutate the original DataFrame. Intended to be the method-chaining alternative to `del df[col]`. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [2, 4, 6], \"b\": [1, 3, 5], \"c\": [7, 8, 9]}) >>> df a b c 0 2 1 7 1 4 3 8 2 6 5 9 >>> df.remove_columns(column_names=['a', 'c']) b 0 1 1 3 2 5 :param df: A pandas DataFrame. :param column_names: The columns to remove. :returns: A pandas DataFrame. \"\"\" return df.drop(columns=column_names)","title":"remove_columns()"},{"location":"api/functions/#janitor.functions.remove_empty","text":"Implementation of remove_empty.","title":"remove_empty"},{"location":"api/functions/#janitor.functions.remove_empty.remove_empty","text":"Drop all rows and columns that are completely null. This method also resets the index (by default) since it doesn't make sense to preserve the index of a completely empty row. This method mutates the original DataFrame. Implementation is inspired from StackOverflow . Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [1, np.nan, 2], ... \"b\": [3, np.nan, 4], ... \"c\": [np.nan, np.nan, np.nan], ... }) >>> df a b c 0 1.0 3.0 NaN 1 NaN NaN NaN 2 2.0 4.0 NaN >>> df.remove_empty() a b 0 1.0 3.0 1 2.0 4.0 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/remove_empty.py @pf.register_dataframe_method def remove_empty(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Drop all rows and columns that are completely null. This method also resets the index (by default) since it doesn't make sense to preserve the index of a completely empty row. This method mutates the original DataFrame. Implementation is inspired from [StackOverflow][so]. [so]: https://stackoverflow.com/questions/38884538/python-pandas-find-all-rows-where-all-values-are-nan Example: >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [1, np.nan, 2], ... \"b\": [3, np.nan, 4], ... \"c\": [np.nan, np.nan, np.nan], ... }) >>> df a b c 0 1.0 3.0 NaN 1 NaN NaN NaN 2 2.0 4.0 NaN >>> df.remove_empty() a b 0 1.0 3.0 1 2.0 4.0 :param df: The pandas DataFrame object. :returns: A pandas DataFrame. \"\"\" # noqa: E501 nanrows = df.index[df.isna().all(axis=1)] df = df.drop(index=nanrows).reset_index(drop=True) nancols = df.columns[df.isna().all(axis=0)] df = df.drop(columns=nancols) return df","title":"remove_empty()"},{"location":"api/functions/#janitor.functions.rename_columns","text":"","title":"rename_columns"},{"location":"api/functions/#janitor.functions.rename_columns.rename_column","text":"Rename a column in place. This method does not mutate the original DataFrame. Example: Change the name of column 'a' to 'a_new'. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.rename_column(old_column_name='a', new_column_name='a_new') a_new b 0 0 a 1 1 b 2 2 c This is just syntactic sugar/a convenience function for renaming one column at a time. If you are convinced that there are multiple columns in need of changing, then use the pandas.DataFrame.rename method. Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required old_column_name str The old column name. required new_column_name str The new column name. required Returns: Type Description DataFrame A pandas DataFrame with renamed columns. Source code in janitor/functions/rename_columns.py @pf.register_dataframe_method @deprecated_alias(old=\"old_column_name\", new=\"new_column_name\") def rename_column( df: pd.DataFrame, old_column_name: str, new_column_name: str, ) -> pd.DataFrame: \"\"\"Rename a column in place. This method does not mutate the original DataFrame. Example: Change the name of column 'a' to 'a_new'. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")}) >>> df.rename_column(old_column_name='a', new_column_name='a_new') a_new b 0 0 a 1 1 b 2 2 c This is just syntactic sugar/a convenience function for renaming one column at a time. If you are convinced that there are multiple columns in need of changing, then use the `pandas.DataFrame.rename` method. :param df: The pandas DataFrame object. :param old_column_name: The old column name. :param new_column_name: The new column name. :returns: A pandas DataFrame with renamed columns. \"\"\" # noqa: E501 check_column(df, [old_column_name]) return df.rename(columns={old_column_name: new_column_name})","title":"rename_column()"},{"location":"api/functions/#janitor.functions.rename_columns.rename_columns","text":"Rename columns. This method does not mutate the original DataFrame. Example: Rename columns using a dictionary which maps old names to new names. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"xyz\")}) >>> df a b 0 0 x 1 1 y 2 2 z >>> df.rename_columns(new_column_names={\"a\": \"a_new\", \"b\": \"b_new\"}) a_new b_new 0 0 x 1 1 y 2 2 z Example: Rename columns using a generic callable. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"xyz\")}) >>> df.rename_columns(function=str.upper) A B 0 0 x 1 1 y 2 2 z One of the new_column_names or function are a required parameter. If both are provided, then new_column_names takes priority and function is never executed. Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required new_column_names Optional[Dict] A dictionary of old and new column names. None function Callable A function which should be applied to all the columns. None Returns: Type Description DataFrame A pandas DataFrame with renamed columns. Exceptions: Type Description ValueError if both new_column_names and function are None. Source code in janitor/functions/rename_columns.py @pf.register_dataframe_method def rename_columns( df: pd.DataFrame, new_column_names: Union[Dict, None] = None, function: Callable = None, ) -> pd.DataFrame: \"\"\"Rename columns. This method does not mutate the original DataFrame. Example: Rename columns using a dictionary which maps old names to new names. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"xyz\")}) >>> df a b 0 0 x 1 1 y 2 2 z >>> df.rename_columns(new_column_names={\"a\": \"a_new\", \"b\": \"b_new\"}) a_new b_new 0 0 x 1 1 y 2 2 z Example: Rename columns using a generic callable. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"xyz\")}) >>> df.rename_columns(function=str.upper) A B 0 0 x 1 1 y 2 2 z One of the `new_column_names` or `function` are a required parameter. If both are provided, then `new_column_names` takes priority and `function` is never executed. :param df: The pandas DataFrame object. :param new_column_names: A dictionary of old and new column names. :param function: A function which should be applied to all the columns. :returns: A pandas DataFrame with renamed columns. :raises ValueError: if both `new_column_names` and `function` are None. \"\"\" # noqa: E501 if new_column_names is None and function is None: raise ValueError( \"One of new_column_names or function must be provided\" ) if new_column_names is not None: check_column(df, new_column_names) return df.rename(columns=new_column_names) return df.rename(mapper=function, axis=\"columns\")","title":"rename_columns()"},{"location":"api/functions/#janitor.functions.reorder_columns","text":"","title":"reorder_columns"},{"location":"api/functions/#janitor.functions.reorder_columns.reorder_columns","text":"Reorder DataFrame columns by specifying desired order as list of col names. Columns not specified retain their order and follow after the columns specified in column_order . All columns specified within the column_order list must be present within df . This method does not mutate the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": [1, 1, 1], \"col2\": [2, 2, 2], \"col3\": [3, 3, 3]}) >>> df col1 col2 col3 0 1 2 3 1 1 2 3 2 1 2 3 >>> df.reorder_columns(['col3', 'col1']) col3 col1 col2 0 3 1 2 1 3 1 2 2 3 1 2 Notice that the column order of df is now col3 , col1 , col2 . Internally, this function uses DataFrame.reindex with copy=False to avoid unnecessary data duplication. Parameters: Name Type Description Default df DataFrame DataFrame to reorder required column_order Union[Iterable[str], pandas.core.indexes.base.Index, Hashable] A list of column names or Pandas Index specifying their order in the returned DataFrame . required Returns: Type Description DataFrame A pandas DataFrame with reordered columns. Exceptions: Type Description IndexError if a column within column_order is not found within the DataFrame. Source code in janitor/functions/reorder_columns.py @pf.register_dataframe_method def reorder_columns( df: pd.DataFrame, column_order: Union[Iterable[str], pd.Index, Hashable] ) -> pd.DataFrame: \"\"\"Reorder DataFrame columns by specifying desired order as list of col names. Columns not specified retain their order and follow after the columns specified in `column_order`. All columns specified within the `column_order` list must be present within `df`. This method does not mutate the original DataFrame. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": [1, 1, 1], \"col2\": [2, 2, 2], \"col3\": [3, 3, 3]}) >>> df col1 col2 col3 0 1 2 3 1 1 2 3 2 1 2 3 >>> df.reorder_columns(['col3', 'col1']) col3 col1 col2 0 3 1 2 1 3 1 2 2 3 1 2 Notice that the column order of `df` is now `col3`, `col1`, `col2`. Internally, this function uses `DataFrame.reindex` with `copy=False` to avoid unnecessary data duplication. :param df: `DataFrame` to reorder :param column_order: A list of column names or Pandas `Index` specifying their order in the returned `DataFrame`. :returns: A pandas DataFrame with reordered columns. :raises IndexError: if a column within `column_order` is not found within the DataFrame. \"\"\" # noqa: E501 check(\"column_order\", column_order, [list, tuple, pd.Index]) if any(col not in df.columns for col in column_order): raise IndexError( \"One or more columns in `column_order` were not found in the \" \"DataFrame.\" ) # if column_order is a Pandas index, needs conversion to list: column_order = list(column_order) return df.reindex( columns=( column_order + [col for col in df.columns if col not in column_order] ), copy=False, )","title":"reorder_columns()"},{"location":"api/functions/#janitor.functions.round_to_fraction","text":"Implementation of round_to_fraction","title":"round_to_fraction"},{"location":"api/functions/#janitor.functions.round_to_fraction.round_to_fraction","text":"Round all values in a column to a fraction. This method mutates the original DataFrame. Taken from the R package . Also, optionally round to a specified number of digits. Example: Round numeric column to the nearest 1/4 value. >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a1\": [1.263, 2.499, np.nan], ... \"a2\": [\"x\", \"y\", \"z\"], ... }) >>> df a1 a2 0 1.263 x 1 2.499 y 2 NaN z >>> df.round_to_fraction(\"a1\", denominator=4) a1 a2 0 1.25 x 1 2.50 y 2 NaN z Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Name of column to round to fraction. required denominator float The denominator of the fraction for rounding. Must be a positive number. required digits float The number of digits for rounding after rounding to the fraction. Default is np.inf (i.e. no subsequent rounding). inf Returns: Type Description DataFrame A pandas DataFrame with a column's values rounded. Exceptions: Type Description ValueError If denominator is not a positive number. Source code in janitor/functions/round_to_fraction.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\") def round_to_fraction( df: pd.DataFrame, column_name: Hashable, denominator: float, digits: float = np.inf, ) -> pd.DataFrame: \"\"\"Round all values in a column to a fraction. This method mutates the original DataFrame. Taken from [the R package](https://github.com/sfirke/janitor/issues/235). Also, optionally round to a specified number of digits. Example: Round numeric column to the nearest 1/4 value. >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a1\": [1.263, 2.499, np.nan], ... \"a2\": [\"x\", \"y\", \"z\"], ... }) >>> df a1 a2 0 1.263 x 1 2.499 y 2 NaN z >>> df.round_to_fraction(\"a1\", denominator=4) a1 a2 0 1.25 x 1 2.50 y 2 NaN z :param df: A pandas DataFrame. :param column_name: Name of column to round to fraction. :param denominator: The denominator of the fraction for rounding. Must be a positive number. :param digits: The number of digits for rounding after rounding to the fraction. Default is np.inf (i.e. no subsequent rounding). :returns: A pandas DataFrame with a column's values rounded. :raises ValueError: If `denominator` is not a positive number. \"\"\" check_column(df, column_name) check(\"denominator\", denominator, [float, int]) check(\"digits\", digits, [float, int]) if denominator <= 0: raise ValueError(\"denominator is expected to be a positive number.\") df[column_name] = round(df[column_name] * denominator, 0) / denominator if not np.isinf(digits): df[column_name] = round(df[column_name], digits) return df","title":"round_to_fraction()"},{"location":"api/functions/#janitor.functions.row_to_names","text":"Implementation of the row_to_names function.","title":"row_to_names"},{"location":"api/functions/#janitor.functions.row_to_names.row_to_names","text":"Elevates a row to be the column names of a DataFrame. This method mutates the original DataFrame. Contains options to remove the elevated row from the DataFrame along with removing the rows above the selected row. Example: Replace column names with the first row and reset the index. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [\"nums\", 6, 9], ... \"b\": [\"chars\", \"x\", \"y\"], ... }) >>> df a b 0 nums chars 1 6 x 2 9 y >>> df.row_to_names(0, remove_row=True, reset_index=True) nums chars 0 6 x 1 9 y Example: Remove rows above the elevated row and the elevated row itself. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [\"bla1\", \"nums\", 6, 9], ... \"b\": [\"bla2\", \"chars\", \"x\", \"y\"], ... }) >>> df a b 0 bla1 bla2 1 nums chars 2 6 x 3 9 y >>> df.row_to_names(1, remove_row=True, remove_rows_above=True, reset_index=True) nums chars 0 6 x 1 9 y Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required row_number int Position of the row containing the variable names. Note that indexing starts from 0. Defaults to 0 (first row). 0 remove_row bool Whether the row should be removed from the DataFrame. Defaults to False. False remove_rows_above bool Whether the rows above the selected row should be removed from the DataFrame. Defaults to False. False reset_index bool Whether the index should be reset on the returning DataFrame. Defaults to False. False Returns: Type Description DataFrame A pandas DataFrame with set column names. Source code in janitor/functions/row_to_names.py @pf.register_dataframe_method def row_to_names( df: pd.DataFrame, row_number: int = 0, remove_row: bool = False, remove_rows_above: bool = False, reset_index: bool = False, ) -> pd.DataFrame: \"\"\"Elevates a row to be the column names of a DataFrame. This method mutates the original DataFrame. Contains options to remove the elevated row from the DataFrame along with removing the rows above the selected row. Example: Replace column names with the first row and reset the index. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [\"nums\", 6, 9], ... \"b\": [\"chars\", \"x\", \"y\"], ... }) >>> df a b 0 nums chars 1 6 x 2 9 y >>> df.row_to_names(0, remove_row=True, reset_index=True) nums chars 0 6 x 1 9 y Example: Remove rows above the elevated row and the elevated row itself. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [\"bla1\", \"nums\", 6, 9], ... \"b\": [\"bla2\", \"chars\", \"x\", \"y\"], ... }) >>> df a b 0 bla1 bla2 1 nums chars 2 6 x 3 9 y >>> df.row_to_names(1, remove_row=True, remove_rows_above=True, reset_index=True) nums chars 0 6 x 1 9 y :param df: A pandas DataFrame. :param row_number: Position of the row containing the variable names. Note that indexing starts from 0. Defaults to 0 (first row). :param remove_row: Whether the row should be removed from the DataFrame. Defaults to False. :param remove_rows_above: Whether the rows above the selected row should be removed from the DataFrame. Defaults to False. :param reset_index: Whether the index should be reset on the returning DataFrame. Defaults to False. :returns: A pandas DataFrame with set column names. \"\"\" # noqa: E501 check(\"row_number\", row_number, [int]) warnings.warn( \"The function row_to_names will, in the official 1.0 release, \" \"change its behaviour to reset the dataframe's index by default. \" \"You can prepare for this change right now by explicitly setting \" \"`reset_index=True` when calling on `row_to_names`.\" ) df.columns = df.iloc[row_number, :] df.columns.name = None if remove_row: df = df.drop(df.index[row_number]) if remove_rows_above: df = df.drop(df.index[range(row_number)]) if reset_index: df = df.reset_index(drop=[\"index\"]) return df","title":"row_to_names()"},{"location":"api/functions/#janitor.functions.select","text":"","title":"select"},{"location":"api/functions/#janitor.functions.select.select","text":"Method-chainable selection of rows and columns. It accepts a string, shell-like glob strings (*string*) , regex, slice, array-like object, or a list of the previous options. Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary. This method does not mutate the original DataFrame. Selection can be inverted with the DropLabel class. New in version 0.24.0 Note The preferred option when selecting columns or rows in a Pandas DataFrame is with .loc or .iloc methods, as they are generally performant. select is primarily for convenience. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]], ... index=['cobra', 'viper', 'sidewinder'], ... columns=['max_speed', 'shield']) >>> df max_speed shield cobra 1 2 viper 4 5 sidewinder 7 8 >>> df.select(rows='cobra', columns='shield') shield cobra 2 Labels can be dropped with the DropLabel class: >>> df.select(rows=DropLabel('cobra')) max_speed shield viper 4 5 sidewinder 7 8 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required rows Valid inputs include: an exact label to look for, a shell-style glob string (e.g. *_thing_* ), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. None columns Valid inputs include: an exact label to look for, a shell-style glob string (e.g. *_thing_* ), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. None Returns: Type Description DataFrame A pandas DataFrame with the specified rows and/or columns selected. Source code in janitor/functions/select.py @pf.register_dataframe_method def select(df: pd.DataFrame, *, rows=None, columns=None) -> pd.DataFrame: \"\"\" Method-chainable selection of rows and columns. It accepts a string, shell-like glob strings `(*string*)`, regex, slice, array-like object, or a list of the previous options. Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary. This method does not mutate the original DataFrame. Selection can be inverted with the `DropLabel` class. !!! info \"New in version 0.24.0\" !!!note The preferred option when selecting columns or rows in a Pandas DataFrame is with `.loc` or `.iloc` methods, as they are generally performant. `select` is primarily for convenience. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]], ... index=['cobra', 'viper', 'sidewinder'], ... columns=['max_speed', 'shield']) >>> df max_speed shield cobra 1 2 viper 4 5 sidewinder 7 8 >>> df.select(rows='cobra', columns='shield') shield cobra 2 Labels can be dropped with the `DropLabel` class: >>> df.select(rows=DropLabel('cobra')) max_speed shield viper 4 5 sidewinder 7 8 :param df: A pandas DataFrame. :param rows: Valid inputs include: an exact label to look for, a shell-style glob string (e.g. `*_thing_*`), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. :param columns: Valid inputs include: an exact label to look for, a shell-style glob string (e.g. `*_thing_*`), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. :returns: A pandas DataFrame with the specified rows and/or columns selected. \"\"\" # noqa: E501 return _select(df, args=None, rows=rows, columns=columns, axis=\"both\")","title":"select()"},{"location":"api/functions/#janitor.functions.select.select_columns","text":"Method-chainable selection of columns. It accepts a string, shell-like glob strings (*string*) , regex, slice, array-like object, or a list of the previous options. Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary. This method does not mutate the original DataFrame. Optional ability to invert selection of columns available as well. Note The preferred option when selecting columns or rows in a Pandas DataFrame is with .loc or .iloc methods, as they are generally performant. select_columns is primarily for convenience. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": [1, 2], \"foo\": [3, 4], \"col2\": [5, 6]}) >>> df col1 foo col2 0 1 3 5 1 2 4 6 >>> df.select_columns(\"col*\") col1 col2 0 1 5 1 2 6 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required args Valid inputs include: an exact column name to look for, a shell-style glob string (e.g. *_thing_* ), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. () invert bool Whether or not to invert the selection. This will result in the selection of the complement of the columns provided. False Returns: Type Description DataFrame A pandas DataFrame with the specified columns selected. Source code in janitor/functions/select.py @pf.register_dataframe_method @deprecated_alias(search_cols=\"search_column_names\") def select_columns( df: pd.DataFrame, *args, invert: bool = False, ) -> pd.DataFrame: \"\"\" Method-chainable selection of columns. It accepts a string, shell-like glob strings `(*string*)`, regex, slice, array-like object, or a list of the previous options. Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary. This method does not mutate the original DataFrame. Optional ability to invert selection of columns available as well. !!!note The preferred option when selecting columns or rows in a Pandas DataFrame is with `.loc` or `.iloc` methods, as they are generally performant. `select_columns` is primarily for convenience. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"col1\": [1, 2], \"foo\": [3, 4], \"col2\": [5, 6]}) >>> df col1 foo col2 0 1 3 5 1 2 4 6 >>> df.select_columns(\"col*\") col1 col2 0 1 5 1 2 6 :param df: A pandas DataFrame. :param args: Valid inputs include: an exact column name to look for, a shell-style glob string (e.g. `*_thing_*`), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. :param invert: Whether or not to invert the selection. This will result in the selection of the complement of the columns provided. :returns: A pandas DataFrame with the specified columns selected. \"\"\" # noqa: E501 return _select(df, args=args, invert=invert, axis=\"columns\")","title":"select_columns()"},{"location":"api/functions/#janitor.functions.select.select_rows","text":"Method-chainable selection of rows. It accepts a string, shell-like glob strings (*string*) , regex, slice, array-like object, or a list of the previous options. Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary. This method does not mutate the original DataFrame. Optional ability to invert selection of rows available as well. New in version 0.24.0 Note The preferred option when selecting columns or rows in a Pandas DataFrame is with .loc or .iloc methods, as they are generally performant. select_rows is primarily for convenience. Example: >>> import pandas as pd >>> import janitor >>> df = {\"col1\": [1, 2], \"foo\": [3, 4], \"col2\": [5, 6]} >>> df = pd.DataFrame.from_dict(df, orient='index') >>> df 0 1 col1 1 2 foo 3 4 col2 5 6 >>> df.select_rows(\"col*\") 0 1 col1 1 2 col2 5 6 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required args Valid inputs include: an exact index name to look for, a shell-style glob string (e.g. *_thing_* ), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. () invert bool Whether or not to invert the selection. This will result in the selection of the complement of the rows provided. False Returns: Type Description DataFrame A pandas DataFrame with the specified rows selected. Source code in janitor/functions/select.py @pf.register_dataframe_method def select_rows( df: pd.DataFrame, *args, invert: bool = False, ) -> pd.DataFrame: \"\"\" Method-chainable selection of rows. It accepts a string, shell-like glob strings `(*string*)`, regex, slice, array-like object, or a list of the previous options. Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary. This method does not mutate the original DataFrame. Optional ability to invert selection of rows available as well. !!! info \"New in version 0.24.0\" !!!note The preferred option when selecting columns or rows in a Pandas DataFrame is with `.loc` or `.iloc` methods, as they are generally performant. `select_rows` is primarily for convenience. Example: >>> import pandas as pd >>> import janitor >>> df = {\"col1\": [1, 2], \"foo\": [3, 4], \"col2\": [5, 6]} >>> df = pd.DataFrame.from_dict(df, orient='index') >>> df 0 1 col1 1 2 foo 3 4 col2 5 6 >>> df.select_rows(\"col*\") 0 1 col1 1 2 col2 5 6 :param df: A pandas DataFrame. :param args: Valid inputs include: an exact index name to look for, a shell-style glob string (e.g. `*_thing_*`), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels. :param invert: Whether or not to invert the selection. This will result in the selection of the complement of the rows provided. :returns: A pandas DataFrame with the specified rows selected. \"\"\" # noqa: E501 return _select(df, args=args, invert=invert, axis=\"index\")","title":"select_rows()"},{"location":"api/functions/#janitor.functions.shuffle","text":"Implementation of shuffle functions.","title":"shuffle"},{"location":"api/functions/#janitor.functions.shuffle.shuffle","text":"Shuffle the rows of the DataFrame. This method does not mutate the original DataFrame. Super-sugary syntax! Underneath the hood, we use df.sample(frac=1) , with the option to set the random state. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"col1\": range(5), ... \"col2\": list(\"abcde\"), ... }) >>> df col1 col2 0 0 a 1 1 b 2 2 c 3 3 d 4 4 e >>> df.shuffle(random_state=42) col1 col2 0 1 b 1 4 e 2 2 c 3 0 a 4 3 d Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required random_state If provided, set a seed for the random number generator. None reset_index bool If True, reset the dataframe index to the default RangeIndex. True Returns: Type Description DataFrame A shuffled pandas DataFrame. Source code in janitor/functions/shuffle.py @pf.register_dataframe_method def shuffle( df: pd.DataFrame, random_state=None, reset_index: bool = True ) -> pd.DataFrame: \"\"\"Shuffle the rows of the DataFrame. This method does not mutate the original DataFrame. Super-sugary syntax! Underneath the hood, we use `df.sample(frac=1)`, with the option to set the random state. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"col1\": range(5), ... \"col2\": list(\"abcde\"), ... }) >>> df col1 col2 0 0 a 1 1 b 2 2 c 3 3 d 4 4 e >>> df.shuffle(random_state=42) col1 col2 0 1 b 1 4 e 2 2 c 3 0 a 4 3 d :param df: A pandas DataFrame. :param random_state: If provided, set a seed for the random number generator. :param reset_index: If True, reset the dataframe index to the default RangeIndex. :returns: A shuffled pandas DataFrame. \"\"\" result = df.sample(frac=1, random_state=random_state) if reset_index: result = result.reset_index(drop=True) return result","title":"shuffle()"},{"location":"api/functions/#janitor.functions.sort_column_value_order","text":"Implementation of the sort_column_value_order function.","title":"sort_column_value_order"},{"location":"api/functions/#janitor.functions.sort_column_value_order.sort_column_value_order","text":"This function adds precedence to certain values in a specified column, then sorts based on that column and any other specified columns. Example: >>> import pandas as pd >>> import janitor >>> import numpy as np >>> company_sales = { ... \"SalesMonth\": [\"Jan\", \"Feb\", \"Feb\", \"Mar\", \"April\"], ... \"Company1\": [150.0, 200.0, 200.0, 300.0, 400.0], ... \"Company2\": [180.0, 250.0, 250.0, np.nan, 500.0], ... \"Company3\": [400.0, 500.0, 500.0, 600.0, 675.0], ... } >>> df = pd.DataFrame.from_dict(company_sales) >>> df SalesMonth Company1 Company2 Company3 0 Jan 150.0 180.0 400.0 1 Feb 200.0 250.0 500.0 2 Feb 200.0 250.0 500.0 3 Mar 300.0 NaN 600.0 4 April 400.0 500.0 675.0 >>> df.sort_column_value_order( ... \"SalesMonth\", ... {\"April\": 1, \"Mar\": 2, \"Feb\": 3, \"Jan\": 4} ... ) SalesMonth Company1 Company2 Company3 4 April 400.0 500.0 675.0 3 Mar 300.0 NaN 600.0 1 Feb 200.0 250.0 500.0 2 Feb 200.0 250.0 500.0 0 Jan 150.0 180.0 400.0 Parameters: Name Type Description Default df DataFrame This is our DataFrame that we are manipulating required column str This is a column name as a string we are using to specify which column to sort by required column_value_order dict This is a dictionary of values that will represent precedence of the values in the specified column required columns This is a list of additional columns that we can sort by None Returns: Type Description DataFrame A sorted pandas DataFrame. Exceptions: Type Description ValueError raises error if chosen Column Name is not in Dataframe, or if column_value_order dictionary is empty. Source code in janitor/functions/sort_column_value_order.py @pf.register_dataframe_method def sort_column_value_order( df: pd.DataFrame, column: str, column_value_order: dict, columns=None ) -> pd.DataFrame: \"\"\" This function adds precedence to certain values in a specified column, then sorts based on that column and any other specified columns. Example: >>> import pandas as pd >>> import janitor >>> import numpy as np >>> company_sales = { ... \"SalesMonth\": [\"Jan\", \"Feb\", \"Feb\", \"Mar\", \"April\"], ... \"Company1\": [150.0, 200.0, 200.0, 300.0, 400.0], ... \"Company2\": [180.0, 250.0, 250.0, np.nan, 500.0], ... \"Company3\": [400.0, 500.0, 500.0, 600.0, 675.0], ... } >>> df = pd.DataFrame.from_dict(company_sales) >>> df SalesMonth Company1 Company2 Company3 0 Jan 150.0 180.0 400.0 1 Feb 200.0 250.0 500.0 2 Feb 200.0 250.0 500.0 3 Mar 300.0 NaN 600.0 4 April 400.0 500.0 675.0 >>> df.sort_column_value_order( ... \"SalesMonth\", ... {\"April\": 1, \"Mar\": 2, \"Feb\": 3, \"Jan\": 4} ... ) SalesMonth Company1 Company2 Company3 4 April 400.0 500.0 675.0 3 Mar 300.0 NaN 600.0 1 Feb 200.0 250.0 500.0 2 Feb 200.0 250.0 500.0 0 Jan 150.0 180.0 400.0 :param df: This is our DataFrame that we are manipulating :param column: This is a column name as a string we are using to specify which column to sort by :param column_value_order: This is a dictionary of values that will represent precedence of the values in the specified column :param columns: This is a list of additional columns that we can sort by :raises ValueError: raises error if chosen Column Name is not in Dataframe, or if column_value_order dictionary is empty. :return: A sorted pandas DataFrame. \"\"\" # Validation checks check_column(df, column, present=True) check(\"column_value_order\", column_value_order, [dict]) if not column_value_order: raise ValueError(\"column_value_order dictionary cannot be empty\") df = df.assign(cond_order=df[column].replace(column_value_order)) sort_by = [\"cond_order\"] if columns is not None: sort_by = [\"cond_order\"] + columns df = df.sort_values(sort_by).remove_columns(\"cond_order\") return df","title":"sort_column_value_order()"},{"location":"api/functions/#janitor.functions.sort_naturally","text":"Implementation of the sort_naturally function.","title":"sort_naturally"},{"location":"api/functions/#janitor.functions.sort_naturally.sort_naturally","text":"Sort a DataFrame by a column using natural sorting. Natural sorting is distinct from the default lexiographical sorting provided by pandas . For example, given the following list of items: [\"A1\", \"A11\", \"A3\", \"A2\", \"A10\"] lexicographical sorting would give us: [\"A1\", \"A10\", \"A11\", \"A2\", \"A3\"] By contrast, \"natural\" sorting would give us: [\"A1\", \"A2\", \"A3\", \"A10\", \"A11\"] This function thus provides natural sorting on a single column of a dataframe. To accomplish this, we do a natural sort on the unique values that are present in the dataframe. Then, we reconstitute the entire dataframe in the naturally sorted order. Natural sorting is provided by the Python package natsort All keyword arguments to natsort should be provided after the column name to sort by is provided. They are passed through to the natsorted function. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"Well\": [\"A21\", \"A3\", \"A21\", \"B2\", \"B51\", \"B12\"], ... \"Value\": [1, 2, 13, 3, 4, 7], ... } ... ) >>> df Well Value 0 A21 1 1 A3 2 2 A21 13 3 B2 3 4 B51 4 5 B12 7 >>> df.sort_naturally(\"Well\") Well Value 1 A3 2 0 A21 1 2 A21 13 3 B2 3 5 B12 7 4 B51 4 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str The column on which natural sorting should take place. required natsorted_kwargs Keyword arguments to be passed to natsort's natsorted function. {} Returns: Type Description DataFrame A sorted pandas DataFrame. Source code in janitor/functions/sort_naturally.py @pf.register_dataframe_method def sort_naturally( df: pd.DataFrame, column_name: str, **natsorted_kwargs ) -> pd.DataFrame: \"\"\"Sort a DataFrame by a column using *natural* sorting. Natural sorting is distinct from the default lexiographical sorting provided by `pandas`. For example, given the following list of items: [\"A1\", \"A11\", \"A3\", \"A2\", \"A10\"] lexicographical sorting would give us: [\"A1\", \"A10\", \"A11\", \"A2\", \"A3\"] By contrast, \"natural\" sorting would give us: [\"A1\", \"A2\", \"A3\", \"A10\", \"A11\"] This function thus provides *natural* sorting on a single column of a dataframe. To accomplish this, we do a natural sort on the unique values that are present in the dataframe. Then, we reconstitute the entire dataframe in the naturally sorted order. Natural sorting is provided by the Python package [natsort](https://natsort.readthedocs.io/en/master/index.html) All keyword arguments to `natsort` should be provided after the column name to sort by is provided. They are passed through to the `natsorted` function. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame( ... { ... \"Well\": [\"A21\", \"A3\", \"A21\", \"B2\", \"B51\", \"B12\"], ... \"Value\": [1, 2, 13, 3, 4, 7], ... } ... ) >>> df Well Value 0 A21 1 1 A3 2 2 A21 13 3 B2 3 4 B51 4 5 B12 7 >>> df.sort_naturally(\"Well\") Well Value 1 A3 2 0 A21 1 2 A21 13 3 B2 3 5 B12 7 4 B51 4 :param df: A pandas DataFrame. :param column_name: The column on which natural sorting should take place. :param natsorted_kwargs: Keyword arguments to be passed to natsort's `natsorted` function. :returns: A sorted pandas DataFrame. \"\"\" new_order = index_natsorted(df[column_name], **natsorted_kwargs) return df.iloc[new_order, :]","title":"sort_naturally()"},{"location":"api/functions/#janitor.functions.take_first","text":"Implementation of take_first function.","title":"take_first"},{"location":"api/functions/#janitor.functions.take_first.take_first","text":"Take the first row within each group specified by subset . Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [\"x\", \"x\", \"y\", \"y\"], \"b\": [0, 1, 2, 3]}) >>> df a b 0 x 0 1 x 1 2 y 2 3 y 3 >>> df.take_first(subset=\"a\", by=\"b\") a b 0 x 0 2 y 2 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required subset Union[Hashable, Iterable[Hashable]] Column(s) defining the group. required by Hashable Column to sort by. required ascending bool Whether or not to sort in ascending order, bool . True Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/take_first.py @pf.register_dataframe_method def take_first( df: pd.DataFrame, subset: Union[Hashable, Iterable[Hashable]], by: Hashable, ascending: bool = True, ) -> pd.DataFrame: \"\"\" Take the first row within each group specified by `subset`. Example: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({\"a\": [\"x\", \"x\", \"y\", \"y\"], \"b\": [0, 1, 2, 3]}) >>> df a b 0 x 0 1 x 1 2 y 2 3 y 3 >>> df.take_first(subset=\"a\", by=\"b\") a b 0 x 0 2 y 2 :param df: A pandas DataFrame. :param subset: Column(s) defining the group. :param by: Column to sort by. :param ascending: Whether or not to sort in ascending order, `bool`. :returns: A pandas DataFrame. \"\"\" result = df.sort_values(by=by, ascending=ascending).drop_duplicates( subset=subset, keep=\"first\" ) return result","title":"take_first()"},{"location":"api/functions/#janitor.functions.then","text":"","title":"then"},{"location":"api/functions/#janitor.functions.then.then","text":"Add an arbitrary function to run in the pyjanitor method chain. This method does not mutate the original DataFrame. Example: A trivial example using a lambda func . >>> import pandas as pd >>> import janitor >>> (pd.DataFrame({\"a\": [1, 2, 3], \"b\": [7, 8, 9]}) ... .then(lambda df: df * 2)) a b 0 2 14 1 4 16 2 6 18 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required func Callable A function you would like to run in the method chain. It should take one parameter and return one parameter, each being the DataFrame object. After that, do whatever you want in the middle. Go crazy. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/then.py @pf.register_dataframe_method def then(df: pd.DataFrame, func: Callable) -> pd.DataFrame: \"\"\"Add an arbitrary function to run in the `pyjanitor` method chain. This method does not mutate the original DataFrame. Example: A trivial example using a lambda `func`. >>> import pandas as pd >>> import janitor >>> (pd.DataFrame({\"a\": [1, 2, 3], \"b\": [7, 8, 9]}) ... .then(lambda df: df * 2)) a b 0 2 14 1 4 16 2 6 18 :param df: A pandas DataFrame. :param func: A function you would like to run in the method chain. It should take one parameter and return one parameter, each being the DataFrame object. After that, do whatever you want in the middle. Go crazy. :returns: A pandas DataFrame. \"\"\" df = func(df) return df","title":"then()"},{"location":"api/functions/#janitor.functions.to_datetime","text":"","title":"to_datetime"},{"location":"api/functions/#janitor.functions.to_datetime.to_datetime","text":"Convert column to a datetime type, in-place. Intended to be the method-chaining equivalent of: df[column_name] = pd.to_datetime(df[column_name], **kwargs) This method mutates the original DataFrame. Example: Converting a string column to datetime type with custom format. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'date': ['20200101', '20200202', '20200303']}) >>> df date 0 20200101 1 20200202 2 20200303 >>> df.to_datetime('date', format='%Y%m%d') date 0 2020-01-01 1 2020-02-02 2 2020-03-03 Read the pandas documentation for to_datetime for more information. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Column name. required kwargs Provide any kwargs that pd.to_datetime can take. {} Returns: Type Description DataFrame A pandas DataFrame with updated datetime data. Source code in janitor/functions/to_datetime.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def to_datetime( df: pd.DataFrame, column_name: Hashable, **kwargs ) -> pd.DataFrame: \"\"\"Convert column to a datetime type, in-place. Intended to be the method-chaining equivalent of: df[column_name] = pd.to_datetime(df[column_name], **kwargs) This method mutates the original DataFrame. Example: Converting a string column to datetime type with custom format. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({'date': ['20200101', '20200202', '20200303']}) >>> df date 0 20200101 1 20200202 2 20200303 >>> df.to_datetime('date', format='%Y%m%d') date 0 2020-01-01 1 2020-02-02 2 2020-03-03 Read the pandas documentation for [`to_datetime`][pd_docs] for more information. [pd_docs]: https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html :param df: A pandas DataFrame. :param column_name: Column name. :param kwargs: Provide any kwargs that `pd.to_datetime` can take. :returns: A pandas DataFrame with updated datetime data. \"\"\" # noqa: E501 df[column_name] = pd.to_datetime(df[column_name], **kwargs) return df","title":"to_datetime()"},{"location":"api/functions/#janitor.functions.toset","text":"Implementation of the toset function.","title":"toset"},{"location":"api/functions/#janitor.functions.toset.toset","text":"Return a set of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period) Example: >>> import pandas as pd >>> import janitor >>> s = pd.Series([1, 2, 3, 5, 5], index=[\"a\", \"b\", \"c\", \"d\", \"e\"]) >>> s a 1 b 2 c 3 d 5 e 5 dtype: int64 >>> s.toset() {1, 2, 3, 5} Parameters: Name Type Description Default series Series A pandas series. required Returns: Type Description Set A set of values. Source code in janitor/functions/toset.py @pf.register_series_method def toset(series: pd.Series) -> Set: \"\"\"Return a set of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period) Example: >>> import pandas as pd >>> import janitor >>> s = pd.Series([1, 2, 3, 5, 5], index=[\"a\", \"b\", \"c\", \"d\", \"e\"]) >>> s a 1 b 2 c 3 d 5 e 5 dtype: int64 >>> s.toset() {1, 2, 3, 5} :param series: A pandas series. :returns: A set of values. \"\"\" return set(series.tolist())","title":"toset()"},{"location":"api/functions/#janitor.functions.transform_columns","text":"","title":"transform_columns"},{"location":"api/functions/#janitor.functions.transform_columns.transform_column","text":"Transform the given column using the provided function. Meant to be the method-chaining equivalent of: df[dest_column_name] = df[column_name].apply(function) Functions can be applied in one of two ways: Element-wise (default; elementwise=True ). Then, the individual column elements will be passed in as the first argument of function . Column-wise ( elementwise=False ). Then, function is expected to take in a pandas Series and return a sequence that is of identical length to the original. If dest_column_name is provided, then the transformation result is stored in that column. Otherwise, the transformed result is stored under the name of the original column. This method does not mutate the original DataFrame. Example: Transform a column in-place with an element-wise function. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [2, 3, 4], ... \"b\": [\"area\", \"pyjanitor\", \"grapefruit\"], ... }) >>> df a b 0 2 area 1 3 pyjanitor 2 4 grapefruit >>> df.transform_column( ... column_name=\"a\", ... function=lambda x: x**2 - 1, ... ) a b 0 3 area 1 8 pyjanitor 2 15 grapefruit Example: Transform a column in-place with an column-wise function. >>> df.transform_column( ... column_name=\"b\", ... function=lambda srs: srs.str[:5], ... elementwise=False, ... ) a b 0 2 area 1 3 pyjan 2 4 grape Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column to transform. required function Callable A function to apply on the column. required dest_column_name Optional[str] The column name to store the transformation result in. Defaults to None, which will result in the original column name being overwritten. If a name is provided here, then a new column with the transformed values will be created. None elementwise bool Whether to apply the function elementwise or not. If elementwise is True, then the function's first argument should be the data type of each datum in the column of data, and should return a transformed datum. If elementwise is False, then the function's should expect a pandas Series passed into it, and return a pandas Series. True Returns: Type Description DataFrame A pandas DataFrame with a transformed column. Source code in janitor/functions/transform_columns.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\", dest_col_name=\"dest_column_name\") def transform_column( df: pd.DataFrame, column_name: Hashable, function: Callable, dest_column_name: Optional[str] = None, elementwise: bool = True, ) -> pd.DataFrame: \"\"\"Transform the given column using the provided function. Meant to be the method-chaining equivalent of: ```python df[dest_column_name] = df[column_name].apply(function) ``` Functions can be applied in one of two ways: - **Element-wise** (default; `elementwise=True`). Then, the individual column elements will be passed in as the first argument of `function`. - **Column-wise** (`elementwise=False`). Then, `function` is expected to take in a pandas Series and return a sequence that is of identical length to the original. If `dest_column_name` is provided, then the transformation result is stored in that column. Otherwise, the transformed result is stored under the name of the original column. This method does not mutate the original DataFrame. Example: Transform a column in-place with an element-wise function. >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"a\": [2, 3, 4], ... \"b\": [\"area\", \"pyjanitor\", \"grapefruit\"], ... }) >>> df a b 0 2 area 1 3 pyjanitor 2 4 grapefruit >>> df.transform_column( ... column_name=\"a\", ... function=lambda x: x**2 - 1, ... ) a b 0 3 area 1 8 pyjanitor 2 15 grapefruit Example: Transform a column in-place with an column-wise function. >>> df.transform_column( ... column_name=\"b\", ... function=lambda srs: srs.str[:5], ... elementwise=False, ... ) a b 0 2 area 1 3 pyjan 2 4 grape :param df: A pandas DataFrame. :param column_name: The column to transform. :param function: A function to apply on the column. :param dest_column_name: The column name to store the transformation result in. Defaults to None, which will result in the original column name being overwritten. If a name is provided here, then a new column with the transformed values will be created. :param elementwise: Whether to apply the function elementwise or not. If `elementwise` is True, then the function's first argument should be the data type of each datum in the column of data, and should return a transformed datum. If `elementwise` is False, then the function's should expect a pandas Series passed into it, and return a pandas Series. :returns: A pandas DataFrame with a transformed column. \"\"\" check_column(df, column_name) if dest_column_name is None: dest_column_name = column_name elif dest_column_name != column_name: # If `dest_column_name` is provided and equals `column_name`, then we # assume that the user's intent is to perform an in-place # transformation (Same behaviour as when `dest_column_name` = None). # Otherwise we throw an error if `dest_column_name` already exists in # df. check_column(df, dest_column_name, present=False) result = _get_transform_column_result( df[column_name], function, elementwise, ) return df.assign(**{dest_column_name: result})","title":"transform_column()"},{"location":"api/functions/#janitor.functions.transform_columns.transform_columns","text":"Transform multiple columns through the same transformation. This method does not mutate the original DataFrame. Super syntactic sugar! Essentially wraps transform_column and calls it repeatedly over all column names provided. User can optionally supply either a suffix to create a new set of columns with the specified suffix, or provide a dictionary mapping each original column name in column_names to its corresponding new column name. Note that all column names must be strings. Example: log10 transform a list of columns, replacing original columns. >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"col1\": [5, 10, 15], ... \"col2\": [3, 6, 9], ... \"col3\": [10, 100, 1_000], ... }) >>> df col1 col2 col3 0 5 3 10 1 10 6 100 2 15 9 1000 >>> df.transform_columns([\"col1\", \"col2\", \"col3\"], np.log10) col1 col2 col3 0 0.698970 0.477121 1.0 1 1.000000 0.778151 2.0 2 1.176091 0.954243 3.0 Example: Using the suffix parameter to create new columns. >>> df.transform_columns([\"col1\", \"col3\"], np.log10, suffix=\"_log\") col1 col2 col3 col1_log col3_log 0 5 3 10 0.698970 1.0 1 10 6 100 1.000000 2.0 2 15 9 1000 1.176091 3.0 Example: Using the new_column_names parameter to create new columns. >>> df.transform_columns( ... [\"col1\", \"col3\"], ... np.log10, ... new_column_names={\"col1\": \"transform1\"}, ... ) col1 col2 col3 transform1 0 5 3 1.0 0.698970 1 10 6 2.0 1.000000 2 15 9 3.0 1.176091 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names Union[List[str], Tuple[str]] An iterable of columns to transform. required function Callable A function to apply on each column. required suffix Optional[str] Suffix to use when creating new columns to hold the transformed values. None elementwise bool Passed on to transform_column ; whether or not to apply the transformation function elementwise (True) or columnwise (False). True new_column_names Optional[Dict[str, str]] An explicit mapping of old column names in column_names to new column names. If any column specified in column_names is not a key in this dictionary, the transformation will happen in-place for that column. None Returns: Type Description DataFrame A pandas DataFrame with transformed columns. Exceptions: Type Description ValueError If both suffix and new_column_names are specified. Source code in janitor/functions/transform_columns.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\", new_names=\"new_column_names\") def transform_columns( df: pd.DataFrame, column_names: Union[List[str], Tuple[str]], function: Callable, suffix: Optional[str] = None, elementwise: bool = True, new_column_names: Optional[Dict[str, str]] = None, ) -> pd.DataFrame: \"\"\"Transform multiple columns through the same transformation. This method does not mutate the original DataFrame. Super syntactic sugar! Essentially wraps [`transform_column`][janitor.functions.transform_columns.transform_column] and calls it repeatedly over all column names provided. User can optionally supply either a suffix to create a new set of columns with the specified suffix, or provide a dictionary mapping each original column name in `column_names` to its corresponding new column name. Note that all column names must be strings. Example: log10 transform a list of columns, replacing original columns. >>> import numpy as np >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"col1\": [5, 10, 15], ... \"col2\": [3, 6, 9], ... \"col3\": [10, 100, 1_000], ... }) >>> df col1 col2 col3 0 5 3 10 1 10 6 100 2 15 9 1000 >>> df.transform_columns([\"col1\", \"col2\", \"col3\"], np.log10) col1 col2 col3 0 0.698970 0.477121 1.0 1 1.000000 0.778151 2.0 2 1.176091 0.954243 3.0 Example: Using the `suffix` parameter to create new columns. >>> df.transform_columns([\"col1\", \"col3\"], np.log10, suffix=\"_log\") col1 col2 col3 col1_log col3_log 0 5 3 10 0.698970 1.0 1 10 6 100 1.000000 2.0 2 15 9 1000 1.176091 3.0 Example: Using the `new_column_names` parameter to create new columns. >>> df.transform_columns( ... [\"col1\", \"col3\"], ... np.log10, ... new_column_names={\"col1\": \"transform1\"}, ... ) col1 col2 col3 transform1 0 5 3 1.0 0.698970 1 10 6 2.0 1.000000 2 15 9 3.0 1.176091 :param df: A pandas DataFrame. :param column_names: An iterable of columns to transform. :param function: A function to apply on each column. :param suffix: Suffix to use when creating new columns to hold the transformed values. :param elementwise: Passed on to `transform_column`; whether or not to apply the transformation function elementwise (True) or columnwise (False). :param new_column_names: An explicit mapping of old column names in `column_names` to new column names. If any column specified in `column_names` is not a key in this dictionary, the transformation will happen in-place for that column. :returns: A pandas DataFrame with transformed columns. :raises ValueError: If both `suffix` and `new_column_names` are specified. \"\"\" # noqa: E501 check(\"column_names\", column_names, [list, tuple]) check_column(df, column_names) if suffix is not None and new_column_names is not None: raise ValueError( \"Only one of `suffix` or `new_column_names` should be specified.\" ) if suffix: check(\"suffix\", suffix, [str]) dest_column_names = {col: col + suffix for col in column_names} elif new_column_names: check(\"new_column_names\", new_column_names, [dict]) dest_column_names = { col: new_column_names.get(col, col) for col in column_names } else: dest_column_names = dict(zip(column_names, column_names)) results = {} for old_col, new_col in dest_column_names.items(): if old_col != new_col: check_column(df, new_col, present=False) results[new_col] = _get_transform_column_result( df[old_col], function, elementwise=elementwise, ) return df.assign(**results)","title":"transform_columns()"},{"location":"api/functions/#janitor.functions.truncate_datetime","text":"Implementation of the truncate_datetime family of functions.","title":"truncate_datetime"},{"location":"api/functions/#janitor.functions.truncate_datetime.truncate_datetime_dataframe","text":"Truncate times down to a user-specified precision of year, month, day, hour, minute, or second. This method does not mutate the original DataFrame. Examples: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"xxxx\", \"yyyy\", \"zzzz\"], ... \"dt\": pd.date_range(\"2020-03-11\", periods=3, freq=\"15H\"), ... }) >>> df foo dt 0 xxxx 2020-03-11 00:00:00 1 yyyy 2020-03-11 15:00:00 2 zzzz 2020-03-12 06:00:00 >>> df.truncate_datetime_dataframe(\"day\") foo dt 0 xxxx 2020-03-11 1 yyyy 2020-03-11 2 zzzz 2020-03-12 Parameters: Name Type Description Default df DataFrame The pandas DataFrame on which to truncate datetime. required datepart str Truncation precision, YEAR, MONTH, DAY, HOUR, MINUTE, SECOND. (String is automagically capitalized) required Returns: Type Description DataFrame A pandas DataFrame with all valid datetimes truncated down to the specified precision. Exceptions: Type Description ValueError If an invalid datepart precision is passed in. Source code in janitor/functions/truncate_datetime.py @pf.register_dataframe_method def truncate_datetime_dataframe( df: pd.DataFrame, datepart: str, ) -> pd.DataFrame: \"\"\"Truncate times down to a user-specified precision of year, month, day, hour, minute, or second. This method does not mutate the original DataFrame. Examples: >>> import pandas as pd >>> import janitor >>> df = pd.DataFrame({ ... \"foo\": [\"xxxx\", \"yyyy\", \"zzzz\"], ... \"dt\": pd.date_range(\"2020-03-11\", periods=3, freq=\"15H\"), ... }) >>> df foo dt 0 xxxx 2020-03-11 00:00:00 1 yyyy 2020-03-11 15:00:00 2 zzzz 2020-03-12 06:00:00 >>> df.truncate_datetime_dataframe(\"day\") foo dt 0 xxxx 2020-03-11 1 yyyy 2020-03-11 2 zzzz 2020-03-12 :param df: The pandas DataFrame on which to truncate datetime. :param datepart: Truncation precision, YEAR, MONTH, DAY, HOUR, MINUTE, SECOND. (String is automagically capitalized) :raises ValueError: If an invalid `datepart` precision is passed in. :returns: A pandas DataFrame with all valid datetimes truncated down to the specified precision. \"\"\" ACCEPTABLE_DATEPARTS = (\"YEAR\", \"MONTH\", \"DAY\", \"HOUR\", \"MINUTE\", \"SECOND\") datepart = datepart.upper() if datepart not in ACCEPTABLE_DATEPARTS: raise ValueError( \"Received an invalid `datepart` precision. \" f\"Please enter any one of {ACCEPTABLE_DATEPARTS}.\" ) dt_cols = [ column for column, coltype in df.dtypes.items() if is_datetime64_any_dtype(coltype) ] if not dt_cols: # avoid copying df if no-op is expected return df df = df.copy() # NOTE: use **kwargs of `applymap` instead of lambda when we upgrade to # pandas >= 1.3.0 df[dt_cols] = df[dt_cols].applymap( lambda x: _truncate_datetime(x, datepart=datepart), ) return df","title":"truncate_datetime_dataframe()"},{"location":"api/functions/#janitor.functions.update_where","text":"Function for updating values based on other column values","title":"update_where"},{"location":"api/functions/#janitor.functions.update_where.update_where","text":"Add multiple conditions to update a column in the dataframe. This method does not mutate the original DataFrame. Example usage: >>> data = { ... \"a\": [1, 2, 3, 4], ... \"b\": [5, 6, 7, 8], ... \"c\": [0, 0, 0, 0], ... } >>> df = pd.DataFrame(data) >>> df a b c 0 1 5 0 1 2 6 0 2 3 7 0 3 4 8 0 >>> df.update_where( ... conditions = (df.a > 2) & (df.b < 8), ... target_column_name = 'c', ... target_val = 10 ... ) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 >>> df.update_where( # supports pandas *query* style string expressions ... conditions = \"a > 2 and b < 8\", ... target_column_name = 'c', ... target_val = 10 ... ) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required conditions Any Conditions used to update a target column and target value. required target_column_name Hashable Column to be updated. If column does not exist in DataFrame, a new column will be created; note that entries that do not get set in the new column will be null. required target_val Any Value to be updated required Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError if conditions does not return a boolean array-like data structure. .. # noqa: DAR402 Source code in janitor/functions/update_where.py @pf.register_dataframe_method @deprecated_alias(target_col=\"target_column_name\") def update_where( df: pd.DataFrame, conditions: Any, target_column_name: Hashable, target_val: Any, ) -> pd.DataFrame: \"\"\" Add multiple conditions to update a column in the dataframe. This method does not mutate the original DataFrame. Example usage: >>> data = { ... \"a\": [1, 2, 3, 4], ... \"b\": [5, 6, 7, 8], ... \"c\": [0, 0, 0, 0], ... } >>> df = pd.DataFrame(data) >>> df a b c 0 1 5 0 1 2 6 0 2 3 7 0 3 4 8 0 >>> df.update_where( ... conditions = (df.a > 2) & (df.b < 8), ... target_column_name = 'c', ... target_val = 10 ... ) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 >>> df.update_where( # supports pandas *query* style string expressions ... conditions = \"a > 2 and b < 8\", ... target_column_name = 'c', ... target_val = 10 ... ) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 :param df: The pandas DataFrame object. :param conditions: Conditions used to update a target column and target value. :param target_column_name: Column to be updated. If column does not exist in DataFrame, a new column will be created; note that entries that do not get set in the new column will be null. :param target_val: Value to be updated :returns: A pandas DataFrame. :raises ValueError: if `conditions` does not return a boolean array-like data structure. .. # noqa: DAR402 \"\"\" df = df.copy() # use query mode if a string expression is passed if isinstance(conditions, str): conditions = df.eval(conditions) if not is_bool_dtype(conditions): raise ValueError( \"\"\" Kindly ensure that `conditions` passed evaluates to a Boolean dtype. \"\"\" ) df.loc[conditions, target_column_name] = target_val return df","title":"update_where()"},{"location":"api/functions/#janitor.functions.utils","text":"Utility functions for all of the functions submodule.","title":"utils"},{"location":"api/functions/#janitor.functions.utils.DropLabel","text":"Helper class for removing labels within the select syntax. label can be any of the types supported in the select , select_rows and select_columns functions. An array of integers not matching the labels is returned. New in version 0.24.0 Parameters: Name Type Description Default label Label(s) to be dropped from the index. required Returns: Type Description A dataclass. Source code in janitor/functions/utils.py @dataclass class DropLabel: \"\"\" Helper class for removing labels within the `select` syntax. `label` can be any of the types supported in the `select`, `select_rows` and `select_columns` functions. An array of integers not matching the labels is returned. !!! info \"New in version 0.24.0\" :param label: Label(s) to be dropped from the index. :returns: A dataclass. \"\"\" label: Any","title":"DropLabel"},{"location":"api/functions/#janitor.functions.utils.patterns","text":"This function converts a string into a compiled regular expression; it can be used to select columns in the index or columns_names arguments of pivot_longer function. Warning : This function is deprecated. Kindly use `re.compile` instead. Parameters: Name Type Description Default regex_pattern Union[str, Pattern] string to be converted to compiled regular expression. required Returns: Type Description Pattern A compile regular expression from provided regex_pattern . Source code in janitor/functions/utils.py def patterns(regex_pattern: Union[str, Pattern]) -> Pattern: \"\"\" This function converts a string into a compiled regular expression; it can be used to select columns in the index or columns_names arguments of `pivot_longer` function. **Warning**: This function is deprecated. Kindly use `re.compile` instead. :param regex_pattern: string to be converted to compiled regular expression. :returns: A compile regular expression from provided `regex_pattern`. \"\"\" warnings.warn( \"This function is deprecated. Kindly use `re.compile` instead.\", DeprecationWarning, stacklevel=2, ) check(\"regular expression\", regex_pattern, [str, Pattern]) return re.compile(regex_pattern)","title":"patterns()"},{"location":"api/functions/#janitor.functions.utils.unionize_dataframe_categories","text":"Given a group of dataframes which contain some categorical columns, for each categorical column present, find all the possible categories across all the dataframes which have that column. Update each dataframes' corresponding column with a new categorical object that contains the original data but has labels for all the possible categories from all dataframes. This is useful when concatenating a list of dataframes which all have the same categorical columns into one dataframe. If, for a given categorical column, all input dataframes do not have at least one instance of all the possible categories, Pandas will change the output dtype of that column from category to object , losing out on dramatic speed gains you get from the former format. Usage example for concatenation of categorical column-containing dataframes: Instead of: concatenated_df = pd.concat([df1, df2, df3], ignore_index=True) which in your case has resulted in category -> object conversion, use: unionized_dataframes = unionize_dataframe_categories(df1, df2, df2) concatenated_df = pd.concat(unionized_dataframes, ignore_index=True) Parameters: Name Type Description Default dataframes The dataframes you wish to unionize the categorical objects for. () column_names Optional[Iterable[pandas.core.dtypes.dtypes.CategoricalDtype]] If supplied, only unionize this subset of columns. None Returns: Type Description List[pandas.core.frame.DataFrame] A list of the category-unioned dataframes in the same order they were provided. Exceptions: Type Description TypeError If any of the inputs are not pandas DataFrames. Source code in janitor/functions/utils.py def unionize_dataframe_categories( *dataframes, column_names: Optional[Iterable[pd.CategoricalDtype]] = None ) -> List[pd.DataFrame]: \"\"\" Given a group of dataframes which contain some categorical columns, for each categorical column present, find all the possible categories across all the dataframes which have that column. Update each dataframes' corresponding column with a new categorical object that contains the original data but has labels for all the possible categories from all dataframes. This is useful when concatenating a list of dataframes which all have the same categorical columns into one dataframe. If, for a given categorical column, all input dataframes do not have at least one instance of all the possible categories, Pandas will change the output dtype of that column from `category` to `object`, losing out on dramatic speed gains you get from the former format. Usage example for concatenation of categorical column-containing dataframes: Instead of: ```python concatenated_df = pd.concat([df1, df2, df3], ignore_index=True) ``` which in your case has resulted in `category` -> `object` conversion, use: ```python unionized_dataframes = unionize_dataframe_categories(df1, df2, df2) concatenated_df = pd.concat(unionized_dataframes, ignore_index=True) ``` :param dataframes: The dataframes you wish to unionize the categorical objects for. :param column_names: If supplied, only unionize this subset of columns. :returns: A list of the category-unioned dataframes in the same order they were provided. :raises TypeError: If any of the inputs are not pandas DataFrames. \"\"\" if any(not isinstance(df, pd.DataFrame) for df in dataframes): raise TypeError(\"Inputs must all be dataframes.\") if column_names is None: # Find all columns across all dataframes that are categorical column_names = set() for dataframe in dataframes: column_names = column_names.union( [ column_name for column_name in dataframe.columns if isinstance( dataframe[column_name].dtype, pd.CategoricalDtype ) ] ) else: column_names = [column_names] # For each categorical column, find all possible values across the DFs category_unions = { column_name: union_categoricals( [df[column_name] for df in dataframes if column_name in df.columns] ) for column_name in column_names } # Make a shallow copy of all DFs and modify the categorical columns # such that they can encode the union of all possible categories for each. refactored_dfs = [] for df in dataframes: df = df.copy(deep=False) for column_name, categorical in category_unions.items(): if column_name in df.columns: df[column_name] = pd.Categorical( df[column_name], categories=categorical.categories ) refactored_dfs.append(df) return refactored_dfs","title":"unionize_dataframe_categories()"},{"location":"api/io/","text":"Input/Output (io) read_commandline(cmd, **kwargs) Read a CSV file based on a command-line command. For example, you may wish to run the following command on sep-quarter.csv before reading it into a pandas DataFrame: cat sep-quarter.csv | grep .SEA1AA In this case, you can use the following Python code to load the dataframe: import janitor as jn df = jn.read_commandline(\"cat data.csv | grep .SEA1AA\") This function assumes that your command line command will return an output that is parsable using pandas.read_csv and StringIO. We default to using pd.read_csv underneath the hood. Keyword arguments are passed through to read_csv. Parameters: Name Type Description Default cmd str Shell command to preprocess a file on disk. required kwargs Keyword arguments that are passed through to pd.read_csv() . {} Returns: Type Description pd.DataFrame A pandas DataFrame parsed from the stdout of the underlying shell. Source code in janitor/io.py def read_commandline(cmd: str, **kwargs) -> pd.DataFrame: \"\"\" Read a CSV file based on a command-line command. For example, you may wish to run the following command on `sep-quarter.csv` before reading it into a pandas DataFrame: ```bash cat sep-quarter.csv | grep .SEA1AA ``` In this case, you can use the following Python code to load the dataframe: ```python import janitor as jn df = jn.read_commandline(\"cat data.csv | grep .SEA1AA\") ``` This function assumes that your command line command will return an output that is parsable using `pandas.read_csv` and StringIO. We default to using `pd.read_csv` underneath the hood. Keyword arguments are passed through to read_csv. :param cmd: Shell command to preprocess a file on disk. :param kwargs: Keyword arguments that are passed through to `pd.read_csv()`. :returns: A pandas DataFrame parsed from the stdout of the underlying shell. \"\"\" check(\"cmd\", cmd, [str]) # adding check=True ensures that an explicit, clear error # is raised, so that the user can see the reason for the failure outcome = subprocess.run( cmd, shell=True, capture_output=True, text=True, check=True ) return pd.read_csv(StringIO(outcome.stdout), **kwargs) read_csvs(files_path, separate_df=False, **kwargs) Read multiple CSV files and return a dictionary of DataFrames, or one concatenated DataFrame. Parameters: Name Type Description Default files_path Union[str, Iterable[str]] The filepath pattern matching the CSV files. Accepts regular expressions, with or without .csv extension. Also accepts iterable of file paths. required separate_df bool If False (default), returns a single Dataframe with the concatenation of the csv files. If True , returns a dictionary of separate DataFrames for each CSV file. False kwargs Keyword arguments to pass into the original pandas read_csv . {} Returns: Type Description Union[pd.DataFrame, dict] DataFrame of concatenated DataFrames or dictionary of DataFrames. Exceptions: Type Description JanitorError if None provided for files_path . JanitorError if length of files_path is 0 . ValueError if no CSV files exist in files_path . ValueError if columns in input CSV files do not match. Source code in janitor/io.py @deprecated_alias(seperate_df=\"separate_df\", filespath=\"files_path\") def read_csvs( files_path: Union[str, Iterable[str]], separate_df: bool = False, **kwargs ) -> Union[pd.DataFrame, dict]: \"\"\" Read multiple CSV files and return a dictionary of DataFrames, or one concatenated DataFrame. :param files_path: The filepath pattern matching the CSV files. Accepts regular expressions, with or without `.csv` extension. Also accepts iterable of file paths. :param separate_df: If `False` (default), returns a single Dataframe with the concatenation of the csv files. If `True`, returns a dictionary of separate DataFrames for each CSV file. :param kwargs: Keyword arguments to pass into the original pandas `read_csv`. :returns: DataFrame of concatenated DataFrames or dictionary of DataFrames. :raises JanitorError: if `None` provided for `files_path`. :raises JanitorError: if length of `files_path` is `0`. :raises ValueError: if no CSV files exist in `files_path`. :raises ValueError: if columns in input CSV files do not match. \"\"\" # Sanitize input if files_path is None: raise JanitorError(\"`None` provided for `files_path`\") if not files_path: raise JanitorError(\"0 length `files_path` provided\") # Read the csv files # String to file/folder or file pattern provided if isinstance(files_path, str): dfs_dict = { os.path.basename(f): pd.read_csv(f, **kwargs) for f in glob(files_path) } # Iterable of file paths provided else: dfs_dict = { os.path.basename(f): pd.read_csv(f, **kwargs) for f in files_path } # Check if dataframes have been read if not dfs_dict: raise ValueError(\"No CSV files to read with the given `files_path`\") # Concatenate the dataframes if requested (default) col_names = list(dfs_dict.values())[0].columns # noqa: PD011 if not separate_df: # If columns do not match raise an error for df in dfs_dict.values(): # noqa: PD011 if not all(df.columns == col_names): raise ValueError( \"Columns in input CSV files do not match.\" \"Files cannot be concatenated.\" ) return pd.concat( list(dfs_dict.values()), ignore_index=True, sort=False, # noqa: PD011 copy=False, ) return dfs_dict xlsx_cells(path, sheetnames=None, start_point=None, end_point=None, read_only=True, include_blank_cells=True, fill=False, font=False, alignment=False, border=False, protection=False, comment=False, **kwargs) Imports data from spreadsheet without coercing it into a rectangle. Each cell is represented by a row in a dataframe, and includes the cell's coordinates, the value, row and column position. The cell formatting (fill, font, border, etc) can also be accessed; usually this is returned as a dictionary in the cell, and the specific cell format attribute can be accessed using pd.Series.str.get . Inspiration for this comes from R's tidyxl package. Example: >>> import pandas as pd >>> from janitor import xlsx_cells >>> pd.set_option(\"display.max_columns\", None) >>> pd.set_option(\"display.expand_frame_repr\", False) >>> pd.set_option(\"max_colwidth\", None) >>> filename = \"../pyjanitor/tests/test_data/worked-examples.xlsx\" # Each cell is returned as a row: >>> xlsx_cells(filename, sheetnames=\"highlights\") value internal_value coordinate row column data_type is_date number_format 0 Age Age A1 1 1 s False General 1 Height Height B1 1 2 s False General 2 1 1 A2 2 1 n False General 3 2 2 B2 2 2 n False General 4 3 3 A3 3 1 n False General 5 4 4 B3 3 2 n False General 6 5 5 A4 4 1 n False General 7 6 6 B4 4 2 n False General # Access cell formatting such as fill : >>> out=xlsx_cells(filename, sheetnames=\"highlights\", fill=True).select_columns(\"value\", \"fill\") >>> out value fill 0 Age {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 1 Height {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 2 1 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 3 2 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 4 3 {'patternType': 'solid', 'fgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}} 5 4 {'patternType': 'solid', 'fgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}} 6 5 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 7 6 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} # specific cell attributes can be accessed by using Pandas' series.str.get : >>> out.fill.str.get(\"fgColor\").str.get(\"rgb\") 0 00000000 1 00000000 2 00000000 3 00000000 4 FFFFFF00 5 FFFFFF00 6 00000000 7 00000000 Name: fill, dtype: object Parameters: Name Type Description Default path Union[str, Workbook] Path to the Excel File. It can also be an openpyxl Workbook. required sheetnames Union[str, list, tuple] Names of the sheets from which the cells are to be extracted. If None , all the sheets in the file are extracted; if it is a string, or list or tuple, only the specified sheets are extracted. None start_point Union[str, int] start coordinates of the Excel sheet. This is useful if the user is only interested in a subsection of the sheet. If start_point is provided, end_point must be provided as well. None end_point Union[str, int] end coordinates of the Excel sheet. This is useful if the user is only interested in a subsection of the sheet. If end_point is provided, start_point must be provided as well. None read_only bool Determines if the entire file is loaded in memory, or streamed. For memory efficiency, read_only should be set to True . Some cell properties like comment , can only be accessed by setting read_only to False . True include_blank_cells bool Determines if cells without a value should be included. True fill bool If True , return fill properties of the cell. It is usually returned as a dictionary. False font bool If True , return font properties of the cell. It is usually returned as a dictionary. False alignment bool If True , return alignment properties of the cell. It is usually returned as a dictionary. False border bool If True , return border properties of the cell. It is usually returned as a dictionary. False protection bool If True , return protection properties of the cell. It is usually returned as a dictionary. False comment bool If True , return comment properties of the cell. It is usually returned as a dictionary. False kwargs Any other attributes of the cell, that can be accessed from openpyxl. {} Returns: Type Description Union[dict, pd.DataFrame] A pandas DataFrame, or a dictionary of DataFrames. Exceptions: Type Description ValueError If kwargs is provided, and one of the keys is a default column. AttributeError If kwargs is provided and any of the keys is not a openpyxl cell attribute. Source code in janitor/io.py def xlsx_cells( path: Union[str, Workbook], sheetnames: Union[str, list, tuple] = None, start_point: Union[str, int] = None, end_point: Union[str, int] = None, read_only: bool = True, include_blank_cells: bool = True, fill: bool = False, font: bool = False, alignment: bool = False, border: bool = False, protection: bool = False, comment: bool = False, **kwargs, ) -> Union[dict, pd.DataFrame]: \"\"\" Imports data from spreadsheet without coercing it into a rectangle. Each cell is represented by a row in a dataframe, and includes the cell's coordinates, the value, row and column position. The cell formatting (fill, font, border, etc) can also be accessed; usually this is returned as a dictionary in the cell, and the specific cell format attribute can be accessed using `pd.Series.str.get`. Inspiration for this comes from R's [tidyxl][link] package. [link]: https://nacnudus.github.io/tidyxl/reference/tidyxl.html Example: >>> import pandas as pd >>> from janitor import xlsx_cells >>> pd.set_option(\"display.max_columns\", None) >>> pd.set_option(\"display.expand_frame_repr\", False) >>> pd.set_option(\"max_colwidth\", None) >>> filename = \"../pyjanitor/tests/test_data/worked-examples.xlsx\" # Each cell is returned as a row: >>> xlsx_cells(filename, sheetnames=\"highlights\") value internal_value coordinate row column data_type is_date number_format 0 Age Age A1 1 1 s False General 1 Height Height B1 1 2 s False General 2 1 1 A2 2 1 n False General 3 2 2 B2 2 2 n False General 4 3 3 A3 3 1 n False General 5 4 4 B3 3 2 n False General 6 5 5 A4 4 1 n False General 7 6 6 B4 4 2 n False General # Access cell formatting such as fill : >>> out=xlsx_cells(filename, sheetnames=\"highlights\", fill=True).select_columns(\"value\", \"fill\") >>> out value fill 0 Age {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 1 Height {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 2 1 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 3 2 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 4 3 {'patternType': 'solid', 'fgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}} 5 4 {'patternType': 'solid', 'fgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}} 6 5 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 7 6 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} # specific cell attributes can be accessed by using Pandas' series.str.get : >>> out.fill.str.get(\"fgColor\").str.get(\"rgb\") 0 00000000 1 00000000 2 00000000 3 00000000 4 FFFFFF00 5 FFFFFF00 6 00000000 7 00000000 Name: fill, dtype: object :param path: Path to the Excel File. It can also be an openpyxl Workbook. :param sheetnames: Names of the sheets from which the cells are to be extracted. If `None`, all the sheets in the file are extracted; if it is a string, or list or tuple, only the specified sheets are extracted. :param start_point: start coordinates of the Excel sheet. This is useful if the user is only interested in a subsection of the sheet. If start_point is provided, end_point must be provided as well. :param end_point: end coordinates of the Excel sheet. This is useful if the user is only interested in a subsection of the sheet. If end_point is provided, start_point must be provided as well. :param read_only: Determines if the entire file is loaded in memory, or streamed. For memory efficiency, read_only should be set to `True`. Some cell properties like `comment`, can only be accessed by setting `read_only` to `False`. :param include_blank_cells: Determines if cells without a value should be included. :param fill: If `True`, return fill properties of the cell. It is usually returned as a dictionary. :param font: If `True`, return font properties of the cell. It is usually returned as a dictionary. :param alignment: If `True`, return alignment properties of the cell. It is usually returned as a dictionary. :param border: If `True`, return border properties of the cell. It is usually returned as a dictionary. :param protection: If `True`, return protection properties of the cell. It is usually returned as a dictionary. :param comment: If `True`, return comment properties of the cell. It is usually returned as a dictionary. :param kwargs: Any other attributes of the cell, that can be accessed from openpyxl. :raises ValueError: If kwargs is provided, and one of the keys is a default column. :raises AttributeError: If kwargs is provided and any of the keys is not a openpyxl cell attribute. :returns: A pandas DataFrame, or a dictionary of DataFrames. \"\"\" # noqa : E501 try: from openpyxl import load_workbook from openpyxl.cell.read_only import ReadOnlyCell from openpyxl.cell.cell import Cell from openpyxl.workbook.workbook import Workbook except ImportError: import_message( submodule=\"io\", package=\"openpyxl\", conda_channel=\"conda-forge\", pip_install=True, ) path_is_workbook = isinstance(path, Workbook) if not path_is_workbook: # for memory efficiency, read_only is set to True # if comments is True, read_only has to be False, # as lazy loading is not enabled for comments if comment and read_only: raise ValueError( \"To access comments, kindly set 'read_only' to False.\" ) path = load_workbook( filename=path, read_only=read_only, keep_links=False ) # start_point and end_point applies if the user is interested in # only a subset of the Excel File and knows the coordinates if start_point or end_point: check(\"start_point\", start_point, [str, int]) check(\"end_point\", end_point, [str, int]) defaults = ( \"value\", \"internal_value\", \"coordinate\", \"row\", \"column\", \"data_type\", \"is_date\", \"number_format\", ) parameters = { \"fill\": fill, \"font\": font, \"alignment\": alignment, \"border\": border, \"protection\": protection, \"comment\": comment, } if kwargs: if path_is_workbook: if path.read_only: _cell = ReadOnlyCell else: _cell = Cell else: if read_only: _cell = ReadOnlyCell else: _cell = Cell attrs = { attr for attr, _ in inspect.getmembers(_cell, not (inspect.isroutine)) if not attr.startswith(\"_\") } for key in kwargs: if key in defaults: raise ValueError( f\"{key} is part of the default attributes \" \"returned as a column.\" ) elif key not in attrs: raise AttributeError( f\"{key} is not a recognized attribute of {_cell}.\" ) parameters.update(kwargs) if not sheetnames: sheetnames = path.sheetnames elif isinstance(sheetnames, str): sheetnames = [sheetnames] else: check(\"sheetnames\", sheetnames, [str, list, tuple]) out = { sheetname: _xlsx_cells( path[sheetname], defaults, parameters, start_point, end_point, include_blank_cells, ) for sheetname in sheetnames } if len(out) == 1: _, out = out.popitem() if (not path_is_workbook) and path.read_only: path.close() return out xlsx_table(path, sheetname, table=None) Returns a DataFrame of values in a table in the Excel file. This applies to an Excel file, where the data range is explicitly specified as a Microsoft Excel table. If there is a single table in the sheet, or a string is provided as an argument to the table parameter, a pandas DataFrame is returned; if there is more than one table in the sheet, and the table argument is None , or a list/tuple of names, a dictionary of DataFrames is returned, where the keys of the dictionary are the table names. Example: >>> import pandas as pd >>> from janitor import xlsx_table >>> filename=\"../pyjanitor/tests/test_data/016-MSPTDA-Excel.xlsx\" # single table >>> xlsx_table(filename, sheetname='Tables', table='dCategory') CategoryID Category 0 1 Beginner 1 2 Advanced 2 3 Freestyle 3 4 Competition 4 5 Long Distance # multiple tables: >>> out=xlsx_table(filename, sheetname=\"Tables\", table=[\"dCategory\", \"dSalesReps\"]) >>> out[\"dCategory\"] CategoryID Category 0 1 Beginner 1 2 Advanced 2 3 Freestyle 3 4 Competition 4 5 Long Distance >>> out[\"dSalesReps\"].head(3) SalesRepID SalesRep Region 0 1 Sioux Radcoolinator NW 1 2 Tyrone Smithe NE 2 3 Chantel Zoya SW Parameters: Name Type Description Default path Union[str, Workbook] Path to the Excel File. It can also be an openpyxl Workbook. required sheetname str Name of the sheet from which the tables are to be extracted. required table Union[str, list, tuple] Name of a table, or list of tables in the sheet. None Returns: Type Description Union[pd.DataFrame, dict] A pandas DataFrame, or a dictionary of DataFrames, if there are multiple arguments for the table parameter, or the argument to table is None . Exceptions: Type Description AttributeError If a workbook is provided, and is a ReadOnlyWorksheet. ValueError If there are no tables in the sheet. KeyError If the provided table does not exist in the sheet. Source code in janitor/io.py def xlsx_table( path: Union[str, Workbook], sheetname: str, table: Union[str, list, tuple] = None, ) -> Union[pd.DataFrame, dict]: \"\"\" Returns a DataFrame of values in a table in the Excel file. This applies to an Excel file, where the data range is explicitly specified as a Microsoft Excel table. If there is a single table in the sheet, or a string is provided as an argument to the `table` parameter, a pandas DataFrame is returned; if there is more than one table in the sheet, and the `table` argument is `None`, or a list/tuple of names, a dictionary of DataFrames is returned, where the keys of the dictionary are the table names. Example: >>> import pandas as pd >>> from janitor import xlsx_table >>> filename=\"../pyjanitor/tests/test_data/016-MSPTDA-Excel.xlsx\" # single table >>> xlsx_table(filename, sheetname='Tables', table='dCategory') CategoryID Category 0 1 Beginner 1 2 Advanced 2 3 Freestyle 3 4 Competition 4 5 Long Distance # multiple tables: >>> out=xlsx_table(filename, sheetname=\"Tables\", table=[\"dCategory\", \"dSalesReps\"]) >>> out[\"dCategory\"] CategoryID Category 0 1 Beginner 1 2 Advanced 2 3 Freestyle 3 4 Competition 4 5 Long Distance >>> out[\"dSalesReps\"].head(3) SalesRepID SalesRep Region 0 1 Sioux Radcoolinator NW 1 2 Tyrone Smithe NE 2 3 Chantel Zoya SW :param path: Path to the Excel File. It can also be an openpyxl Workbook. :param sheetname: Name of the sheet from which the tables are to be extracted. :param table: Name of a table, or list of tables in the sheet. :returns: A pandas DataFrame, or a dictionary of DataFrames, if there are multiple arguments for the `table` parameter, or the argument to `table` is `None`. :raises AttributeError: If a workbook is provided, and is a ReadOnlyWorksheet. :raises ValueError: If there are no tables in the sheet. :raises KeyError: If the provided table does not exist in the sheet. \"\"\" # noqa : E501 try: from openpyxl import load_workbook from openpyxl.workbook.workbook import Workbook except ImportError: import_message( submodule=\"io\", package=\"openpyxl\", conda_channel=\"conda-forge\", pip_install=True, ) if isinstance(path, Workbook): ws = path[sheetname] else: ws = load_workbook( filename=path, read_only=False, keep_links=False, data_only=True ) ws = ws[sheetname] try: contents = ws.tables except AttributeError as error: raise AttributeError( \"Accessing the tables is not supported for ReadOnlyWorksheet\" ) from error if not contents: raise ValueError(f\"There is no table in '{sheetname}' sheet.\") class TableArgs(NamedTuple): \"\"\" Named Tuple to easily index values from the tables in the sheet. \"\"\" table_name: str ref: str headerRowCount: int if isinstance(table, str): table = [table] if table is not None: check(\"table\", table, [str, list, tuple]) try: data = [] for key in table: outcome = TableArgs( key, contents[key].ref, contents[key].headerRowCount ) data.append(outcome) except KeyError as error: raise KeyError( f\"Table {error} is not in the '{sheetname}' sheet.\" ) from error else: data = ( TableArgs(key, contents[key].ref, contents[key].headerRowCount) for key in contents ) frame = {} for table_arg in data: content = [[cell.value for cell in row] for row in ws[table_arg.ref]] if table_arg.headerRowCount: header, *content = content else: header = [f\"C{num}\" for num in range(len(content[0]))] frame[table_arg.table_name] = pd.DataFrame( content, columns=header, copy=False ) if len(frame) == 1: _, frame = frame.popitem() return frame","title":"Input/Output (io)"},{"location":"api/io/#inputoutput-io","text":"","title":"Input/Output (io)"},{"location":"api/io/#janitor.io.read_commandline","text":"Read a CSV file based on a command-line command. For example, you may wish to run the following command on sep-quarter.csv before reading it into a pandas DataFrame: cat sep-quarter.csv | grep .SEA1AA In this case, you can use the following Python code to load the dataframe: import janitor as jn df = jn.read_commandline(\"cat data.csv | grep .SEA1AA\") This function assumes that your command line command will return an output that is parsable using pandas.read_csv and StringIO. We default to using pd.read_csv underneath the hood. Keyword arguments are passed through to read_csv. Parameters: Name Type Description Default cmd str Shell command to preprocess a file on disk. required kwargs Keyword arguments that are passed through to pd.read_csv() . {} Returns: Type Description pd.DataFrame A pandas DataFrame parsed from the stdout of the underlying shell. Source code in janitor/io.py def read_commandline(cmd: str, **kwargs) -> pd.DataFrame: \"\"\" Read a CSV file based on a command-line command. For example, you may wish to run the following command on `sep-quarter.csv` before reading it into a pandas DataFrame: ```bash cat sep-quarter.csv | grep .SEA1AA ``` In this case, you can use the following Python code to load the dataframe: ```python import janitor as jn df = jn.read_commandline(\"cat data.csv | grep .SEA1AA\") ``` This function assumes that your command line command will return an output that is parsable using `pandas.read_csv` and StringIO. We default to using `pd.read_csv` underneath the hood. Keyword arguments are passed through to read_csv. :param cmd: Shell command to preprocess a file on disk. :param kwargs: Keyword arguments that are passed through to `pd.read_csv()`. :returns: A pandas DataFrame parsed from the stdout of the underlying shell. \"\"\" check(\"cmd\", cmd, [str]) # adding check=True ensures that an explicit, clear error # is raised, so that the user can see the reason for the failure outcome = subprocess.run( cmd, shell=True, capture_output=True, text=True, check=True ) return pd.read_csv(StringIO(outcome.stdout), **kwargs)","title":"read_commandline()"},{"location":"api/io/#janitor.io.read_csvs","text":"Read multiple CSV files and return a dictionary of DataFrames, or one concatenated DataFrame. Parameters: Name Type Description Default files_path Union[str, Iterable[str]] The filepath pattern matching the CSV files. Accepts regular expressions, with or without .csv extension. Also accepts iterable of file paths. required separate_df bool If False (default), returns a single Dataframe with the concatenation of the csv files. If True , returns a dictionary of separate DataFrames for each CSV file. False kwargs Keyword arguments to pass into the original pandas read_csv . {} Returns: Type Description Union[pd.DataFrame, dict] DataFrame of concatenated DataFrames or dictionary of DataFrames. Exceptions: Type Description JanitorError if None provided for files_path . JanitorError if length of files_path is 0 . ValueError if no CSV files exist in files_path . ValueError if columns in input CSV files do not match. Source code in janitor/io.py @deprecated_alias(seperate_df=\"separate_df\", filespath=\"files_path\") def read_csvs( files_path: Union[str, Iterable[str]], separate_df: bool = False, **kwargs ) -> Union[pd.DataFrame, dict]: \"\"\" Read multiple CSV files and return a dictionary of DataFrames, or one concatenated DataFrame. :param files_path: The filepath pattern matching the CSV files. Accepts regular expressions, with or without `.csv` extension. Also accepts iterable of file paths. :param separate_df: If `False` (default), returns a single Dataframe with the concatenation of the csv files. If `True`, returns a dictionary of separate DataFrames for each CSV file. :param kwargs: Keyword arguments to pass into the original pandas `read_csv`. :returns: DataFrame of concatenated DataFrames or dictionary of DataFrames. :raises JanitorError: if `None` provided for `files_path`. :raises JanitorError: if length of `files_path` is `0`. :raises ValueError: if no CSV files exist in `files_path`. :raises ValueError: if columns in input CSV files do not match. \"\"\" # Sanitize input if files_path is None: raise JanitorError(\"`None` provided for `files_path`\") if not files_path: raise JanitorError(\"0 length `files_path` provided\") # Read the csv files # String to file/folder or file pattern provided if isinstance(files_path, str): dfs_dict = { os.path.basename(f): pd.read_csv(f, **kwargs) for f in glob(files_path) } # Iterable of file paths provided else: dfs_dict = { os.path.basename(f): pd.read_csv(f, **kwargs) for f in files_path } # Check if dataframes have been read if not dfs_dict: raise ValueError(\"No CSV files to read with the given `files_path`\") # Concatenate the dataframes if requested (default) col_names = list(dfs_dict.values())[0].columns # noqa: PD011 if not separate_df: # If columns do not match raise an error for df in dfs_dict.values(): # noqa: PD011 if not all(df.columns == col_names): raise ValueError( \"Columns in input CSV files do not match.\" \"Files cannot be concatenated.\" ) return pd.concat( list(dfs_dict.values()), ignore_index=True, sort=False, # noqa: PD011 copy=False, ) return dfs_dict","title":"read_csvs()"},{"location":"api/io/#janitor.io.xlsx_cells","text":"Imports data from spreadsheet without coercing it into a rectangle. Each cell is represented by a row in a dataframe, and includes the cell's coordinates, the value, row and column position. The cell formatting (fill, font, border, etc) can also be accessed; usually this is returned as a dictionary in the cell, and the specific cell format attribute can be accessed using pd.Series.str.get . Inspiration for this comes from R's tidyxl package. Example: >>> import pandas as pd >>> from janitor import xlsx_cells >>> pd.set_option(\"display.max_columns\", None) >>> pd.set_option(\"display.expand_frame_repr\", False) >>> pd.set_option(\"max_colwidth\", None) >>> filename = \"../pyjanitor/tests/test_data/worked-examples.xlsx\" # Each cell is returned as a row: >>> xlsx_cells(filename, sheetnames=\"highlights\") value internal_value coordinate row column data_type is_date number_format 0 Age Age A1 1 1 s False General 1 Height Height B1 1 2 s False General 2 1 1 A2 2 1 n False General 3 2 2 B2 2 2 n False General 4 3 3 A3 3 1 n False General 5 4 4 B3 3 2 n False General 6 5 5 A4 4 1 n False General 7 6 6 B4 4 2 n False General # Access cell formatting such as fill : >>> out=xlsx_cells(filename, sheetnames=\"highlights\", fill=True).select_columns(\"value\", \"fill\") >>> out value fill 0 Age {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 1 Height {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 2 1 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 3 2 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 4 3 {'patternType': 'solid', 'fgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}} 5 4 {'patternType': 'solid', 'fgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}} 6 5 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 7 6 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} # specific cell attributes can be accessed by using Pandas' series.str.get : >>> out.fill.str.get(\"fgColor\").str.get(\"rgb\") 0 00000000 1 00000000 2 00000000 3 00000000 4 FFFFFF00 5 FFFFFF00 6 00000000 7 00000000 Name: fill, dtype: object Parameters: Name Type Description Default path Union[str, Workbook] Path to the Excel File. It can also be an openpyxl Workbook. required sheetnames Union[str, list, tuple] Names of the sheets from which the cells are to be extracted. If None , all the sheets in the file are extracted; if it is a string, or list or tuple, only the specified sheets are extracted. None start_point Union[str, int] start coordinates of the Excel sheet. This is useful if the user is only interested in a subsection of the sheet. If start_point is provided, end_point must be provided as well. None end_point Union[str, int] end coordinates of the Excel sheet. This is useful if the user is only interested in a subsection of the sheet. If end_point is provided, start_point must be provided as well. None read_only bool Determines if the entire file is loaded in memory, or streamed. For memory efficiency, read_only should be set to True . Some cell properties like comment , can only be accessed by setting read_only to False . True include_blank_cells bool Determines if cells without a value should be included. True fill bool If True , return fill properties of the cell. It is usually returned as a dictionary. False font bool If True , return font properties of the cell. It is usually returned as a dictionary. False alignment bool If True , return alignment properties of the cell. It is usually returned as a dictionary. False border bool If True , return border properties of the cell. It is usually returned as a dictionary. False protection bool If True , return protection properties of the cell. It is usually returned as a dictionary. False comment bool If True , return comment properties of the cell. It is usually returned as a dictionary. False kwargs Any other attributes of the cell, that can be accessed from openpyxl. {} Returns: Type Description Union[dict, pd.DataFrame] A pandas DataFrame, or a dictionary of DataFrames. Exceptions: Type Description ValueError If kwargs is provided, and one of the keys is a default column. AttributeError If kwargs is provided and any of the keys is not a openpyxl cell attribute. Source code in janitor/io.py def xlsx_cells( path: Union[str, Workbook], sheetnames: Union[str, list, tuple] = None, start_point: Union[str, int] = None, end_point: Union[str, int] = None, read_only: bool = True, include_blank_cells: bool = True, fill: bool = False, font: bool = False, alignment: bool = False, border: bool = False, protection: bool = False, comment: bool = False, **kwargs, ) -> Union[dict, pd.DataFrame]: \"\"\" Imports data from spreadsheet without coercing it into a rectangle. Each cell is represented by a row in a dataframe, and includes the cell's coordinates, the value, row and column position. The cell formatting (fill, font, border, etc) can also be accessed; usually this is returned as a dictionary in the cell, and the specific cell format attribute can be accessed using `pd.Series.str.get`. Inspiration for this comes from R's [tidyxl][link] package. [link]: https://nacnudus.github.io/tidyxl/reference/tidyxl.html Example: >>> import pandas as pd >>> from janitor import xlsx_cells >>> pd.set_option(\"display.max_columns\", None) >>> pd.set_option(\"display.expand_frame_repr\", False) >>> pd.set_option(\"max_colwidth\", None) >>> filename = \"../pyjanitor/tests/test_data/worked-examples.xlsx\" # Each cell is returned as a row: >>> xlsx_cells(filename, sheetnames=\"highlights\") value internal_value coordinate row column data_type is_date number_format 0 Age Age A1 1 1 s False General 1 Height Height B1 1 2 s False General 2 1 1 A2 2 1 n False General 3 2 2 B2 2 2 n False General 4 3 3 A3 3 1 n False General 5 4 4 B3 3 2 n False General 6 5 5 A4 4 1 n False General 7 6 6 B4 4 2 n False General # Access cell formatting such as fill : >>> out=xlsx_cells(filename, sheetnames=\"highlights\", fill=True).select_columns(\"value\", \"fill\") >>> out value fill 0 Age {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 1 Height {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 2 1 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 3 2 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 4 3 {'patternType': 'solid', 'fgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}} 5 4 {'patternType': 'solid', 'fgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}} 6 5 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} 7 6 {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}} # specific cell attributes can be accessed by using Pandas' series.str.get : >>> out.fill.str.get(\"fgColor\").str.get(\"rgb\") 0 00000000 1 00000000 2 00000000 3 00000000 4 FFFFFF00 5 FFFFFF00 6 00000000 7 00000000 Name: fill, dtype: object :param path: Path to the Excel File. It can also be an openpyxl Workbook. :param sheetnames: Names of the sheets from which the cells are to be extracted. If `None`, all the sheets in the file are extracted; if it is a string, or list or tuple, only the specified sheets are extracted. :param start_point: start coordinates of the Excel sheet. This is useful if the user is only interested in a subsection of the sheet. If start_point is provided, end_point must be provided as well. :param end_point: end coordinates of the Excel sheet. This is useful if the user is only interested in a subsection of the sheet. If end_point is provided, start_point must be provided as well. :param read_only: Determines if the entire file is loaded in memory, or streamed. For memory efficiency, read_only should be set to `True`. Some cell properties like `comment`, can only be accessed by setting `read_only` to `False`. :param include_blank_cells: Determines if cells without a value should be included. :param fill: If `True`, return fill properties of the cell. It is usually returned as a dictionary. :param font: If `True`, return font properties of the cell. It is usually returned as a dictionary. :param alignment: If `True`, return alignment properties of the cell. It is usually returned as a dictionary. :param border: If `True`, return border properties of the cell. It is usually returned as a dictionary. :param protection: If `True`, return protection properties of the cell. It is usually returned as a dictionary. :param comment: If `True`, return comment properties of the cell. It is usually returned as a dictionary. :param kwargs: Any other attributes of the cell, that can be accessed from openpyxl. :raises ValueError: If kwargs is provided, and one of the keys is a default column. :raises AttributeError: If kwargs is provided and any of the keys is not a openpyxl cell attribute. :returns: A pandas DataFrame, or a dictionary of DataFrames. \"\"\" # noqa : E501 try: from openpyxl import load_workbook from openpyxl.cell.read_only import ReadOnlyCell from openpyxl.cell.cell import Cell from openpyxl.workbook.workbook import Workbook except ImportError: import_message( submodule=\"io\", package=\"openpyxl\", conda_channel=\"conda-forge\", pip_install=True, ) path_is_workbook = isinstance(path, Workbook) if not path_is_workbook: # for memory efficiency, read_only is set to True # if comments is True, read_only has to be False, # as lazy loading is not enabled for comments if comment and read_only: raise ValueError( \"To access comments, kindly set 'read_only' to False.\" ) path = load_workbook( filename=path, read_only=read_only, keep_links=False ) # start_point and end_point applies if the user is interested in # only a subset of the Excel File and knows the coordinates if start_point or end_point: check(\"start_point\", start_point, [str, int]) check(\"end_point\", end_point, [str, int]) defaults = ( \"value\", \"internal_value\", \"coordinate\", \"row\", \"column\", \"data_type\", \"is_date\", \"number_format\", ) parameters = { \"fill\": fill, \"font\": font, \"alignment\": alignment, \"border\": border, \"protection\": protection, \"comment\": comment, } if kwargs: if path_is_workbook: if path.read_only: _cell = ReadOnlyCell else: _cell = Cell else: if read_only: _cell = ReadOnlyCell else: _cell = Cell attrs = { attr for attr, _ in inspect.getmembers(_cell, not (inspect.isroutine)) if not attr.startswith(\"_\") } for key in kwargs: if key in defaults: raise ValueError( f\"{key} is part of the default attributes \" \"returned as a column.\" ) elif key not in attrs: raise AttributeError( f\"{key} is not a recognized attribute of {_cell}.\" ) parameters.update(kwargs) if not sheetnames: sheetnames = path.sheetnames elif isinstance(sheetnames, str): sheetnames = [sheetnames] else: check(\"sheetnames\", sheetnames, [str, list, tuple]) out = { sheetname: _xlsx_cells( path[sheetname], defaults, parameters, start_point, end_point, include_blank_cells, ) for sheetname in sheetnames } if len(out) == 1: _, out = out.popitem() if (not path_is_workbook) and path.read_only: path.close() return out","title":"xlsx_cells()"},{"location":"api/io/#janitor.io.xlsx_table","text":"Returns a DataFrame of values in a table in the Excel file. This applies to an Excel file, where the data range is explicitly specified as a Microsoft Excel table. If there is a single table in the sheet, or a string is provided as an argument to the table parameter, a pandas DataFrame is returned; if there is more than one table in the sheet, and the table argument is None , or a list/tuple of names, a dictionary of DataFrames is returned, where the keys of the dictionary are the table names. Example: >>> import pandas as pd >>> from janitor import xlsx_table >>> filename=\"../pyjanitor/tests/test_data/016-MSPTDA-Excel.xlsx\" # single table >>> xlsx_table(filename, sheetname='Tables', table='dCategory') CategoryID Category 0 1 Beginner 1 2 Advanced 2 3 Freestyle 3 4 Competition 4 5 Long Distance # multiple tables: >>> out=xlsx_table(filename, sheetname=\"Tables\", table=[\"dCategory\", \"dSalesReps\"]) >>> out[\"dCategory\"] CategoryID Category 0 1 Beginner 1 2 Advanced 2 3 Freestyle 3 4 Competition 4 5 Long Distance >>> out[\"dSalesReps\"].head(3) SalesRepID SalesRep Region 0 1 Sioux Radcoolinator NW 1 2 Tyrone Smithe NE 2 3 Chantel Zoya SW Parameters: Name Type Description Default path Union[str, Workbook] Path to the Excel File. It can also be an openpyxl Workbook. required sheetname str Name of the sheet from which the tables are to be extracted. required table Union[str, list, tuple] Name of a table, or list of tables in the sheet. None Returns: Type Description Union[pd.DataFrame, dict] A pandas DataFrame, or a dictionary of DataFrames, if there are multiple arguments for the table parameter, or the argument to table is None . Exceptions: Type Description AttributeError If a workbook is provided, and is a ReadOnlyWorksheet. ValueError If there are no tables in the sheet. KeyError If the provided table does not exist in the sheet. Source code in janitor/io.py def xlsx_table( path: Union[str, Workbook], sheetname: str, table: Union[str, list, tuple] = None, ) -> Union[pd.DataFrame, dict]: \"\"\" Returns a DataFrame of values in a table in the Excel file. This applies to an Excel file, where the data range is explicitly specified as a Microsoft Excel table. If there is a single table in the sheet, or a string is provided as an argument to the `table` parameter, a pandas DataFrame is returned; if there is more than one table in the sheet, and the `table` argument is `None`, or a list/tuple of names, a dictionary of DataFrames is returned, where the keys of the dictionary are the table names. Example: >>> import pandas as pd >>> from janitor import xlsx_table >>> filename=\"../pyjanitor/tests/test_data/016-MSPTDA-Excel.xlsx\" # single table >>> xlsx_table(filename, sheetname='Tables', table='dCategory') CategoryID Category 0 1 Beginner 1 2 Advanced 2 3 Freestyle 3 4 Competition 4 5 Long Distance # multiple tables: >>> out=xlsx_table(filename, sheetname=\"Tables\", table=[\"dCategory\", \"dSalesReps\"]) >>> out[\"dCategory\"] CategoryID Category 0 1 Beginner 1 2 Advanced 2 3 Freestyle 3 4 Competition 4 5 Long Distance >>> out[\"dSalesReps\"].head(3) SalesRepID SalesRep Region 0 1 Sioux Radcoolinator NW 1 2 Tyrone Smithe NE 2 3 Chantel Zoya SW :param path: Path to the Excel File. It can also be an openpyxl Workbook. :param sheetname: Name of the sheet from which the tables are to be extracted. :param table: Name of a table, or list of tables in the sheet. :returns: A pandas DataFrame, or a dictionary of DataFrames, if there are multiple arguments for the `table` parameter, or the argument to `table` is `None`. :raises AttributeError: If a workbook is provided, and is a ReadOnlyWorksheet. :raises ValueError: If there are no tables in the sheet. :raises KeyError: If the provided table does not exist in the sheet. \"\"\" # noqa : E501 try: from openpyxl import load_workbook from openpyxl.workbook.workbook import Workbook except ImportError: import_message( submodule=\"io\", package=\"openpyxl\", conda_channel=\"conda-forge\", pip_install=True, ) if isinstance(path, Workbook): ws = path[sheetname] else: ws = load_workbook( filename=path, read_only=False, keep_links=False, data_only=True ) ws = ws[sheetname] try: contents = ws.tables except AttributeError as error: raise AttributeError( \"Accessing the tables is not supported for ReadOnlyWorksheet\" ) from error if not contents: raise ValueError(f\"There is no table in '{sheetname}' sheet.\") class TableArgs(NamedTuple): \"\"\" Named Tuple to easily index values from the tables in the sheet. \"\"\" table_name: str ref: str headerRowCount: int if isinstance(table, str): table = [table] if table is not None: check(\"table\", table, [str, list, tuple]) try: data = [] for key in table: outcome = TableArgs( key, contents[key].ref, contents[key].headerRowCount ) data.append(outcome) except KeyError as error: raise KeyError( f\"Table {error} is not in the '{sheetname}' sheet.\" ) from error else: data = ( TableArgs(key, contents[key].ref, contents[key].headerRowCount) for key in contents ) frame = {} for table_arg in data: content = [[cell.value for cell in row] for row in ws[table_arg.ref]] if table_arg.headerRowCount: header, *content = content else: header = [f\"C{num}\" for num in range(len(content[0]))] frame[table_arg.table_name] = pd.DataFrame( content, columns=header, copy=False ) if len(frame) == 1: _, frame = frame.popitem() return frame","title":"xlsx_table()"},{"location":"api/math/","text":"Math Miscellaneous mathematical operators. Lazy loading used here to speed up imports. ecdf(s) Return cumulative distribution of values in a series. Intended to be used with the following pattern: df = pd.DataFrame(...) # Obtain ECDF values to be plotted x, y = df[\"column_name\"].ecdf() # Plot ECDF values plt.scatter(x, y) Null values must be dropped from the series, otherwise a ValueError is raised. Also, if the dtype of the series is not numeric, a TypeError is raised. >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 4, 0, 1, 2, 1, 1, 3]) >>> x, y = s.ecdf() >>> x # doctest: +SKIP array([0, 0, 1, 1, 1, 2, 3, 4]) >>> y # doctest: +SKIP array([0.125, 0.25 , 0.375, 0.5 , 0.625, 0.75 , 0.875, 1. ]) Parameters: Name Type Description Default s Series A pandas series. dtype should be numeric. required Returns: Type Description Tuple[numpy.ndarray, numpy.ndarray] (x, y) . x : sorted array of values. y : cumulative fraction of data points with value x or lower. Exceptions: Type Description TypeError if series is not numeric. ValueError if series contains nulls. Source code in janitor/math.py @pf.register_series_method def ecdf(s: pd.Series) -> Tuple[np.ndarray, np.ndarray]: \"\"\" Return cumulative distribution of values in a series. Intended to be used with the following pattern: ```python df = pd.DataFrame(...) # Obtain ECDF values to be plotted x, y = df[\"column_name\"].ecdf() # Plot ECDF values plt.scatter(x, y) ``` Null values must be dropped from the series, otherwise a `ValueError` is raised. Also, if the `dtype` of the series is not numeric, a `TypeError` is raised. >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 4, 0, 1, 2, 1, 1, 3]) >>> x, y = s.ecdf() >>> x # doctest: +SKIP array([0, 0, 1, 1, 1, 2, 3, 4]) >>> y # doctest: +SKIP array([0.125, 0.25 , 0.375, 0.5 , 0.625, 0.75 , 0.875, 1. ]) :param s: A pandas series. `dtype` should be numeric. :returns: `(x, y)`. `x`: sorted array of values. `y`: cumulative fraction of data points with value `x` or lower. :raises TypeError: if series is not numeric. :raises ValueError: if series contains nulls. \"\"\" if not pdtypes.is_numeric_dtype(s): raise TypeError(f\"series {s.name} must be numeric!\") if not s.isna().sum() == 0: raise ValueError(f\"series {s.name} contains nulls. Please drop them.\") n = len(s) x = np.sort(s) y = np.arange(1, n + 1) / n return x, y exp(s) Take the exponential transform of the series. >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.exp() 0 1.000000 1 2.718282 2 20.085537 Name: numbers, dtype: float64 Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def exp(s: pd.Series) -> pd.Series: \"\"\" Take the exponential transform of the series. >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.exp() 0 1.000000 1 2.718282 2 20.085537 Name: numbers, dtype: float64 :param s: Input Series. :return: Transformed Series. \"\"\" return np.exp(s) log(s, error='warn') Take natural logarithm of the Series. Each value in the series should be positive. Use error to control the behavior if there are nonpositive entries in the series. >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.log(error=\"ignore\") 0 NaN 1 0.000000 2 1.098612 Name: numbers, dtype: float64 Parameters: Name Type Description Default s Series Input Series. required error str Determines behavior when taking the log of nonpositive entries. If 'warn' then a RuntimeWarning is thrown. If 'raise' , then a RuntimeError is thrown. Otherwise, nothing is thrown and log of nonpositive values is np.nan ; defaults to 'warn' . 'warn' Returns: Type Description Series Transformed Series. Exceptions: Type Description RuntimeError Raised when there are nonpositive values in the Series and error='raise' . Source code in janitor/math.py @pf.register_series_method def log(s: pd.Series, error: str = \"warn\") -> pd.Series: \"\"\" Take natural logarithm of the Series. Each value in the series should be positive. Use `error` to control the behavior if there are nonpositive entries in the series. >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.log(error=\"ignore\") 0 NaN 1 0.000000 2 1.098612 Name: numbers, dtype: float64 :param s: Input Series. :param error: Determines behavior when taking the log of nonpositive entries. If `'warn'` then a `RuntimeWarning` is thrown. If `'raise'`, then a `RuntimeError` is thrown. Otherwise, nothing is thrown and log of nonpositive values is `np.nan`; defaults to `'warn'`. :raises RuntimeError: Raised when there are nonpositive values in the Series and `error='raise'`. :return: Transformed Series. \"\"\" s = s.copy() nonpositive = s <= 0 if (nonpositive).any(): msg = f\"Log taken on {nonpositive.sum()} nonpositive value(s)\" if error.lower() == \"warn\": warnings.warn(msg, RuntimeWarning) if error.lower() == \"raise\": raise RuntimeError(msg) else: pass s[nonpositive] = np.nan return np.log(s) logit(s, error='warn') Take logit transform of the Series where: logit(p) = log(p/(1-p)) Each value in the series should be between 0 and 1. Use error to control the behavior if any series entries are outside of (0, 1). >>> import pandas as pd >>> import janitor >>> s = pd.Series([0.1, 0.5, 0.9], name=\"numbers\") >>> s.logit() 0 -2.197225 1 0.000000 2 2.197225 Name: numbers, dtype: float64 Parameters: Name Type Description Default s Series Input Series. required error str Determines behavior when s is outside of (0, 1) . If 'warn' then a RuntimeWarning is thrown. If 'raise' , then a RuntimeError is thrown. Otherwise, nothing is thrown and np.nan is returned for the problematic entries; defaults to 'warn' . 'warn' Returns: Type Description Series Transformed Series. Exceptions: Type Description RuntimeError if error is set to 'raise' . Source code in janitor/math.py @pf.register_series_method def logit(s: pd.Series, error: str = \"warn\") -> pd.Series: \"\"\" Take logit transform of the Series where: ```python logit(p) = log(p/(1-p)) ``` Each value in the series should be between 0 and 1. Use `error` to control the behavior if any series entries are outside of (0, 1). >>> import pandas as pd >>> import janitor >>> s = pd.Series([0.1, 0.5, 0.9], name=\"numbers\") >>> s.logit() 0 -2.197225 1 0.000000 2 2.197225 Name: numbers, dtype: float64 :param s: Input Series. :param error: Determines behavior when `s` is outside of `(0, 1)`. If `'warn'` then a `RuntimeWarning` is thrown. If `'raise'`, then a `RuntimeError` is thrown. Otherwise, nothing is thrown and `np.nan` is returned for the problematic entries; defaults to `'warn'`. :return: Transformed Series. :raises RuntimeError: if `error` is set to `'raise'`. \"\"\" s = s.copy() outside_support = (s <= 0) | (s >= 1) if (outside_support).any(): msg = f\"{outside_support.sum()} value(s) are outside of (0, 1)\" if error.lower() == \"warn\": warnings.warn(msg, RuntimeWarning) if error.lower() == \"raise\": raise RuntimeError(msg) else: pass s[outside_support] = np.nan return scipy_special.logit(s) normal_cdf(s) Transforms the Series via the CDF of the Normal distribution. >>> import pandas as pd >>> import janitor >>> s = pd.Series([-1, 0, 3], name=\"numbers\") >>> s.normal_cdf() 0 0.158655 1 0.500000 2 0.998650 dtype: float64 Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def normal_cdf(s: pd.Series) -> pd.Series: \"\"\" Transforms the Series via the CDF of the Normal distribution. >>> import pandas as pd >>> import janitor >>> s = pd.Series([-1, 0, 3], name=\"numbers\") >>> s.normal_cdf() 0 0.158655 1 0.500000 2 0.998650 dtype: float64 :param s: Input Series. :return: Transformed Series. \"\"\" return pd.Series(ss.norm.cdf(s), index=s.index) probit(s, error='warn') Transforms the Series via the inverse CDF of the Normal distribution. Each value in the series should be between 0 and 1. Use error to control the behavior if any series entries are outside of (0, 1). >>> import pandas as pd >>> import janitor >>> s = pd.Series([0.1, 0.5, 0.8], name=\"numbers\") >>> s.probit() 0 -1.281552 1 0.000000 2 0.841621 dtype: float64 Parameters: Name Type Description Default s Series Input Series. required error str Determines behavior when s is outside of (0, 1) . If 'warn' then a RuntimeWarning is thrown. If 'raise' , then a RuntimeError is thrown. Otherwise, nothing is thrown and np.nan is returned for the problematic entries; defaults to 'warn' . 'warn' Returns: Type Description Series Transformed Series Exceptions: Type Description RuntimeError Raised when there are problematic values in the Series and error='raise' . Source code in janitor/math.py @pf.register_series_method def probit(s: pd.Series, error: str = \"warn\") -> pd.Series: \"\"\" Transforms the Series via the inverse CDF of the Normal distribution. Each value in the series should be between 0 and 1. Use `error` to control the behavior if any series entries are outside of (0, 1). >>> import pandas as pd >>> import janitor >>> s = pd.Series([0.1, 0.5, 0.8], name=\"numbers\") >>> s.probit() 0 -1.281552 1 0.000000 2 0.841621 dtype: float64 :param s: Input Series. :param error: Determines behavior when `s` is outside of `(0, 1)`. If `'warn'` then a `RuntimeWarning` is thrown. If `'raise'`, then a `RuntimeError` is thrown. Otherwise, nothing is thrown and `np.nan` is returned for the problematic entries; defaults to `'warn'`. :raises RuntimeError: Raised when there are problematic values in the Series and `error='raise'`. :return: Transformed Series \"\"\" s = s.copy() outside_support = (s <= 0) | (s >= 1) if (outside_support).any(): msg = f\"{outside_support.sum()} value(s) are outside of (0, 1)\" if error.lower() == \"warn\": warnings.warn(msg, RuntimeWarning) if error.lower() == \"raise\": raise RuntimeError(msg) else: pass s[outside_support] = np.nan with np.errstate(all=\"ignore\"): out = pd.Series(ss.norm.ppf(s), index=s.index) return out sigmoid(s) Take the sigmoid transform of the series where: sigmoid(x) = 1 / (1 + exp(-x)) >>> import pandas as pd >>> import janitor >>> s = pd.Series([-1, 0, 4], name=\"numbers\") >>> s.sigmoid() 0 0.268941 1 0.500000 2 0.982014 Name: numbers, dtype: float64 Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def sigmoid(s: pd.Series) -> pd.Series: \"\"\" Take the sigmoid transform of the series where: ```python sigmoid(x) = 1 / (1 + exp(-x)) ``` >>> import pandas as pd >>> import janitor >>> s = pd.Series([-1, 0, 4], name=\"numbers\") >>> s.sigmoid() 0 0.268941 1 0.500000 2 0.982014 Name: numbers, dtype: float64 :param s: Input Series. :return: Transformed Series. \"\"\" return scipy_special.expit(s) softmax(s) Take the softmax transform of the series. The softmax function transforms each element of a collection by computing the exponential of each element divided by the sum of the exponentials of all the elements. That is, if x is a one-dimensional numpy array or pandas Series: softmax(x) = exp(x)/sum(exp(x)) >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.softmax() 0 0.042010 1 0.114195 2 0.843795 Name: numbers, dtype: float64 Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def softmax(s: pd.Series) -> pd.Series: \"\"\" Take the softmax transform of the series. The softmax function transforms each element of a collection by computing the exponential of each element divided by the sum of the exponentials of all the elements. That is, if x is a one-dimensional numpy array or pandas Series: ```python softmax(x) = exp(x)/sum(exp(x)) ``` >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.softmax() 0 0.042010 1 0.114195 2 0.843795 Name: numbers, dtype: float64 :param s: Input Series. :return: Transformed Series. \"\"\" return pd.Series(scipy_special.softmax(s), index=s.index, name=s.name) z_score(s, moments_dict=None, keys=('mean', 'std')) Transforms the Series into z-scores where: z = (s - s.mean()) / s.std() >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.z_score() 0 -0.872872 1 -0.218218 2 1.091089 Name: numbers, dtype: float64 Parameters: Name Type Description Default s Series Input Series. required moments_dict dict If not None , then the mean and standard deviation used to compute the z-score transformation is saved as entries in moments_dict with keys determined by the keys argument; defaults to None . None keys Tuple[str, str] Determines the keys saved in moments_dict if moments are saved; defaults to ( 'mean' , 'std' ). ('mean', 'std') Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def z_score( s: pd.Series, moments_dict: dict = None, keys: Tuple[str, str] = (\"mean\", \"std\"), ) -> pd.Series: \"\"\" Transforms the Series into z-scores where: ```python z = (s - s.mean()) / s.std() ``` >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.z_score() 0 -0.872872 1 -0.218218 2 1.091089 Name: numbers, dtype: float64 :param s: Input Series. :param moments_dict: If not `None`, then the mean and standard deviation used to compute the z-score transformation is saved as entries in `moments_dict` with keys determined by the `keys` argument; defaults to `None`. :param keys: Determines the keys saved in `moments_dict` if moments are saved; defaults to (`'mean'`, `'std'`). :return: Transformed Series. \"\"\" mean = s.mean() std = s.std() if std == 0: return 0 if moments_dict is not None: moments_dict[keys[0]] = mean moments_dict[keys[1]] = std return (s - mean) / std","title":"Math"},{"location":"api/math/#math","text":"Miscellaneous mathematical operators. Lazy loading used here to speed up imports.","title":"Math"},{"location":"api/math/#janitor.math.ecdf","text":"Return cumulative distribution of values in a series. Intended to be used with the following pattern: df = pd.DataFrame(...) # Obtain ECDF values to be plotted x, y = df[\"column_name\"].ecdf() # Plot ECDF values plt.scatter(x, y) Null values must be dropped from the series, otherwise a ValueError is raised. Also, if the dtype of the series is not numeric, a TypeError is raised. >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 4, 0, 1, 2, 1, 1, 3]) >>> x, y = s.ecdf() >>> x # doctest: +SKIP array([0, 0, 1, 1, 1, 2, 3, 4]) >>> y # doctest: +SKIP array([0.125, 0.25 , 0.375, 0.5 , 0.625, 0.75 , 0.875, 1. ]) Parameters: Name Type Description Default s Series A pandas series. dtype should be numeric. required Returns: Type Description Tuple[numpy.ndarray, numpy.ndarray] (x, y) . x : sorted array of values. y : cumulative fraction of data points with value x or lower. Exceptions: Type Description TypeError if series is not numeric. ValueError if series contains nulls. Source code in janitor/math.py @pf.register_series_method def ecdf(s: pd.Series) -> Tuple[np.ndarray, np.ndarray]: \"\"\" Return cumulative distribution of values in a series. Intended to be used with the following pattern: ```python df = pd.DataFrame(...) # Obtain ECDF values to be plotted x, y = df[\"column_name\"].ecdf() # Plot ECDF values plt.scatter(x, y) ``` Null values must be dropped from the series, otherwise a `ValueError` is raised. Also, if the `dtype` of the series is not numeric, a `TypeError` is raised. >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 4, 0, 1, 2, 1, 1, 3]) >>> x, y = s.ecdf() >>> x # doctest: +SKIP array([0, 0, 1, 1, 1, 2, 3, 4]) >>> y # doctest: +SKIP array([0.125, 0.25 , 0.375, 0.5 , 0.625, 0.75 , 0.875, 1. ]) :param s: A pandas series. `dtype` should be numeric. :returns: `(x, y)`. `x`: sorted array of values. `y`: cumulative fraction of data points with value `x` or lower. :raises TypeError: if series is not numeric. :raises ValueError: if series contains nulls. \"\"\" if not pdtypes.is_numeric_dtype(s): raise TypeError(f\"series {s.name} must be numeric!\") if not s.isna().sum() == 0: raise ValueError(f\"series {s.name} contains nulls. Please drop them.\") n = len(s) x = np.sort(s) y = np.arange(1, n + 1) / n return x, y","title":"ecdf()"},{"location":"api/math/#janitor.math.exp","text":"Take the exponential transform of the series. >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.exp() 0 1.000000 1 2.718282 2 20.085537 Name: numbers, dtype: float64 Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def exp(s: pd.Series) -> pd.Series: \"\"\" Take the exponential transform of the series. >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.exp() 0 1.000000 1 2.718282 2 20.085537 Name: numbers, dtype: float64 :param s: Input Series. :return: Transformed Series. \"\"\" return np.exp(s)","title":"exp()"},{"location":"api/math/#janitor.math.log","text":"Take natural logarithm of the Series. Each value in the series should be positive. Use error to control the behavior if there are nonpositive entries in the series. >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.log(error=\"ignore\") 0 NaN 1 0.000000 2 1.098612 Name: numbers, dtype: float64 Parameters: Name Type Description Default s Series Input Series. required error str Determines behavior when taking the log of nonpositive entries. If 'warn' then a RuntimeWarning is thrown. If 'raise' , then a RuntimeError is thrown. Otherwise, nothing is thrown and log of nonpositive values is np.nan ; defaults to 'warn' . 'warn' Returns: Type Description Series Transformed Series. Exceptions: Type Description RuntimeError Raised when there are nonpositive values in the Series and error='raise' . Source code in janitor/math.py @pf.register_series_method def log(s: pd.Series, error: str = \"warn\") -> pd.Series: \"\"\" Take natural logarithm of the Series. Each value in the series should be positive. Use `error` to control the behavior if there are nonpositive entries in the series. >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.log(error=\"ignore\") 0 NaN 1 0.000000 2 1.098612 Name: numbers, dtype: float64 :param s: Input Series. :param error: Determines behavior when taking the log of nonpositive entries. If `'warn'` then a `RuntimeWarning` is thrown. If `'raise'`, then a `RuntimeError` is thrown. Otherwise, nothing is thrown and log of nonpositive values is `np.nan`; defaults to `'warn'`. :raises RuntimeError: Raised when there are nonpositive values in the Series and `error='raise'`. :return: Transformed Series. \"\"\" s = s.copy() nonpositive = s <= 0 if (nonpositive).any(): msg = f\"Log taken on {nonpositive.sum()} nonpositive value(s)\" if error.lower() == \"warn\": warnings.warn(msg, RuntimeWarning) if error.lower() == \"raise\": raise RuntimeError(msg) else: pass s[nonpositive] = np.nan return np.log(s)","title":"log()"},{"location":"api/math/#janitor.math.logit","text":"Take logit transform of the Series where: logit(p) = log(p/(1-p)) Each value in the series should be between 0 and 1. Use error to control the behavior if any series entries are outside of (0, 1). >>> import pandas as pd >>> import janitor >>> s = pd.Series([0.1, 0.5, 0.9], name=\"numbers\") >>> s.logit() 0 -2.197225 1 0.000000 2 2.197225 Name: numbers, dtype: float64 Parameters: Name Type Description Default s Series Input Series. required error str Determines behavior when s is outside of (0, 1) . If 'warn' then a RuntimeWarning is thrown. If 'raise' , then a RuntimeError is thrown. Otherwise, nothing is thrown and np.nan is returned for the problematic entries; defaults to 'warn' . 'warn' Returns: Type Description Series Transformed Series. Exceptions: Type Description RuntimeError if error is set to 'raise' . Source code in janitor/math.py @pf.register_series_method def logit(s: pd.Series, error: str = \"warn\") -> pd.Series: \"\"\" Take logit transform of the Series where: ```python logit(p) = log(p/(1-p)) ``` Each value in the series should be between 0 and 1. Use `error` to control the behavior if any series entries are outside of (0, 1). >>> import pandas as pd >>> import janitor >>> s = pd.Series([0.1, 0.5, 0.9], name=\"numbers\") >>> s.logit() 0 -2.197225 1 0.000000 2 2.197225 Name: numbers, dtype: float64 :param s: Input Series. :param error: Determines behavior when `s` is outside of `(0, 1)`. If `'warn'` then a `RuntimeWarning` is thrown. If `'raise'`, then a `RuntimeError` is thrown. Otherwise, nothing is thrown and `np.nan` is returned for the problematic entries; defaults to `'warn'`. :return: Transformed Series. :raises RuntimeError: if `error` is set to `'raise'`. \"\"\" s = s.copy() outside_support = (s <= 0) | (s >= 1) if (outside_support).any(): msg = f\"{outside_support.sum()} value(s) are outside of (0, 1)\" if error.lower() == \"warn\": warnings.warn(msg, RuntimeWarning) if error.lower() == \"raise\": raise RuntimeError(msg) else: pass s[outside_support] = np.nan return scipy_special.logit(s)","title":"logit()"},{"location":"api/math/#janitor.math.normal_cdf","text":"Transforms the Series via the CDF of the Normal distribution. >>> import pandas as pd >>> import janitor >>> s = pd.Series([-1, 0, 3], name=\"numbers\") >>> s.normal_cdf() 0 0.158655 1 0.500000 2 0.998650 dtype: float64 Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def normal_cdf(s: pd.Series) -> pd.Series: \"\"\" Transforms the Series via the CDF of the Normal distribution. >>> import pandas as pd >>> import janitor >>> s = pd.Series([-1, 0, 3], name=\"numbers\") >>> s.normal_cdf() 0 0.158655 1 0.500000 2 0.998650 dtype: float64 :param s: Input Series. :return: Transformed Series. \"\"\" return pd.Series(ss.norm.cdf(s), index=s.index)","title":"normal_cdf()"},{"location":"api/math/#janitor.math.probit","text":"Transforms the Series via the inverse CDF of the Normal distribution. Each value in the series should be between 0 and 1. Use error to control the behavior if any series entries are outside of (0, 1). >>> import pandas as pd >>> import janitor >>> s = pd.Series([0.1, 0.5, 0.8], name=\"numbers\") >>> s.probit() 0 -1.281552 1 0.000000 2 0.841621 dtype: float64 Parameters: Name Type Description Default s Series Input Series. required error str Determines behavior when s is outside of (0, 1) . If 'warn' then a RuntimeWarning is thrown. If 'raise' , then a RuntimeError is thrown. Otherwise, nothing is thrown and np.nan is returned for the problematic entries; defaults to 'warn' . 'warn' Returns: Type Description Series Transformed Series Exceptions: Type Description RuntimeError Raised when there are problematic values in the Series and error='raise' . Source code in janitor/math.py @pf.register_series_method def probit(s: pd.Series, error: str = \"warn\") -> pd.Series: \"\"\" Transforms the Series via the inverse CDF of the Normal distribution. Each value in the series should be between 0 and 1. Use `error` to control the behavior if any series entries are outside of (0, 1). >>> import pandas as pd >>> import janitor >>> s = pd.Series([0.1, 0.5, 0.8], name=\"numbers\") >>> s.probit() 0 -1.281552 1 0.000000 2 0.841621 dtype: float64 :param s: Input Series. :param error: Determines behavior when `s` is outside of `(0, 1)`. If `'warn'` then a `RuntimeWarning` is thrown. If `'raise'`, then a `RuntimeError` is thrown. Otherwise, nothing is thrown and `np.nan` is returned for the problematic entries; defaults to `'warn'`. :raises RuntimeError: Raised when there are problematic values in the Series and `error='raise'`. :return: Transformed Series \"\"\" s = s.copy() outside_support = (s <= 0) | (s >= 1) if (outside_support).any(): msg = f\"{outside_support.sum()} value(s) are outside of (0, 1)\" if error.lower() == \"warn\": warnings.warn(msg, RuntimeWarning) if error.lower() == \"raise\": raise RuntimeError(msg) else: pass s[outside_support] = np.nan with np.errstate(all=\"ignore\"): out = pd.Series(ss.norm.ppf(s), index=s.index) return out","title":"probit()"},{"location":"api/math/#janitor.math.sigmoid","text":"Take the sigmoid transform of the series where: sigmoid(x) = 1 / (1 + exp(-x)) >>> import pandas as pd >>> import janitor >>> s = pd.Series([-1, 0, 4], name=\"numbers\") >>> s.sigmoid() 0 0.268941 1 0.500000 2 0.982014 Name: numbers, dtype: float64 Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def sigmoid(s: pd.Series) -> pd.Series: \"\"\" Take the sigmoid transform of the series where: ```python sigmoid(x) = 1 / (1 + exp(-x)) ``` >>> import pandas as pd >>> import janitor >>> s = pd.Series([-1, 0, 4], name=\"numbers\") >>> s.sigmoid() 0 0.268941 1 0.500000 2 0.982014 Name: numbers, dtype: float64 :param s: Input Series. :return: Transformed Series. \"\"\" return scipy_special.expit(s)","title":"sigmoid()"},{"location":"api/math/#janitor.math.softmax","text":"Take the softmax transform of the series. The softmax function transforms each element of a collection by computing the exponential of each element divided by the sum of the exponentials of all the elements. That is, if x is a one-dimensional numpy array or pandas Series: softmax(x) = exp(x)/sum(exp(x)) >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.softmax() 0 0.042010 1 0.114195 2 0.843795 Name: numbers, dtype: float64 Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def softmax(s: pd.Series) -> pd.Series: \"\"\" Take the softmax transform of the series. The softmax function transforms each element of a collection by computing the exponential of each element divided by the sum of the exponentials of all the elements. That is, if x is a one-dimensional numpy array or pandas Series: ```python softmax(x) = exp(x)/sum(exp(x)) ``` >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.softmax() 0 0.042010 1 0.114195 2 0.843795 Name: numbers, dtype: float64 :param s: Input Series. :return: Transformed Series. \"\"\" return pd.Series(scipy_special.softmax(s), index=s.index, name=s.name)","title":"softmax()"},{"location":"api/math/#janitor.math.z_score","text":"Transforms the Series into z-scores where: z = (s - s.mean()) / s.std() >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.z_score() 0 -0.872872 1 -0.218218 2 1.091089 Name: numbers, dtype: float64 Parameters: Name Type Description Default s Series Input Series. required moments_dict dict If not None , then the mean and standard deviation used to compute the z-score transformation is saved as entries in moments_dict with keys determined by the keys argument; defaults to None . None keys Tuple[str, str] Determines the keys saved in moments_dict if moments are saved; defaults to ( 'mean' , 'std' ). ('mean', 'std') Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def z_score( s: pd.Series, moments_dict: dict = None, keys: Tuple[str, str] = (\"mean\", \"std\"), ) -> pd.Series: \"\"\" Transforms the Series into z-scores where: ```python z = (s - s.mean()) / s.std() ``` >>> import pandas as pd >>> import janitor >>> s = pd.Series([0, 1, 3], name=\"numbers\") >>> s.z_score() 0 -0.872872 1 -0.218218 2 1.091089 Name: numbers, dtype: float64 :param s: Input Series. :param moments_dict: If not `None`, then the mean and standard deviation used to compute the z-score transformation is saved as entries in `moments_dict` with keys determined by the `keys` argument; defaults to `None`. :param keys: Determines the keys saved in `moments_dict` if moments are saved; defaults to (`'mean'`, `'std'`). :return: Transformed Series. \"\"\" mean = s.mean() std = s.std() if std == 0: return 0 if moments_dict is not None: moments_dict[keys[0]] = mean moments_dict[keys[1]] = std return (s - mean) / std","title":"z_score()"},{"location":"api/ml/","text":"Machine Learning Machine learning specific functions. get_features_targets(df, target_column_names, feature_column_names=None) Get the features and targets as separate DataFrames/Series. This method does not mutate the original DataFrame. The behaviour is as such: target_column_names is mandatory. If feature_column_names is present, then we will respect the column names inside there. If feature_column_names is not passed in, then we will assume that the rest of the columns are feature columns, and return them. import pandas as pd import janitor.ml df = pd.DataFrame( ... {\"a\": [1, 2, 3], \"b\": [-2, 0, 4], \"c\": [1.23, 7.89, 4.56]} ... ) X, Y = df.get_features_targets(target_column_names=[\"a\", \"c\"]) X b 0 -2 1 0 2 4 Y a c 0 1 1.23 1 2 7.89 2 3 4.56 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required target_column_names Union[str, List, Tuple, Hashable] Either a column name or an iterable (list or tuple) of column names that are the target(s) to be predicted. required feature_column_names Union[str, Iterable[str], Hashable] (optional) The column name or iterable of column names that are the features (a.k.a. predictors) used to predict the targets. None Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] (X, Y) the feature matrix ( X ) and the target matrix ( Y ). Both are pandas DataFrames. Source code in janitor/ml.py @pf.register_dataframe_method @deprecated_alias( target_columns=\"target_column_names\", feature_columns=\"feature_column_names\", ) def get_features_targets( df: pd.DataFrame, target_column_names: Union[str, Union[List, Tuple], Hashable], feature_column_names: Optional[Union[str, Iterable[str], Hashable]] = None, ) -> Tuple[pd.DataFrame, pd.DataFrame]: \"\"\" Get the features and targets as separate DataFrames/Series. This method does not mutate the original DataFrame. The behaviour is as such: - `target_column_names` is mandatory. - If `feature_column_names` is present, then we will respect the column names inside there. - If `feature_column_names` is not passed in, then we will assume that the rest of the columns are feature columns, and return them. >>> import pandas as pd >>> import janitor.ml >>> df = pd.DataFrame( ... {\"a\": [1, 2, 3], \"b\": [-2, 0, 4], \"c\": [1.23, 7.89, 4.56]} ... ) >>> X, Y = df.get_features_targets(target_column_names=[\"a\", \"c\"]) >>> X b 0 -2 1 0 2 4 >>> Y a c 0 1 1.23 1 2 7.89 2 3 4.56 :param df: The pandas DataFrame object. :param target_column_names: Either a column name or an iterable (list or tuple) of column names that are the target(s) to be predicted. :param feature_column_names: (optional) The column name or iterable of column names that are the features (a.k.a. predictors) used to predict the targets. :returns: `(X, Y)` the feature matrix (`X`) and the target matrix (`Y`). Both are pandas DataFrames. \"\"\" Y = df[target_column_names] if feature_column_names: X = df[feature_column_names] else: if isinstance(target_column_names, (list, tuple)): # noqa: W503 xcols = [c for c in df.columns if c not in target_column_names] else: xcols = [c for c in df.columns if target_column_names != c] X = df[xcols] return X, Y","title":"Machine Learning"},{"location":"api/ml/#machine-learning","text":"Machine learning specific functions.","title":"Machine Learning"},{"location":"api/ml/#janitor.ml.get_features_targets","text":"Get the features and targets as separate DataFrames/Series. This method does not mutate the original DataFrame. The behaviour is as such: target_column_names is mandatory. If feature_column_names is present, then we will respect the column names inside there. If feature_column_names is not passed in, then we will assume that the rest of the columns are feature columns, and return them. import pandas as pd import janitor.ml df = pd.DataFrame( ... {\"a\": [1, 2, 3], \"b\": [-2, 0, 4], \"c\": [1.23, 7.89, 4.56]} ... ) X, Y = df.get_features_targets(target_column_names=[\"a\", \"c\"]) X b 0 -2 1 0 2 4 Y a c 0 1 1.23 1 2 7.89 2 3 4.56 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required target_column_names Union[str, List, Tuple, Hashable] Either a column name or an iterable (list or tuple) of column names that are the target(s) to be predicted. required feature_column_names Union[str, Iterable[str], Hashable] (optional) The column name or iterable of column names that are the features (a.k.a. predictors) used to predict the targets. None Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] (X, Y) the feature matrix ( X ) and the target matrix ( Y ). Both are pandas DataFrames. Source code in janitor/ml.py @pf.register_dataframe_method @deprecated_alias( target_columns=\"target_column_names\", feature_columns=\"feature_column_names\", ) def get_features_targets( df: pd.DataFrame, target_column_names: Union[str, Union[List, Tuple], Hashable], feature_column_names: Optional[Union[str, Iterable[str], Hashable]] = None, ) -> Tuple[pd.DataFrame, pd.DataFrame]: \"\"\" Get the features and targets as separate DataFrames/Series. This method does not mutate the original DataFrame. The behaviour is as such: - `target_column_names` is mandatory. - If `feature_column_names` is present, then we will respect the column names inside there. - If `feature_column_names` is not passed in, then we will assume that the rest of the columns are feature columns, and return them. >>> import pandas as pd >>> import janitor.ml >>> df = pd.DataFrame( ... {\"a\": [1, 2, 3], \"b\": [-2, 0, 4], \"c\": [1.23, 7.89, 4.56]} ... ) >>> X, Y = df.get_features_targets(target_column_names=[\"a\", \"c\"]) >>> X b 0 -2 1 0 2 4 >>> Y a c 0 1 1.23 1 2 7.89 2 3 4.56 :param df: The pandas DataFrame object. :param target_column_names: Either a column name or an iterable (list or tuple) of column names that are the target(s) to be predicted. :param feature_column_names: (optional) The column name or iterable of column names that are the features (a.k.a. predictors) used to predict the targets. :returns: `(X, Y)` the feature matrix (`X`) and the target matrix (`Y`). Both are pandas DataFrames. \"\"\" Y = df[target_column_names] if feature_column_names: X = df[feature_column_names] else: if isinstance(target_column_names, (list, tuple)): # noqa: W503 xcols = [c for c in df.columns if c not in target_column_names] else: xcols = [c for c in df.columns if target_column_names != c] X = df[xcols] return X, Y","title":"get_features_targets()"},{"location":"api/timeseries/","text":"Timeseries Time series-specific data cleaning functions. fill_missing_timestamps(df, frequency, first_time_stamp=None, last_time_stamp=None) Fills a DataFrame with missing timestamps based on a defined frequency. If timestamps are missing, this function will re-index the DataFrame. If timestamps are not missing, then the function will return the DataFrame unmodified. Functional usage example: import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.fill_missing_timestamps( df=df, frequency=\"1H\", ) Method chaining example: import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .fill_missing_timestamps(frequency=\"1H\") ) Parameters: Name Type Description Default df DataFrame DataFrame which needs to be tested for missing timestamps required frequency str sampling frequency of the data. Acceptable frequency strings are available here . Check offset aliases under time series in user guide required first_time_stamp Timestamp timestamp expected to start from; defaults to None . If no input is provided, assumes the minimum value in time_series . None last_time_stamp Timestamp timestamp expected to end with; defaults to None . If no input is provided, assumes the maximum value in time_series . None Returns: Type Description DataFrame DataFrame that has a complete set of contiguous datetimes. Source code in janitor/timeseries.py @pf.register_dataframe_method def fill_missing_timestamps( df: pd.DataFrame, frequency: str, first_time_stamp: pd.Timestamp = None, last_time_stamp: pd.Timestamp = None, ) -> pd.DataFrame: \"\"\" Fills a DataFrame with missing timestamps based on a defined frequency. If timestamps are missing, this function will re-index the DataFrame. If timestamps are not missing, then the function will return the DataFrame unmodified. Functional usage example: ```python import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.fill_missing_timestamps( df=df, frequency=\"1H\", ) ``` Method chaining example: ```python import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .fill_missing_timestamps(frequency=\"1H\") ) ``` :param df: DataFrame which needs to be tested for missing timestamps :param frequency: sampling frequency of the data. Acceptable frequency strings are available [here](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases). Check offset aliases under time series in user guide :param first_time_stamp: timestamp expected to start from; defaults to `None`. If no input is provided, assumes the minimum value in `time_series`. :param last_time_stamp: timestamp expected to end with; defaults to `None`. If no input is provided, assumes the maximum value in `time_series`. :returns: DataFrame that has a complete set of contiguous datetimes. \"\"\" # Check all the inputs are the correct data type check(\"frequency\", frequency, [str]) check(\"first_time_stamp\", first_time_stamp, [pd.Timestamp, type(None)]) check(\"last_time_stamp\", last_time_stamp, [pd.Timestamp, type(None)]) if first_time_stamp is None: first_time_stamp = df.index.min() if last_time_stamp is None: last_time_stamp = df.index.max() # Generate expected timestamps expected_timestamps = pd.date_range( start=first_time_stamp, end=last_time_stamp, freq=frequency ) return df.reindex(expected_timestamps) flag_jumps(df, scale='percentage', direction='any', threshold=0.0, strict=False) Create boolean column(s) that flag whether or not the change between consecutive rows exceeds a provided threshold. Functional usage example: import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = flag_jumps( df=df, scale=\"absolute\", direction=\"any\", threshold=2, ) Method chaining example: import pandas as pd import janitor.timeseries df = ( pd.DatFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2, ) ) Detailed chaining examples: # Applies specified criteria across all columns of the DataFrame # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2 ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\", col2=\"percentage\"), direction=dict(col1=\"increasing\", col2=\"any\"), threshold=dict(col1=1, col2=0.5), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), direction=dict(col2=\"increasing\"), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for only those columns found in # specified criteria df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), threshold=dict(col2=1), strict=True, ) ) Parameters: Name Type Description Default df DataFrame DataFrame which needs to be flagged for changes between consecutive rows above a certain threshold. required scale Union[str, Dict[str, str]] Type of scaling approach to use. Acceptable arguments are 'absolute' (consider the difference between rows) and 'percentage' (consider the percentage change between rows); defaults to 'percentage' . 'percentage' direction Union[str, Dict[str, str]] Type of method used to handle the sign change when comparing consecutive rows. Acceptable arguments are 'increasing' (only consider rows that are increasing in value), 'decreasing' (only consider rows that are decreasing in value), and 'any' (consider rows that are either increasing or decreasing; sign is ignored); defaults to 'any' . 'any' threshold Union[int, float, Dict[str, Union[int, float]]] The value to check if consecutive row comparisons exceed. Always uses a greater than comparison. Must be >= 0.0 ; defaults to 0.0 . 0.0 strict bool flag to enable/disable appending of a flag column for each column in the provided DataFrame. If set to True , will only append a flag column for those columns found in at least one of the input dictionaries. If set to False , will appen a flag column for each column found in the provided DataFrame. If criteria is not specified, the defaults for each criteria is used; defaults to False . False Returns: Type Description DataFrame DataFrame that has flag jump columns. Exceptions: Type Description JanitorError if strict=True and at least one of scale , direction , or threshold inputs is not a dictionary. JanitorError if scale is not one of (\"absolute\", \"percentage\") . JanitorError if direction is not one of (\"increasing\", \"decreasing\", \"any\") . JanitorError if threshold is less than 0.0 . Source code in janitor/timeseries.py @pf.register_dataframe_method def flag_jumps( df: pd.DataFrame, scale: Union[str, Dict[str, str]] = \"percentage\", direction: Union[str, Dict[str, str]] = \"any\", threshold: Union[int, float, Dict[str, Union[int, float]]] = 0.0, strict: bool = False, ) -> pd.DataFrame: \"\"\" Create boolean column(s) that flag whether or not the change between consecutive rows exceeds a provided threshold. Functional usage example: ```python import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = flag_jumps( df=df, scale=\"absolute\", direction=\"any\", threshold=2, ) ``` Method chaining example: ```python import pandas as pd import janitor.timeseries df = ( pd.DatFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2, ) ) ``` Detailed chaining examples: ```python # Applies specified criteria across all columns of the DataFrame # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2 ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\", col2=\"percentage\"), direction=dict(col1=\"increasing\", col2=\"any\"), threshold=dict(col1=1, col2=0.5), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), direction=dict(col2=\"increasing\"), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for only those columns found in # specified criteria df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), threshold=dict(col2=1), strict=True, ) ) ``` :param df: DataFrame which needs to be flagged for changes between consecutive rows above a certain threshold. :param scale: Type of scaling approach to use. Acceptable arguments are `'absolute'` (consider the difference between rows) and `'percentage'` (consider the percentage change between rows); defaults to `'percentage'`. :param direction: Type of method used to handle the sign change when comparing consecutive rows. Acceptable arguments are `'increasing'` (only consider rows that are increasing in value), `'decreasing'` (only consider rows that are decreasing in value), and `'any'` (consider rows that are either increasing or decreasing; sign is ignored); defaults to `'any'`. :param threshold: The value to check if consecutive row comparisons exceed. Always uses a greater than comparison. Must be `>= 0.0`; defaults to `0.0`. :param strict: flag to enable/disable appending of a flag column for each column in the provided DataFrame. If set to `True`, will only append a flag column for those columns found in at least one of the input dictionaries. If set to `False`, will appen a flag column for each column found in the provided DataFrame. If criteria is not specified, the defaults for each criteria is used; defaults to `False`. :returns: DataFrame that has `flag jump` columns. :raises JanitorError: if `strict=True` and at least one of `scale`, `direction`, or `threshold` inputs is not a dictionary. :raises JanitorError: if `scale` is not one of `(\"absolute\", \"percentage\")`. :raises JanitorError: if `direction` is not one of `(\"increasing\", \"decreasing\", \"any\")`. :raises JanitorError: if `threshold` is less than `0.0`. \"\"\" df = df.copy() if strict: if ( any(isinstance(arg, dict) for arg in (scale, direction, threshold)) is False ): raise JanitorError( \"When enacting 'strict=True', 'scale', 'direction', or \" + \"'threshold' must be a dictionary.\" ) # Only append a flag col for the cols that appear # in at least one of the input dicts arg_keys = [ arg.keys() for arg in (scale, direction, threshold) if isinstance(arg, dict) ] cols = set(itertools.chain.from_iterable(arg_keys)) else: # Append a flag col for each col in the DataFrame cols = df.columns columns_to_add = {} for col in sorted(cols): # Allow arguments to be a mix of dict and single instances s = scale.get(col, \"percentage\") if isinstance(scale, dict) else scale d = ( direction.get(col, \"any\") if isinstance(direction, dict) else direction ) t = ( threshold.get(col, 0.0) if isinstance(threshold, dict) else threshold ) columns_to_add[f\"{col}_jump_flag\"] = _flag_jumps_single_col( df, col, scale=s, direction=d, threshold=t ) df = df.assign(**columns_to_add) return df sort_timestamps_monotonically(df, direction='increasing', strict=False) Sort DataFrame such that index is monotonic. If timestamps are monotonic, this function will return the DataFrame unmodified. If timestamps are not monotonic, then the function will sort the DataFrame. Functional usage example: import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.sort_timestamps_monotonically( direction=\"increasing\" ) Method chaining example: import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .sort_timestamps_monotonically(direction=\"increasing\") ) Parameters: Name Type Description Default df DataFrame DataFrame which needs to be tested for monotonicity. required direction str type of monotonicity desired. Acceptable arguments are 'increasing' or 'decreasing' . 'increasing' strict bool flag to enable/disable strict monotonicity. If set to True , will remove duplicates in the index by retaining first occurrence of value in index. If set to False , will not test for duplicates in the index; defaults to False . False Returns: Type Description DataFrame DataFrame that has monotonically increasing (or decreasing) timestamps. Source code in janitor/timeseries.py @pf.register_dataframe_method def sort_timestamps_monotonically( df: pd.DataFrame, direction: str = \"increasing\", strict: bool = False ) -> pd.DataFrame: \"\"\" Sort DataFrame such that index is monotonic. If timestamps are monotonic, this function will return the DataFrame unmodified. If timestamps are not monotonic, then the function will sort the DataFrame. Functional usage example: ```python import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.sort_timestamps_monotonically( direction=\"increasing\" ) ``` Method chaining example: ```python import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .sort_timestamps_monotonically(direction=\"increasing\") ) ``` :param df: DataFrame which needs to be tested for monotonicity. :param direction: type of monotonicity desired. Acceptable arguments are `'increasing'` or `'decreasing'`. :param strict: flag to enable/disable strict monotonicity. If set to `True`, will remove duplicates in the index by retaining first occurrence of value in index. If set to `False`, will not test for duplicates in the index; defaults to `False`. :returns: DataFrame that has monotonically increasing (or decreasing) timestamps. \"\"\" # Check all the inputs are the correct data type check(\"df\", df, [pd.DataFrame]) check(\"direction\", direction, [str]) check(\"strict\", strict, [bool]) # Remove duplicates if requested if strict: df = df[~df.index.duplicated(keep=\"first\")] # Sort timestamps if direction == \"increasing\": df = df.sort_index() else: df = df.sort_index(ascending=False) # Return the DataFrame return df","title":"Timeseries"},{"location":"api/timeseries/#timeseries","text":"Time series-specific data cleaning functions.","title":"Timeseries"},{"location":"api/timeseries/#janitor.timeseries.fill_missing_timestamps","text":"Fills a DataFrame with missing timestamps based on a defined frequency. If timestamps are missing, this function will re-index the DataFrame. If timestamps are not missing, then the function will return the DataFrame unmodified. Functional usage example: import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.fill_missing_timestamps( df=df, frequency=\"1H\", ) Method chaining example: import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .fill_missing_timestamps(frequency=\"1H\") ) Parameters: Name Type Description Default df DataFrame DataFrame which needs to be tested for missing timestamps required frequency str sampling frequency of the data. Acceptable frequency strings are available here . Check offset aliases under time series in user guide required first_time_stamp Timestamp timestamp expected to start from; defaults to None . If no input is provided, assumes the minimum value in time_series . None last_time_stamp Timestamp timestamp expected to end with; defaults to None . If no input is provided, assumes the maximum value in time_series . None Returns: Type Description DataFrame DataFrame that has a complete set of contiguous datetimes. Source code in janitor/timeseries.py @pf.register_dataframe_method def fill_missing_timestamps( df: pd.DataFrame, frequency: str, first_time_stamp: pd.Timestamp = None, last_time_stamp: pd.Timestamp = None, ) -> pd.DataFrame: \"\"\" Fills a DataFrame with missing timestamps based on a defined frequency. If timestamps are missing, this function will re-index the DataFrame. If timestamps are not missing, then the function will return the DataFrame unmodified. Functional usage example: ```python import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.fill_missing_timestamps( df=df, frequency=\"1H\", ) ``` Method chaining example: ```python import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .fill_missing_timestamps(frequency=\"1H\") ) ``` :param df: DataFrame which needs to be tested for missing timestamps :param frequency: sampling frequency of the data. Acceptable frequency strings are available [here](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases). Check offset aliases under time series in user guide :param first_time_stamp: timestamp expected to start from; defaults to `None`. If no input is provided, assumes the minimum value in `time_series`. :param last_time_stamp: timestamp expected to end with; defaults to `None`. If no input is provided, assumes the maximum value in `time_series`. :returns: DataFrame that has a complete set of contiguous datetimes. \"\"\" # Check all the inputs are the correct data type check(\"frequency\", frequency, [str]) check(\"first_time_stamp\", first_time_stamp, [pd.Timestamp, type(None)]) check(\"last_time_stamp\", last_time_stamp, [pd.Timestamp, type(None)]) if first_time_stamp is None: first_time_stamp = df.index.min() if last_time_stamp is None: last_time_stamp = df.index.max() # Generate expected timestamps expected_timestamps = pd.date_range( start=first_time_stamp, end=last_time_stamp, freq=frequency ) return df.reindex(expected_timestamps)","title":"fill_missing_timestamps()"},{"location":"api/timeseries/#janitor.timeseries.flag_jumps","text":"Create boolean column(s) that flag whether or not the change between consecutive rows exceeds a provided threshold. Functional usage example: import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = flag_jumps( df=df, scale=\"absolute\", direction=\"any\", threshold=2, ) Method chaining example: import pandas as pd import janitor.timeseries df = ( pd.DatFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2, ) ) Detailed chaining examples: # Applies specified criteria across all columns of the DataFrame # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2 ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\", col2=\"percentage\"), direction=dict(col1=\"increasing\", col2=\"any\"), threshold=dict(col1=1, col2=0.5), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), direction=dict(col2=\"increasing\"), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for only those columns found in # specified criteria df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), threshold=dict(col2=1), strict=True, ) ) Parameters: Name Type Description Default df DataFrame DataFrame which needs to be flagged for changes between consecutive rows above a certain threshold. required scale Union[str, Dict[str, str]] Type of scaling approach to use. Acceptable arguments are 'absolute' (consider the difference between rows) and 'percentage' (consider the percentage change between rows); defaults to 'percentage' . 'percentage' direction Union[str, Dict[str, str]] Type of method used to handle the sign change when comparing consecutive rows. Acceptable arguments are 'increasing' (only consider rows that are increasing in value), 'decreasing' (only consider rows that are decreasing in value), and 'any' (consider rows that are either increasing or decreasing; sign is ignored); defaults to 'any' . 'any' threshold Union[int, float, Dict[str, Union[int, float]]] The value to check if consecutive row comparisons exceed. Always uses a greater than comparison. Must be >= 0.0 ; defaults to 0.0 . 0.0 strict bool flag to enable/disable appending of a flag column for each column in the provided DataFrame. If set to True , will only append a flag column for those columns found in at least one of the input dictionaries. If set to False , will appen a flag column for each column found in the provided DataFrame. If criteria is not specified, the defaults for each criteria is used; defaults to False . False Returns: Type Description DataFrame DataFrame that has flag jump columns. Exceptions: Type Description JanitorError if strict=True and at least one of scale , direction , or threshold inputs is not a dictionary. JanitorError if scale is not one of (\"absolute\", \"percentage\") . JanitorError if direction is not one of (\"increasing\", \"decreasing\", \"any\") . JanitorError if threshold is less than 0.0 . Source code in janitor/timeseries.py @pf.register_dataframe_method def flag_jumps( df: pd.DataFrame, scale: Union[str, Dict[str, str]] = \"percentage\", direction: Union[str, Dict[str, str]] = \"any\", threshold: Union[int, float, Dict[str, Union[int, float]]] = 0.0, strict: bool = False, ) -> pd.DataFrame: \"\"\" Create boolean column(s) that flag whether or not the change between consecutive rows exceeds a provided threshold. Functional usage example: ```python import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = flag_jumps( df=df, scale=\"absolute\", direction=\"any\", threshold=2, ) ``` Method chaining example: ```python import pandas as pd import janitor.timeseries df = ( pd.DatFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2, ) ) ``` Detailed chaining examples: ```python # Applies specified criteria across all columns of the DataFrame # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2 ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\", col2=\"percentage\"), direction=dict(col1=\"increasing\", col2=\"any\"), threshold=dict(col1=1, col2=0.5), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), direction=dict(col2=\"increasing\"), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for only those columns found in # specified criteria df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), threshold=dict(col2=1), strict=True, ) ) ``` :param df: DataFrame which needs to be flagged for changes between consecutive rows above a certain threshold. :param scale: Type of scaling approach to use. Acceptable arguments are `'absolute'` (consider the difference between rows) and `'percentage'` (consider the percentage change between rows); defaults to `'percentage'`. :param direction: Type of method used to handle the sign change when comparing consecutive rows. Acceptable arguments are `'increasing'` (only consider rows that are increasing in value), `'decreasing'` (only consider rows that are decreasing in value), and `'any'` (consider rows that are either increasing or decreasing; sign is ignored); defaults to `'any'`. :param threshold: The value to check if consecutive row comparisons exceed. Always uses a greater than comparison. Must be `>= 0.0`; defaults to `0.0`. :param strict: flag to enable/disable appending of a flag column for each column in the provided DataFrame. If set to `True`, will only append a flag column for those columns found in at least one of the input dictionaries. If set to `False`, will appen a flag column for each column found in the provided DataFrame. If criteria is not specified, the defaults for each criteria is used; defaults to `False`. :returns: DataFrame that has `flag jump` columns. :raises JanitorError: if `strict=True` and at least one of `scale`, `direction`, or `threshold` inputs is not a dictionary. :raises JanitorError: if `scale` is not one of `(\"absolute\", \"percentage\")`. :raises JanitorError: if `direction` is not one of `(\"increasing\", \"decreasing\", \"any\")`. :raises JanitorError: if `threshold` is less than `0.0`. \"\"\" df = df.copy() if strict: if ( any(isinstance(arg, dict) for arg in (scale, direction, threshold)) is False ): raise JanitorError( \"When enacting 'strict=True', 'scale', 'direction', or \" + \"'threshold' must be a dictionary.\" ) # Only append a flag col for the cols that appear # in at least one of the input dicts arg_keys = [ arg.keys() for arg in (scale, direction, threshold) if isinstance(arg, dict) ] cols = set(itertools.chain.from_iterable(arg_keys)) else: # Append a flag col for each col in the DataFrame cols = df.columns columns_to_add = {} for col in sorted(cols): # Allow arguments to be a mix of dict and single instances s = scale.get(col, \"percentage\") if isinstance(scale, dict) else scale d = ( direction.get(col, \"any\") if isinstance(direction, dict) else direction ) t = ( threshold.get(col, 0.0) if isinstance(threshold, dict) else threshold ) columns_to_add[f\"{col}_jump_flag\"] = _flag_jumps_single_col( df, col, scale=s, direction=d, threshold=t ) df = df.assign(**columns_to_add) return df","title":"flag_jumps()"},{"location":"api/timeseries/#janitor.timeseries.sort_timestamps_monotonically","text":"Sort DataFrame such that index is monotonic. If timestamps are monotonic, this function will return the DataFrame unmodified. If timestamps are not monotonic, then the function will sort the DataFrame. Functional usage example: import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.sort_timestamps_monotonically( direction=\"increasing\" ) Method chaining example: import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .sort_timestamps_monotonically(direction=\"increasing\") ) Parameters: Name Type Description Default df DataFrame DataFrame which needs to be tested for monotonicity. required direction str type of monotonicity desired. Acceptable arguments are 'increasing' or 'decreasing' . 'increasing' strict bool flag to enable/disable strict monotonicity. If set to True , will remove duplicates in the index by retaining first occurrence of value in index. If set to False , will not test for duplicates in the index; defaults to False . False Returns: Type Description DataFrame DataFrame that has monotonically increasing (or decreasing) timestamps. Source code in janitor/timeseries.py @pf.register_dataframe_method def sort_timestamps_monotonically( df: pd.DataFrame, direction: str = \"increasing\", strict: bool = False ) -> pd.DataFrame: \"\"\" Sort DataFrame such that index is monotonic. If timestamps are monotonic, this function will return the DataFrame unmodified. If timestamps are not monotonic, then the function will sort the DataFrame. Functional usage example: ```python import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.sort_timestamps_monotonically( direction=\"increasing\" ) ``` Method chaining example: ```python import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .sort_timestamps_monotonically(direction=\"increasing\") ) ``` :param df: DataFrame which needs to be tested for monotonicity. :param direction: type of monotonicity desired. Acceptable arguments are `'increasing'` or `'decreasing'`. :param strict: flag to enable/disable strict monotonicity. If set to `True`, will remove duplicates in the index by retaining first occurrence of value in index. If set to `False`, will not test for duplicates in the index; defaults to `False`. :returns: DataFrame that has monotonically increasing (or decreasing) timestamps. \"\"\" # Check all the inputs are the correct data type check(\"df\", df, [pd.DataFrame]) check(\"direction\", direction, [str]) check(\"strict\", strict, [bool]) # Remove duplicates if requested if strict: df = df[~df.index.duplicated(keep=\"first\")] # Sort timestamps if direction == \"increasing\": df = df.sort_index() else: df = df.sort_index(ascending=False) # Return the DataFrame return df","title":"sort_timestamps_monotonically()"},{"location":"api/utils/","text":"Utils Miscellaneous mathematical operators. Lazy loading used here to speed up imports. check(varname, value, expected_types) One-liner syntactic sugar for checking types. It can also check callables. Example usage: check('x', x, [int, float]) Parameters: Name Type Description Default varname str The name of the variable (for diagnostic error message). required value The value of the varname . required expected_types list The type(s) the item is expected to be. required Exceptions: Type Description TypeError if data is not the expected type. Source code in janitor/utils.py def check(varname: str, value, expected_types: list): \"\"\" One-liner syntactic sugar for checking types. It can also check callables. Example usage: ```python check('x', x, [int, float]) ``` :param varname: The name of the variable (for diagnostic error message). :param value: The value of the `varname`. :param expected_types: The type(s) the item is expected to be. :raises TypeError: if data is not the expected type. \"\"\" is_expected_type: bool = False for t in expected_types: if t is callable: is_expected_type = t(value) else: is_expected_type = isinstance(value, t) if is_expected_type: break if not is_expected_type: raise TypeError(f\"{varname} should be one of {expected_types}.\") check_column(df, column_names, present=True) One-liner syntactic sugar for checking the presence or absence of columns. Example usage: check(df, ['a', 'b'], present=True) This will check whether columns 'a' and 'b' are present in df 's columns. One can also guarantee that 'a' and 'b' are not present by switching to present=False . Parameters: Name Type Description Default df DataFrame The name of the variable. required column_names Union[Iterable, str] A list of column names we want to check to see if present (or absent) in df . required present bool If True (default), checks to see if all of column_names are in df.columns . If False , checks that none of column_names are in df.columns . True Exceptions: Type Description ValueError if data is not the expected type. Source code in janitor/utils.py def check_column( df: pd.DataFrame, column_names: Union[Iterable, str], present: bool = True ): \"\"\" One-liner syntactic sugar for checking the presence or absence of columns. Example usage: ```python check(df, ['a', 'b'], present=True) ``` This will check whether columns `'a'` and `'b'` are present in `df`'s columns. One can also guarantee that `'a'` and `'b'` are not present by switching to `present=False`. :param df: The name of the variable. :param column_names: A list of column names we want to check to see if present (or absent) in `df`. :param present: If `True` (default), checks to see if all of `column_names` are in `df.columns`. If `False`, checks that none of `column_names` are in `df.columns`. :raises ValueError: if data is not the expected type. \"\"\" if isinstance(column_names, str) or not isinstance(column_names, Iterable): column_names = [column_names] for column_name in column_names: if present and column_name not in df.columns: # skipcq: PYL-R1720 raise ValueError( f\"{column_name} not present in dataframe columns!\" ) elif not present and column_name in df.columns: raise ValueError( f\"{column_name} already present in dataframe columns!\" ) deprecated_alias(**aliases) Used as a decorator when deprecating old function argument names, while keeping backwards compatibility. Implementation is inspired from StackOverflow . Functional usage example: @deprecated_alias(a='alpha', b='beta') def simple_sum(alpha, beta): return alpha + beta Parameters: Name Type Description Default aliases Dictionary of aliases for a function's arguments. {} Returns: Type Description Callable Your original function wrapped with the kwarg redirection function. Source code in janitor/utils.py def deprecated_alias(**aliases) -> Callable: \"\"\" Used as a decorator when deprecating old function argument names, while keeping backwards compatibility. Implementation is inspired from [`StackOverflow`][stack_link]. [stack_link]: https://stackoverflow.com/questions/49802412/how-to-implement-deprecation-in-python-with-argument-alias Functional usage example: ```python @deprecated_alias(a='alpha', b='beta') def simple_sum(alpha, beta): return alpha + beta ``` :param aliases: Dictionary of aliases for a function's arguments. :return: Your original function wrapped with the `kwarg` redirection function. \"\"\" # noqa: E501 def decorator(func): @wraps(func) def wrapper(*args, **kwargs): rename_kwargs(func.__name__, kwargs, aliases) return func(*args, **kwargs) return wrapper return decorator deprecated_kwargs(*arguments, *, message=\"The keyword argument '{argument}' of '{func_name}' is deprecated.\", error=True) Used as a decorator when deprecating function's keyword arguments. Example: from janitor.utils import deprecated_kwargs @deprecated_kwargs('x', 'y') def plus(a, b, x=0, y=0): return a + b Parameters: Name Type Description Default arguments list The list of deprecated keyword arguments. () message str The message of ValueError or DeprecationWarning . It should be a string or a string template. If a string template defaults input func_name and argument . \"The keyword argument '{argument}' of '{func_name}' is deprecated.\" error bool If True raises ValueError else returns DeprecationWarning . True Returns: Type Description Callable The original function wrapped with the deprecated kwargs checking function. Exceptions: Type Description ValueError If one of arguments is in the decorated function's keyword arguments. # noqa: DAR402 Source code in janitor/utils.py def deprecated_kwargs( *arguments: list[str], message: str = ( \"The keyword argument '{argument}' of '{func_name}' is deprecated.\" ), error: bool = True, ) -> Callable: \"\"\" Used as a decorator when deprecating function's keyword arguments. Example: ```python from janitor.utils import deprecated_kwargs @deprecated_kwargs('x', 'y') def plus(a, b, x=0, y=0): return a + b ``` :param arguments: The list of deprecated keyword arguments. :param message: The message of `ValueError` or `DeprecationWarning`. It should be a string or a string template. If a string template defaults input `func_name` and `argument`. :param error: If True raises `ValueError` else returns `DeprecationWarning`. :return: The original function wrapped with the deprecated `kwargs` checking function. :raises ValueError: If one of `arguments` is in the decorated function's keyword arguments. # noqa: DAR402 \"\"\" def decorator(func): @wraps(func) def wrapper(*args, **kwargs): for argument in arguments: if argument in kwargs: msg = message.format( func_name=func.__name__, argument=argument, ) if error: raise ValueError(msg) else: warn(msg, DeprecationWarning) return func(*args, **kwargs) return wrapper return decorator idempotent(func, df, *args, **kwargs) Raises an error if a function operating on a DataFrame is not idempotent. That is, func(func(df)) = func(df) is not True for all df . Parameters: Name Type Description Default func Callable A Python method. required df DataFrame A pandas DataFrame . required args Positional arguments supplied to the method. () kwargs Keyword arguments supplied to the method. {} Exceptions: Type Description ValueError If func is found to not be idempotent for the given DataFrame ( df ). Source code in janitor/utils.py def idempotent(func: Callable, df: pd.DataFrame, *args, **kwargs): \"\"\" Raises an error if a function operating on a DataFrame is not idempotent. That is, `func(func(df)) = func(df)` is not `True` for all `df`. :param func: A Python method. :param df: A pandas `DataFrame`. :param args: Positional arguments supplied to the method. :param kwargs: Keyword arguments supplied to the method. :raises ValueError: If `func` is found to not be idempotent for the given DataFrame (`df`). \"\"\" if not func(df, *args, **kwargs) == func( func(df, *args, **kwargs), *args, **kwargs ): raise ValueError( \"Supplied function is not idempotent for the given DataFrame.\" ) import_message(submodule, package, conda_channel=None, pip_install=False) Return warning if package is not found. Generic message for indicating to the user when a function relies on an optional module / package that is not currently installed. Includes installation instructions. Used in chemistry.py and biology.py . Parameters: Name Type Description Default submodule str pyjanitor submodule that needs an external dependency. required package str External package this submodule relies on. required conda_channel str conda channel package can be installed from, if at all. None pip_install bool Whether package can be installed via pip . False Source code in janitor/utils.py def import_message( submodule: str, package: str, conda_channel: str = None, pip_install: bool = False, ): \"\"\" Return warning if package is not found. Generic message for indicating to the user when a function relies on an optional module / package that is not currently installed. Includes installation instructions. Used in `chemistry.py` and `biology.py`. :param submodule: `pyjanitor` submodule that needs an external dependency. :param package: External package this submodule relies on. :param conda_channel: `conda` channel package can be installed from, if at all. :param pip_install: Whether package can be installed via `pip`. \"\"\" is_conda = os.path.exists(os.path.join(sys.prefix, \"conda-meta\")) installable = True if is_conda: if conda_channel is None: installable = False installation = f\"{package} cannot be installed via conda\" else: installation = f\"conda install -c {conda_channel} {package}\" else: if pip_install: installation = f\"pip install {package}\" else: installable = False installation = f\"{package} cannot be installed via pip\" print( f\"To use the janitor submodule {submodule}, you need to install \" f\"{package}.\" ) print() if installable: print(\"To do so, use the following command:\") print() print(f\" {installation}\") else: print(f\"{installation}\") is_connected(url) This is a helper function to check if the client is connected to the internet. Example: print(is_connected(\"www.google.com\")) console >> True Parameters: Name Type Description Default url str We take a test url to check if we are able to create a valid connection. required Returns: Type Description bool We return a boolean that signifies our connection to the internet Exceptions: Type Description OSError if connection to URL cannot be established Source code in janitor/utils.py def is_connected(url: str) -> bool: \"\"\" This is a helper function to check if the client is connected to the internet. Example: print(is_connected(\"www.google.com\")) console >> True :param url: We take a test url to check if we are able to create a valid connection. :raises OSError: if connection to `URL` cannot be established :return: We return a boolean that signifies our connection to the internet \"\"\" try: sock = socket.create_connection((url, 80)) if sock is not None: sock.close() return True except OSError as e: warn( \"There was an issue connecting to the internet. \" \"Please see original error below.\" ) raise e return False refactored_function(message) Used as a decorator when refactoring functions. Implementation is inspired from Hacker Noon . Functional usage example: @refactored_function( message=\"simple_sum() has been refactored. Use hard_sum() instead.\" ) def simple_sum(alpha, beta): return alpha + beta Parameters: Name Type Description Default message str Message to use in warning user about refactoring. required Returns: Type Description Callable Your original function wrapped with the kwarg redirection function. Source code in janitor/utils.py def refactored_function(message: str) -> Callable: \"\"\" Used as a decorator when refactoring functions. Implementation is inspired from [`Hacker Noon`][hacker_link]. [hacker_link]: https://hackernoon.com/why-refactoring-how-to-restructure-python-package-51b89aa91987 Functional usage example: ```python @refactored_function( message=\"simple_sum() has been refactored. Use hard_sum() instead.\" ) def simple_sum(alpha, beta): return alpha + beta ``` :param message: Message to use in warning user about refactoring. :return: Your original function wrapped with the kwarg redirection function. \"\"\" # noqa: E501 def decorator(func): def emit_warning(*args, **kwargs): warn(message, FutureWarning) return func(*args, **kwargs) return emit_warning return decorator rename_kwargs(func_name, kwargs, aliases) Used to update deprecated argument names with new names. Throws a TypeError if both arguments are provided, and warns if old alias is used. Nothing is returned as the passed kwargs are modified directly. Implementation is inspired from StackOverflow . Parameters: Name Type Description Default func_name str name of decorated function. required kwargs Dict Arguments supplied to the method. required aliases Dict Dictionary of aliases for a function's arguments. required Exceptions: Type Description TypeError if both arguments are provided. Source code in janitor/utils.py def rename_kwargs(func_name: str, kwargs: Dict, aliases: Dict): \"\"\" Used to update deprecated argument names with new names. Throws a `TypeError` if both arguments are provided, and warns if old alias is used. Nothing is returned as the passed `kwargs` are modified directly. Implementation is inspired from [`StackOverflow`][stack_link]. [stack_link]: https://stackoverflow.com/questions/49802412/how-to-implement-deprecation-in-python-with-argument-alias :param func_name: name of decorated function. :param kwargs: Arguments supplied to the method. :param aliases: Dictionary of aliases for a function's arguments. :raises TypeError: if both arguments are provided. \"\"\" # noqa: E501 for old_alias, new_alias in aliases.items(): if old_alias in kwargs: if new_alias in kwargs: raise TypeError( f\"{func_name} received both {old_alias} and {new_alias}\" ) warn( f\"{old_alias} is deprecated; use {new_alias}\", DeprecationWarning, ) kwargs[new_alias] = kwargs.pop(old_alias) skiperror(f, return_x=False, return_val=nan) Decorator for escaping any error in a function. Example usage: df[column].apply( skiperror(transform, return_val=3, return_x=False)) # Can also be used as shown below @skiperror(return_val=3, return_x=False) def transform(x): pass Parameters: Name Type Description Default f Callable the function to be wrapped. required return_x bool whether or not the original value that caused error should be returned. False return_val the value to be returned when an error hits. Ignored if return_x is True . nan Returns: Type Description Callable the wrapped function. Source code in janitor/utils.py def skiperror( f: Callable, return_x: bool = False, return_val=np.nan ) -> Callable: \"\"\" Decorator for escaping any error in a function. Example usage: ```python df[column].apply( skiperror(transform, return_val=3, return_x=False)) # Can also be used as shown below @skiperror(return_val=3, return_x=False) def transform(x): pass ``` :param f: the function to be wrapped. :param return_x: whether or not the original value that caused error should be returned. :param return_val: the value to be returned when an error hits. Ignored if `return_x` is `True`. :returns: the wrapped function. \"\"\" def _wrapped(x, *args, **kwargs): try: return f(x, *args, **kwargs) except Exception: # skipcq: PYL-W0703 if return_x: return x return return_val return _wrapped skipna(f) Decorator for escaping np.nan and None in a function. Example usage: df[column].apply(skipna(transform)) # Can also be used as shown below @skipna def transform(x): pass Parameters: Name Type Description Default f Callable the function to be wrapped. required Returns: Type Description Callable the wrapped function. Source code in janitor/utils.py def skipna(f: Callable) -> Callable: \"\"\" Decorator for escaping `np.nan` and `None` in a function. Example usage: ```python df[column].apply(skipna(transform)) # Can also be used as shown below @skipna def transform(x): pass ``` :param f: the function to be wrapped. :returns: the wrapped function. \"\"\" def _wrapped(x, *args, **kwargs): if (type(x) is float and np.isnan(x)) or x is None: return np.nan return f(x, *args, **kwargs) return _wrapped","title":"Utils"},{"location":"api/utils/#utils","text":"Miscellaneous mathematical operators. Lazy loading used here to speed up imports.","title":"Utils"},{"location":"api/utils/#janitor.utils.check","text":"One-liner syntactic sugar for checking types. It can also check callables. Example usage: check('x', x, [int, float]) Parameters: Name Type Description Default varname str The name of the variable (for diagnostic error message). required value The value of the varname . required expected_types list The type(s) the item is expected to be. required Exceptions: Type Description TypeError if data is not the expected type. Source code in janitor/utils.py def check(varname: str, value, expected_types: list): \"\"\" One-liner syntactic sugar for checking types. It can also check callables. Example usage: ```python check('x', x, [int, float]) ``` :param varname: The name of the variable (for diagnostic error message). :param value: The value of the `varname`. :param expected_types: The type(s) the item is expected to be. :raises TypeError: if data is not the expected type. \"\"\" is_expected_type: bool = False for t in expected_types: if t is callable: is_expected_type = t(value) else: is_expected_type = isinstance(value, t) if is_expected_type: break if not is_expected_type: raise TypeError(f\"{varname} should be one of {expected_types}.\")","title":"check()"},{"location":"api/utils/#janitor.utils.check_column","text":"One-liner syntactic sugar for checking the presence or absence of columns. Example usage: check(df, ['a', 'b'], present=True) This will check whether columns 'a' and 'b' are present in df 's columns. One can also guarantee that 'a' and 'b' are not present by switching to present=False . Parameters: Name Type Description Default df DataFrame The name of the variable. required column_names Union[Iterable, str] A list of column names we want to check to see if present (or absent) in df . required present bool If True (default), checks to see if all of column_names are in df.columns . If False , checks that none of column_names are in df.columns . True Exceptions: Type Description ValueError if data is not the expected type. Source code in janitor/utils.py def check_column( df: pd.DataFrame, column_names: Union[Iterable, str], present: bool = True ): \"\"\" One-liner syntactic sugar for checking the presence or absence of columns. Example usage: ```python check(df, ['a', 'b'], present=True) ``` This will check whether columns `'a'` and `'b'` are present in `df`'s columns. One can also guarantee that `'a'` and `'b'` are not present by switching to `present=False`. :param df: The name of the variable. :param column_names: A list of column names we want to check to see if present (or absent) in `df`. :param present: If `True` (default), checks to see if all of `column_names` are in `df.columns`. If `False`, checks that none of `column_names` are in `df.columns`. :raises ValueError: if data is not the expected type. \"\"\" if isinstance(column_names, str) or not isinstance(column_names, Iterable): column_names = [column_names] for column_name in column_names: if present and column_name not in df.columns: # skipcq: PYL-R1720 raise ValueError( f\"{column_name} not present in dataframe columns!\" ) elif not present and column_name in df.columns: raise ValueError( f\"{column_name} already present in dataframe columns!\" )","title":"check_column()"},{"location":"api/utils/#janitor.utils.deprecated_alias","text":"Used as a decorator when deprecating old function argument names, while keeping backwards compatibility. Implementation is inspired from StackOverflow . Functional usage example: @deprecated_alias(a='alpha', b='beta') def simple_sum(alpha, beta): return alpha + beta Parameters: Name Type Description Default aliases Dictionary of aliases for a function's arguments. {} Returns: Type Description Callable Your original function wrapped with the kwarg redirection function. Source code in janitor/utils.py def deprecated_alias(**aliases) -> Callable: \"\"\" Used as a decorator when deprecating old function argument names, while keeping backwards compatibility. Implementation is inspired from [`StackOverflow`][stack_link]. [stack_link]: https://stackoverflow.com/questions/49802412/how-to-implement-deprecation-in-python-with-argument-alias Functional usage example: ```python @deprecated_alias(a='alpha', b='beta') def simple_sum(alpha, beta): return alpha + beta ``` :param aliases: Dictionary of aliases for a function's arguments. :return: Your original function wrapped with the `kwarg` redirection function. \"\"\" # noqa: E501 def decorator(func): @wraps(func) def wrapper(*args, **kwargs): rename_kwargs(func.__name__, kwargs, aliases) return func(*args, **kwargs) return wrapper return decorator","title":"deprecated_alias()"},{"location":"api/utils/#janitor.utils.deprecated_kwargs","text":"Used as a decorator when deprecating function's keyword arguments. Example: from janitor.utils import deprecated_kwargs @deprecated_kwargs('x', 'y') def plus(a, b, x=0, y=0): return a + b Parameters: Name Type Description Default arguments list The list of deprecated keyword arguments. () message str The message of ValueError or DeprecationWarning . It should be a string or a string template. If a string template defaults input func_name and argument . \"The keyword argument '{argument}' of '{func_name}' is deprecated.\" error bool If True raises ValueError else returns DeprecationWarning . True Returns: Type Description Callable The original function wrapped with the deprecated kwargs checking function. Exceptions: Type Description ValueError If one of arguments is in the decorated function's keyword arguments. # noqa: DAR402 Source code in janitor/utils.py def deprecated_kwargs( *arguments: list[str], message: str = ( \"The keyword argument '{argument}' of '{func_name}' is deprecated.\" ), error: bool = True, ) -> Callable: \"\"\" Used as a decorator when deprecating function's keyword arguments. Example: ```python from janitor.utils import deprecated_kwargs @deprecated_kwargs('x', 'y') def plus(a, b, x=0, y=0): return a + b ``` :param arguments: The list of deprecated keyword arguments. :param message: The message of `ValueError` or `DeprecationWarning`. It should be a string or a string template. If a string template defaults input `func_name` and `argument`. :param error: If True raises `ValueError` else returns `DeprecationWarning`. :return: The original function wrapped with the deprecated `kwargs` checking function. :raises ValueError: If one of `arguments` is in the decorated function's keyword arguments. # noqa: DAR402 \"\"\" def decorator(func): @wraps(func) def wrapper(*args, **kwargs): for argument in arguments: if argument in kwargs: msg = message.format( func_name=func.__name__, argument=argument, ) if error: raise ValueError(msg) else: warn(msg, DeprecationWarning) return func(*args, **kwargs) return wrapper return decorator","title":"deprecated_kwargs()"},{"location":"api/utils/#janitor.utils.idempotent","text":"Raises an error if a function operating on a DataFrame is not idempotent. That is, func(func(df)) = func(df) is not True for all df . Parameters: Name Type Description Default func Callable A Python method. required df DataFrame A pandas DataFrame . required args Positional arguments supplied to the method. () kwargs Keyword arguments supplied to the method. {} Exceptions: Type Description ValueError If func is found to not be idempotent for the given DataFrame ( df ). Source code in janitor/utils.py def idempotent(func: Callable, df: pd.DataFrame, *args, **kwargs): \"\"\" Raises an error if a function operating on a DataFrame is not idempotent. That is, `func(func(df)) = func(df)` is not `True` for all `df`. :param func: A Python method. :param df: A pandas `DataFrame`. :param args: Positional arguments supplied to the method. :param kwargs: Keyword arguments supplied to the method. :raises ValueError: If `func` is found to not be idempotent for the given DataFrame (`df`). \"\"\" if not func(df, *args, **kwargs) == func( func(df, *args, **kwargs), *args, **kwargs ): raise ValueError( \"Supplied function is not idempotent for the given DataFrame.\" )","title":"idempotent()"},{"location":"api/utils/#janitor.utils.import_message","text":"Return warning if package is not found. Generic message for indicating to the user when a function relies on an optional module / package that is not currently installed. Includes installation instructions. Used in chemistry.py and biology.py . Parameters: Name Type Description Default submodule str pyjanitor submodule that needs an external dependency. required package str External package this submodule relies on. required conda_channel str conda channel package can be installed from, if at all. None pip_install bool Whether package can be installed via pip . False Source code in janitor/utils.py def import_message( submodule: str, package: str, conda_channel: str = None, pip_install: bool = False, ): \"\"\" Return warning if package is not found. Generic message for indicating to the user when a function relies on an optional module / package that is not currently installed. Includes installation instructions. Used in `chemistry.py` and `biology.py`. :param submodule: `pyjanitor` submodule that needs an external dependency. :param package: External package this submodule relies on. :param conda_channel: `conda` channel package can be installed from, if at all. :param pip_install: Whether package can be installed via `pip`. \"\"\" is_conda = os.path.exists(os.path.join(sys.prefix, \"conda-meta\")) installable = True if is_conda: if conda_channel is None: installable = False installation = f\"{package} cannot be installed via conda\" else: installation = f\"conda install -c {conda_channel} {package}\" else: if pip_install: installation = f\"pip install {package}\" else: installable = False installation = f\"{package} cannot be installed via pip\" print( f\"To use the janitor submodule {submodule}, you need to install \" f\"{package}.\" ) print() if installable: print(\"To do so, use the following command:\") print() print(f\" {installation}\") else: print(f\"{installation}\")","title":"import_message()"},{"location":"api/utils/#janitor.utils.is_connected","text":"This is a helper function to check if the client is connected to the internet. Example: print(is_connected(\"www.google.com\")) console >> True Parameters: Name Type Description Default url str We take a test url to check if we are able to create a valid connection. required Returns: Type Description bool We return a boolean that signifies our connection to the internet Exceptions: Type Description OSError if connection to URL cannot be established Source code in janitor/utils.py def is_connected(url: str) -> bool: \"\"\" This is a helper function to check if the client is connected to the internet. Example: print(is_connected(\"www.google.com\")) console >> True :param url: We take a test url to check if we are able to create a valid connection. :raises OSError: if connection to `URL` cannot be established :return: We return a boolean that signifies our connection to the internet \"\"\" try: sock = socket.create_connection((url, 80)) if sock is not None: sock.close() return True except OSError as e: warn( \"There was an issue connecting to the internet. \" \"Please see original error below.\" ) raise e return False","title":"is_connected()"},{"location":"api/utils/#janitor.utils.refactored_function","text":"Used as a decorator when refactoring functions. Implementation is inspired from Hacker Noon . Functional usage example: @refactored_function( message=\"simple_sum() has been refactored. Use hard_sum() instead.\" ) def simple_sum(alpha, beta): return alpha + beta Parameters: Name Type Description Default message str Message to use in warning user about refactoring. required Returns: Type Description Callable Your original function wrapped with the kwarg redirection function. Source code in janitor/utils.py def refactored_function(message: str) -> Callable: \"\"\" Used as a decorator when refactoring functions. Implementation is inspired from [`Hacker Noon`][hacker_link]. [hacker_link]: https://hackernoon.com/why-refactoring-how-to-restructure-python-package-51b89aa91987 Functional usage example: ```python @refactored_function( message=\"simple_sum() has been refactored. Use hard_sum() instead.\" ) def simple_sum(alpha, beta): return alpha + beta ``` :param message: Message to use in warning user about refactoring. :return: Your original function wrapped with the kwarg redirection function. \"\"\" # noqa: E501 def decorator(func): def emit_warning(*args, **kwargs): warn(message, FutureWarning) return func(*args, **kwargs) return emit_warning return decorator","title":"refactored_function()"},{"location":"api/utils/#janitor.utils.rename_kwargs","text":"Used to update deprecated argument names with new names. Throws a TypeError if both arguments are provided, and warns if old alias is used. Nothing is returned as the passed kwargs are modified directly. Implementation is inspired from StackOverflow . Parameters: Name Type Description Default func_name str name of decorated function. required kwargs Dict Arguments supplied to the method. required aliases Dict Dictionary of aliases for a function's arguments. required Exceptions: Type Description TypeError if both arguments are provided. Source code in janitor/utils.py def rename_kwargs(func_name: str, kwargs: Dict, aliases: Dict): \"\"\" Used to update deprecated argument names with new names. Throws a `TypeError` if both arguments are provided, and warns if old alias is used. Nothing is returned as the passed `kwargs` are modified directly. Implementation is inspired from [`StackOverflow`][stack_link]. [stack_link]: https://stackoverflow.com/questions/49802412/how-to-implement-deprecation-in-python-with-argument-alias :param func_name: name of decorated function. :param kwargs: Arguments supplied to the method. :param aliases: Dictionary of aliases for a function's arguments. :raises TypeError: if both arguments are provided. \"\"\" # noqa: E501 for old_alias, new_alias in aliases.items(): if old_alias in kwargs: if new_alias in kwargs: raise TypeError( f\"{func_name} received both {old_alias} and {new_alias}\" ) warn( f\"{old_alias} is deprecated; use {new_alias}\", DeprecationWarning, ) kwargs[new_alias] = kwargs.pop(old_alias)","title":"rename_kwargs()"},{"location":"api/utils/#janitor.utils.skiperror","text":"Decorator for escaping any error in a function. Example usage: df[column].apply( skiperror(transform, return_val=3, return_x=False)) # Can also be used as shown below @skiperror(return_val=3, return_x=False) def transform(x): pass Parameters: Name Type Description Default f Callable the function to be wrapped. required return_x bool whether or not the original value that caused error should be returned. False return_val the value to be returned when an error hits. Ignored if return_x is True . nan Returns: Type Description Callable the wrapped function. Source code in janitor/utils.py def skiperror( f: Callable, return_x: bool = False, return_val=np.nan ) -> Callable: \"\"\" Decorator for escaping any error in a function. Example usage: ```python df[column].apply( skiperror(transform, return_val=3, return_x=False)) # Can also be used as shown below @skiperror(return_val=3, return_x=False) def transform(x): pass ``` :param f: the function to be wrapped. :param return_x: whether or not the original value that caused error should be returned. :param return_val: the value to be returned when an error hits. Ignored if `return_x` is `True`. :returns: the wrapped function. \"\"\" def _wrapped(x, *args, **kwargs): try: return f(x, *args, **kwargs) except Exception: # skipcq: PYL-W0703 if return_x: return x return return_val return _wrapped","title":"skiperror()"},{"location":"api/utils/#janitor.utils.skipna","text":"Decorator for escaping np.nan and None in a function. Example usage: df[column].apply(skipna(transform)) # Can also be used as shown below @skipna def transform(x): pass Parameters: Name Type Description Default f Callable the function to be wrapped. required Returns: Type Description Callable the wrapped function. Source code in janitor/utils.py def skipna(f: Callable) -> Callable: \"\"\" Decorator for escaping `np.nan` and `None` in a function. Example usage: ```python df[column].apply(skipna(transform)) # Can also be used as shown below @skipna def transform(x): pass ``` :param f: the function to be wrapped. :returns: the wrapped function. \"\"\" def _wrapped(x, *args, **kwargs): if (type(x) is float and np.isnan(x)) or x is None: return np.nan return f(x, *args, **kwargs) return _wrapped","title":"skipna()"},{"location":"api/xarray/","text":"XArray Functions to augment XArray DataArrays and Datasets with additional functionality. clone_using(da, np_arr, use_coords=True, use_attrs=False, new_name=None) Given a NumPy array, return an XArray DataArray which contains the same dimension names and (optionally) coordinates and other properties as the supplied DataArray . This is similar to xr.DataArray.copy() with more specificity for the type of cloning you would like to perform - the different properties that you desire to mirror in the new DataArray . If the coordinates from the source DataArray are not desired, the shape of the source and new NumPy arrays don't need to match. The number of dimensions do, however. Usage example - making a new DataArray from a previous one, keeping the dimension names but dropping the coordinates (the input NumPy array is of a different size): >>> import xarray as xr >>> import janitor.xarray >>> da = xr.DataArray( ... np.zeros((512, 1024)), dims=[\"ax_1\", \"ax_2\"], ... coords=dict(ax_1=np.linspace(0, 1, 512), ... ax_2=np.logspace(-2, 2, 1024)), ... name=\"original\" ... ) >>> new_da = da.clone_using( ... np.ones((4, 6)), new_name='new_and_improved', use_coords=False ... ) >>> new_da <xarray.DataArray 'new_and_improved' (ax_1: 4, ax_2: 6)> array([[1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.]]) Dimensions without coordinates: ax_1, ax_2 Parameters: Name Type Description Default da DataArray The DataArray supplied by the method itself. required np_arr <built-in function array> The NumPy array which will be wrapped in a new DataArray given the properties copied over from the source DataArray . required use_coords bool If True , use the coordinates of the source DataArray for the coordinates of the newly-generated array. Shapes must match in this case. If False , only the number of dimensions must match. True use_attrs bool If True , copy over the attrs from the source DataArray . The data inside attrs itself is not copied, only the mapping. Otherwise, use the supplied attrs. False new_name str If set, use as the new name of the returned DataArray . Otherwise, use the name of `da``. None Returns: Type Description DataArray A DataArray styled like the input DataArray containing the NumPy array data. Exceptions: Type Description ValueError if number of dimensions in NumPy array and DataArray do not match. ValueError if shape of NumPy array and DataArray do not match. Source code in janitor/xarray/functions.py @pf.register_xarray_dataarray_method def clone_using( da: xr.DataArray, np_arr: np.array, use_coords: bool = True, use_attrs: bool = False, new_name: str = None, ) -> xr.DataArray: \"\"\" Given a NumPy array, return an XArray `DataArray` which contains the same dimension names and (optionally) coordinates and other properties as the supplied `DataArray`. This is similar to `xr.DataArray.copy()` with more specificity for the type of cloning you would like to perform - the different properties that you desire to mirror in the new `DataArray`. If the coordinates from the source `DataArray` are not desired, the shape of the source and new NumPy arrays don't need to match. The number of dimensions do, however. Usage example - making a new `DataArray` from a previous one, keeping the dimension names but dropping the coordinates (the input NumPy array is of a different size): >>> import xarray as xr >>> import janitor.xarray >>> da = xr.DataArray( ... np.zeros((512, 1024)), dims=[\"ax_1\", \"ax_2\"], ... coords=dict(ax_1=np.linspace(0, 1, 512), ... ax_2=np.logspace(-2, 2, 1024)), ... name=\"original\" ... ) >>> new_da = da.clone_using( ... np.ones((4, 6)), new_name='new_and_improved', use_coords=False ... ) >>> new_da <xarray.DataArray 'new_and_improved' (ax_1: 4, ax_2: 6)> array([[1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.]]) Dimensions without coordinates: ax_1, ax_2 :param da: The `DataArray` supplied by the method itself. :param np_arr: The NumPy array which will be wrapped in a new `DataArray` given the properties copied over from the source `DataArray`. :param use_coords: If `True`, use the coordinates of the source `DataArray` for the coordinates of the newly-generated array. Shapes must match in this case. If `False`, only the number of dimensions must match. :param use_attrs: If `True`, copy over the `attrs` from the source `DataArray`. The data inside `attrs` itself is not copied, only the mapping. Otherwise, use the supplied attrs. :param new_name: If set, use as the new name of the returned `DataArray`. Otherwise, use the name of `da``. :return: A `DataArray` styled like the input `DataArray` containing the NumPy array data. :raises ValueError: if number of dimensions in `NumPy` array and `DataArray` do not match. :raises ValueError: if shape of `NumPy` array and `DataArray` do not match. \"\"\" if np_arr.ndim != da.ndim: raise ValueError( \"Number of dims in the NumPy array and the DataArray \" \"must match.\" ) if use_coords and not all( np_ax_len == da_ax_len for np_ax_len, da_ax_len in zip(np_arr.shape, da.shape) ): raise ValueError( \"Input NumPy array and DataArray must have the same \" \"shape if copying over coordinates.\" ) return xr.DataArray( np_arr, dims=da.dims, coords=da.coords if use_coords else None, attrs=da.attrs.copy() if use_attrs else None, name=new_name if new_name is not None else da.name, ) convert_datetime_to_number(da_or_ds, time_units, dim='time') Convert the coordinates of a datetime axis to a human-readable float representation. Usage example to convert a DataArray 's time dimension coordinates from a minutes to seconds: >>> import numpy as np >>> import xarray as xr >>> import janitor.xarray >>> timepoints = 5 >>> da = xr.DataArray( ... np.array([2, 8, 0, 1, 7, 7]), ... dims='time', ... coords=dict(time=np.arange(6) * np.timedelta64(1, 'm')) ... ) >>> da_minutes = da.convert_datetime_to_number('s', dim='time') >>> da_minutes <xarray.DataArray (time: 6)> array([2, 8, 0, 1, 7, 7]) Coordinates: * time (time) float64 0.0 60.0 120.0 180.0 240.0 300.0 Parameters: Name Type Description Default da_or_ds Union[xarray.core.dataarray.DataArray, xarray.core.dataset.Dataset] XArray object. required time_units str Numpy timedelta string specification for the unit you would like to convert the coordinates to. required dim str the time dimension whose coordinates are datetime objects. 'time' Returns: Type Description The original XArray object with the time dimension reassigned. Source code in janitor/xarray/functions.py @pf.register_xarray_dataset_method @pf.register_xarray_dataarray_method def convert_datetime_to_number( da_or_ds: Union[xr.DataArray, xr.Dataset], time_units: str, dim: str = \"time\", ): \"\"\" Convert the coordinates of a datetime axis to a human-readable float representation. Usage example to convert a `DataArray`'s time dimension coordinates from a minutes to seconds: >>> import numpy as np >>> import xarray as xr >>> import janitor.xarray >>> timepoints = 5 >>> da = xr.DataArray( ... np.array([2, 8, 0, 1, 7, 7]), ... dims='time', ... coords=dict(time=np.arange(6) * np.timedelta64(1, 'm')) ... ) >>> da_minutes = da.convert_datetime_to_number('s', dim='time') >>> da_minutes <xarray.DataArray (time: 6)> array([2, 8, 0, 1, 7, 7]) Coordinates: * time (time) float64 0.0 60.0 120.0 180.0 240.0 300.0 :param da_or_ds: XArray object. :param time_units: Numpy timedelta string specification for the unit you would like to convert the coordinates to. :param dim: the time dimension whose coordinates are datetime objects. :return: The original XArray object with the time dimension reassigned. \"\"\" times = da_or_ds.coords[dim].data / np.timedelta64(1, time_units) return da_or_ds.assign_coords({dim: times})","title":"XArray"},{"location":"api/xarray/#xarray","text":"Functions to augment XArray DataArrays and Datasets with additional functionality.","title":"XArray"},{"location":"api/xarray/#janitor.xarray.functions.clone_using","text":"Given a NumPy array, return an XArray DataArray which contains the same dimension names and (optionally) coordinates and other properties as the supplied DataArray . This is similar to xr.DataArray.copy() with more specificity for the type of cloning you would like to perform - the different properties that you desire to mirror in the new DataArray . If the coordinates from the source DataArray are not desired, the shape of the source and new NumPy arrays don't need to match. The number of dimensions do, however. Usage example - making a new DataArray from a previous one, keeping the dimension names but dropping the coordinates (the input NumPy array is of a different size): >>> import xarray as xr >>> import janitor.xarray >>> da = xr.DataArray( ... np.zeros((512, 1024)), dims=[\"ax_1\", \"ax_2\"], ... coords=dict(ax_1=np.linspace(0, 1, 512), ... ax_2=np.logspace(-2, 2, 1024)), ... name=\"original\" ... ) >>> new_da = da.clone_using( ... np.ones((4, 6)), new_name='new_and_improved', use_coords=False ... ) >>> new_da <xarray.DataArray 'new_and_improved' (ax_1: 4, ax_2: 6)> array([[1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.]]) Dimensions without coordinates: ax_1, ax_2 Parameters: Name Type Description Default da DataArray The DataArray supplied by the method itself. required np_arr <built-in function array> The NumPy array which will be wrapped in a new DataArray given the properties copied over from the source DataArray . required use_coords bool If True , use the coordinates of the source DataArray for the coordinates of the newly-generated array. Shapes must match in this case. If False , only the number of dimensions must match. True use_attrs bool If True , copy over the attrs from the source DataArray . The data inside attrs itself is not copied, only the mapping. Otherwise, use the supplied attrs. False new_name str If set, use as the new name of the returned DataArray . Otherwise, use the name of `da``. None Returns: Type Description DataArray A DataArray styled like the input DataArray containing the NumPy array data. Exceptions: Type Description ValueError if number of dimensions in NumPy array and DataArray do not match. ValueError if shape of NumPy array and DataArray do not match. Source code in janitor/xarray/functions.py @pf.register_xarray_dataarray_method def clone_using( da: xr.DataArray, np_arr: np.array, use_coords: bool = True, use_attrs: bool = False, new_name: str = None, ) -> xr.DataArray: \"\"\" Given a NumPy array, return an XArray `DataArray` which contains the same dimension names and (optionally) coordinates and other properties as the supplied `DataArray`. This is similar to `xr.DataArray.copy()` with more specificity for the type of cloning you would like to perform - the different properties that you desire to mirror in the new `DataArray`. If the coordinates from the source `DataArray` are not desired, the shape of the source and new NumPy arrays don't need to match. The number of dimensions do, however. Usage example - making a new `DataArray` from a previous one, keeping the dimension names but dropping the coordinates (the input NumPy array is of a different size): >>> import xarray as xr >>> import janitor.xarray >>> da = xr.DataArray( ... np.zeros((512, 1024)), dims=[\"ax_1\", \"ax_2\"], ... coords=dict(ax_1=np.linspace(0, 1, 512), ... ax_2=np.logspace(-2, 2, 1024)), ... name=\"original\" ... ) >>> new_da = da.clone_using( ... np.ones((4, 6)), new_name='new_and_improved', use_coords=False ... ) >>> new_da <xarray.DataArray 'new_and_improved' (ax_1: 4, ax_2: 6)> array([[1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.]]) Dimensions without coordinates: ax_1, ax_2 :param da: The `DataArray` supplied by the method itself. :param np_arr: The NumPy array which will be wrapped in a new `DataArray` given the properties copied over from the source `DataArray`. :param use_coords: If `True`, use the coordinates of the source `DataArray` for the coordinates of the newly-generated array. Shapes must match in this case. If `False`, only the number of dimensions must match. :param use_attrs: If `True`, copy over the `attrs` from the source `DataArray`. The data inside `attrs` itself is not copied, only the mapping. Otherwise, use the supplied attrs. :param new_name: If set, use as the new name of the returned `DataArray`. Otherwise, use the name of `da``. :return: A `DataArray` styled like the input `DataArray` containing the NumPy array data. :raises ValueError: if number of dimensions in `NumPy` array and `DataArray` do not match. :raises ValueError: if shape of `NumPy` array and `DataArray` do not match. \"\"\" if np_arr.ndim != da.ndim: raise ValueError( \"Number of dims in the NumPy array and the DataArray \" \"must match.\" ) if use_coords and not all( np_ax_len == da_ax_len for np_ax_len, da_ax_len in zip(np_arr.shape, da.shape) ): raise ValueError( \"Input NumPy array and DataArray must have the same \" \"shape if copying over coordinates.\" ) return xr.DataArray( np_arr, dims=da.dims, coords=da.coords if use_coords else None, attrs=da.attrs.copy() if use_attrs else None, name=new_name if new_name is not None else da.name, )","title":"clone_using()"},{"location":"api/xarray/#janitor.xarray.functions.convert_datetime_to_number","text":"Convert the coordinates of a datetime axis to a human-readable float representation. Usage example to convert a DataArray 's time dimension coordinates from a minutes to seconds: >>> import numpy as np >>> import xarray as xr >>> import janitor.xarray >>> timepoints = 5 >>> da = xr.DataArray( ... np.array([2, 8, 0, 1, 7, 7]), ... dims='time', ... coords=dict(time=np.arange(6) * np.timedelta64(1, 'm')) ... ) >>> da_minutes = da.convert_datetime_to_number('s', dim='time') >>> da_minutes <xarray.DataArray (time: 6)> array([2, 8, 0, 1, 7, 7]) Coordinates: * time (time) float64 0.0 60.0 120.0 180.0 240.0 300.0 Parameters: Name Type Description Default da_or_ds Union[xarray.core.dataarray.DataArray, xarray.core.dataset.Dataset] XArray object. required time_units str Numpy timedelta string specification for the unit you would like to convert the coordinates to. required dim str the time dimension whose coordinates are datetime objects. 'time' Returns: Type Description The original XArray object with the time dimension reassigned. Source code in janitor/xarray/functions.py @pf.register_xarray_dataset_method @pf.register_xarray_dataarray_method def convert_datetime_to_number( da_or_ds: Union[xr.DataArray, xr.Dataset], time_units: str, dim: str = \"time\", ): \"\"\" Convert the coordinates of a datetime axis to a human-readable float representation. Usage example to convert a `DataArray`'s time dimension coordinates from a minutes to seconds: >>> import numpy as np >>> import xarray as xr >>> import janitor.xarray >>> timepoints = 5 >>> da = xr.DataArray( ... np.array([2, 8, 0, 1, 7, 7]), ... dims='time', ... coords=dict(time=np.arange(6) * np.timedelta64(1, 'm')) ... ) >>> da_minutes = da.convert_datetime_to_number('s', dim='time') >>> da_minutes <xarray.DataArray (time: 6)> array([2, 8, 0, 1, 7, 7]) Coordinates: * time (time) float64 0.0 60.0 120.0 180.0 240.0 300.0 :param da_or_ds: XArray object. :param time_units: Numpy timedelta string specification for the unit you would like to convert the coordinates to. :param dim: the time dimension whose coordinates are datetime objects. :return: The original XArray object with the time dimension reassigned. \"\"\" times = da_or_ds.coords[dim].data / np.timedelta64(1, time_units) return da_or_ds.assign_coords({dim: times})","title":"convert_datetime_to_number()"},{"location":"development/lazy_imports/","text":"Lazy Imports In pyjanitor , we use lazy imports to speed up import janitor . Prior to using lazy imports, import janitor would take about 1-2 seconds to complete, thereby causing significant delays for downstream consumers of pyjanitor . Slow importing be undesirable as it would slow down programs that demand low latency. A brief history of the decision The original issue was raised by @ericmjl in issue ( #1059 ). The basis there is that the scientific Python community was hurting with imports that took a long time, especially the ones that depended on SciPy and Pandas. As pyjanitor is a package that depends on pandas , it was important for us to see if we could improve the speed at which imports happened. Current Speed Benchmark As of 5 April 2022, imports take about ~0.5 seconds (give or take) to complete on a GitHub Codespaces workspace. This is much more desirable than the original 1-2 seconds, also measured on a GitHub Codespaces workspace. How to benchmark To benchmark, we run the following line: python -X importtime -c \"import janitor\" 2> timing.log Then, using the tuna CLI tool, we can view the timing log: tuna timing.log Note: You may need to install tuna using pip install -U tuna . tuna 's development repository is on GitHub You'll be redirected to your browser, where the web UI will allow you to see which imports are causing time delays. Which imports to lazily load Generally speaking, the external imports are the ones that when lazily loaded, will give the maximal gain in speed. You can also opt to lazily load pyjanitor submodules, but we doubt they will give much advantage in speed.","title":"Lazy Imports"},{"location":"development/lazy_imports/#lazy-imports","text":"In pyjanitor , we use lazy imports to speed up import janitor . Prior to using lazy imports, import janitor would take about 1-2 seconds to complete, thereby causing significant delays for downstream consumers of pyjanitor . Slow importing be undesirable as it would slow down programs that demand low latency.","title":"Lazy Imports"},{"location":"development/lazy_imports/#a-brief-history-of-the-decision","text":"The original issue was raised by @ericmjl in issue ( #1059 ). The basis there is that the scientific Python community was hurting with imports that took a long time, especially the ones that depended on SciPy and Pandas. As pyjanitor is a package that depends on pandas , it was important for us to see if we could improve the speed at which imports happened.","title":"A brief history of the decision"},{"location":"development/lazy_imports/#current-speed-benchmark","text":"As of 5 April 2022, imports take about ~0.5 seconds (give or take) to complete on a GitHub Codespaces workspace. This is much more desirable than the original 1-2 seconds, also measured on a GitHub Codespaces workspace.","title":"Current Speed Benchmark"},{"location":"development/lazy_imports/#how-to-benchmark","text":"To benchmark, we run the following line: python -X importtime -c \"import janitor\" 2> timing.log Then, using the tuna CLI tool, we can view the timing log: tuna timing.log Note: You may need to install tuna using pip install -U tuna . tuna 's development repository is on GitHub You'll be redirected to your browser, where the web UI will allow you to see which imports are causing time delays.","title":"How to benchmark"},{"location":"development/lazy_imports/#which-imports-to-lazily-load","text":"Generally speaking, the external imports are the ones that when lazily loaded, will give the maximal gain in speed. You can also opt to lazily load pyjanitor submodules, but we doubt they will give much advantage in speed.","title":"Which imports to lazily load"}]}